Current modulesystem is already LMOD modules, nothing to change for env2lmod
Starting to activate virtual environment
Activated virtual environment
config files: ['configs/VRC/X3D_S.yaml']
[11/07 13:08:10][INFO] train_net.py:  540: Train with config:
[11/07 13:08:10][INFO] train_net.py:  541: CfgNode({'CONTRASTIVE': CfgNode({'T': 0.07, 'DIM': 128, 'LENGTH': 239975, 'QUEUE_LEN': 65536, 'MOMENTUM': 0.5, 'MOMENTUM_ANNEALING': False, 'TYPE': 'mem', 'INTERP_MEMORY': False, 'MEM_TYPE': '1d', 'NUM_CLASSES_DOWNSTREAM': 400, 'NUM_MLP_LAYERS': 1, 'MLP_DIM': 2048, 'BN_MLP': False, 'BN_SYNC_MLP': False, 'LOCAL_SHUFFLE_BN': True, 'MOCO_MULTI_VIEW_QUEUE': False, 'DELTA_CLIPS_MIN': -inf, 'DELTA_CLIPS_MAX': inf, 'PREDICTOR_DEPTHS': [], 'SEQUENTIAL': False, 'SIMCLR_DIST_ON': True, 'SWAV_QEUE_LEN': 0, 'KNN_ON': True}), 'BN': CfgNode({'USE_PRECISE_STATS': True, 'NUM_BATCHES_PRECISE': 200, 'WEIGHT_DECAY': 0.0, 'NORM_TYPE': 'batchnorm', 'NUM_SPLITS': 1, 'NUM_SYNC_DEVICES': 1, 'GLOBAL_SYNC': False}), 'TRAIN': CfgNode({'ENABLE': True, 'KILL_LOSS_EXPLOSION_FACTOR': 0.0, 'DATASET': 'kinetics', 'BATCH_SIZE': 4, 'EVAL_PERIOD': 10, 'CHECKPOINT_PERIOD': 10, 'AUTO_RESUME': True, 'CHECKPOINT_FILE_PATH': '', 'CHECKPOINT_TYPE': 'pytorch', 'CHECKPOINT_INFLATE': False, 'CHECKPOINT_EPOCH_RESET': False, 'CHECKPOINT_CLEAR_NAME_PATTERN': (), 'MIXED_PRECISION': False, 'CHECKPOINT_IN_INIT': False}), 'AUG': CfgNode({'ENABLE': False, 'NUM_SAMPLE': 1, 'COLOR_JITTER': 0.4, 'AA_TYPE': 'rand-m9-mstd0.5-inc1', 'INTERPOLATION': 'bicubic', 'RE_PROB': 0.25, 'RE_MODE': 'pixel', 'RE_COUNT': 1, 'RE_SPLIT': False, 'GEN_MASK_LOADER': False, 'MASK_TUBE': False, 'MASK_FRAMES': False, 'MASK_WINDOW_SIZE': [8, 7, 7], 'MASK_RATIO': 0.0, 'MAX_MASK_PATCHES_PER_BLOCK': None}), 'VIS_MASK': CfgNode({'ENABLE': False}), 'MIXUP': CfgNode({'ENABLE': False, 'ALPHA': 0.8, 'CUTMIX_ALPHA': 1.0, 'PROB': 1.0, 'SWITCH_PROB': 0.5, 'LABEL_SMOOTH_VALUE': 0.1}), 'TEST': CfgNode({'ENABLE': True, 'DATASET': 'kinetics', 'BATCH_SIZE': 4, 'CHECKPOINT_FILE_PATH': '', 'NUM_ENSEMBLE_VIEWS': 10, 'NUM_SPATIAL_CROPS': 3, 'CHECKPOINT_TYPE': 'pytorch', 'SAVE_RESULTS_PATH': '', 'NUM_TEMPORAL_CLIPS': []}), 'RESNET': CfgNode({'TRANS_FUNC': 'x3d_transform', 'NUM_GROUPS': 1, 'WIDTH_PER_GROUP': 64, 'INPLACE_RELU': True, 'STRIDE_1X1': False, 'ZERO_INIT_FINAL_BN': True, 'ZERO_INIT_FINAL_CONV': False, 'DEPTH': 50, 'NUM_BLOCK_TEMP_KERNEL': [[3], [4], [6], [3]], 'SPATIAL_STRIDES': [[1], [2], [2], [2]], 'SPATIAL_DILATIONS': [[1], [1], [1], [1]]}), 'X3D': CfgNode({'WIDTH_FACTOR': 2.0, 'DEPTH_FACTOR': 2.2, 'BOTTLENECK_FACTOR': 2.25, 'DIM_C5': 2048, 'DIM_C1': 12, 'SCALE_RES2': False, 'BN_LIN5': False, 'CHANNELWISE_3x3x3': True}), 'NONLOCAL': CfgNode({'LOCATION': [[[]], [[]], [[]], [[]]], 'GROUP': [[1], [1], [1], [1]], 'INSTANTIATION': 'dot_product', 'POOL': [[[1, 2, 2], [1, 2, 2]], [[1, 2, 2], [1, 2, 2]], [[1, 2, 2], [1, 2, 2]], [[1, 2, 2], [1, 2, 2]]]}), 'MODEL': CfgNode({'ARCH': 'x3d', 'MODEL_NAME': 'X3D', 'NUM_CLASSES': 13, 'LOSS_FUNC': 'cross_entropy', 'SINGLE_PATHWAY_ARCH': ['2d', 'c2d', 'i3d', 'slow', 'x3d', 'mvit', 'maskmvit'], 'MULTI_PATHWAY_ARCH': ['slowfast'], 'DROPOUT_RATE': 0.5, 'DROPCONNECT_RATE': 0.0, 'FC_INIT_STD': 0.01, 'HEAD_ACT': 'softmax', 'ACT_CHECKPOINT': False, 'DETACH_FINAL_FC': False, 'FROZEN_BN': False, 'FP16_ALLREDUCE': False}), 'MVIT': CfgNode({'MODE': 'conv', 'POOL_FIRST': False, 'CLS_EMBED_ON': True, 'PATCH_KERNEL': [3, 7, 7], 'PATCH_STRIDE': [2, 4, 4], 'PATCH_PADDING': [2, 4, 4], 'PATCH_2D': False, 'EMBED_DIM': 96, 'NUM_HEADS': 1, 'MLP_RATIO': 4.0, 'QKV_BIAS': True, 'DROPPATH_RATE': 0.1, 'LAYER_SCALE_INIT_VALUE': 0.0, 'DEPTH': 16, 'NORM': 'layernorm', 'DIM_MUL': [], 'HEAD_MUL': [], 'POOL_KV_STRIDE': [], 'POOL_KV_STRIDE_ADAPTIVE': None, 'POOL_Q_STRIDE': [], 'POOL_KVQ_KERNEL': None, 'ZERO_DECAY_POS_CLS': True, 'NORM_STEM': False, 'SEP_POS_EMBED': False, 'DROPOUT_RATE': 0.0, 'USE_ABS_POS': True, 'REL_POS_SPATIAL': False, 'REL_POS_TEMPORAL': False, 'REL_POS_ZERO_INIT': False, 'RESIDUAL_POOLING': False, 'DIM_MUL_IN_ATT': False, 'SEPARATE_QKV': False, 'HEAD_INIT_SCALE': 1.0, 'USE_MEAN_POOLING': False, 'USE_FIXED_SINCOS_POS': False, 'REV': CfgNode({'ENABLE': False, 'RESPATH_FUSE': 'concat', 'BUFFER_LAYERS': [], 'RES_PATH': 'conv', 'PRE_Q_FUSION': 'avg'})}), 'MASK': CfgNode({'ENABLE': False, 'MAE_ON': False, 'MAE_RND_MASK': False, 'PER_FRAME_MASKING': False, 'TIME_STRIDE_LOSS': True, 'NORM_PRED_PIXEL': True, 'SCALE_INIT_BY_DEPTH': False, 'DECODER_EMBED_DIM': 512, 'DECODER_SEP_POS_EMBED': False, 'DEC_KV_KERNEL': [], 'DEC_KV_STRIDE': [], 'PRETRAIN_DEPTH': [15], 'HEAD_TYPE': 'separate', 'DECODER_DEPTH': 0, 'PRED_HOG': False}), 'SLOWFAST': CfgNode({'BETA_INV': 8, 'ALPHA': 8, 'FUSION_CONV_CHANNEL_RATIO': 2, 'FUSION_KERNEL_SZ': 5}), 'DATA': CfgNode({'PATH_TO_DATA_DIR': '/cluster/work/cvl/robocup/data/Debug_Dataset', 'PATH_LABEL_SEPARATOR': ';', 'PATH_PREFIX': '', 'NUM_FRAMES': 15, 'SAMPLING_RATE': 6, 'TRAIN_PCA_EIGVAL': [0.225, 0.224, 0.229], 'TRAIN_PCA_EIGVEC': [[-0.5675, 0.7192, 0.4009], [-0.5808, -0.0045, -0.814], [-0.5836, -0.6948, 0.4203]], 'PATH_TO_PRELOAD_IMDB': '', 'MEAN': [0.45, 0.45, 0.45], 'INPUT_CHANNEL_NUM': [3], 'STD': [0.225, 0.225, 0.225], 'TRAIN_JITTER_SCALES': [182, 228], 'TRAIN_JITTER_SCALES_RELATIVE': [], 'TRAIN_JITTER_ASPECT_RELATIVE': [], 'USE_OFFSET_SAMPLING': False, 'TRAIN_JITTER_MOTION_SHIFT': False, 'TRAIN_CROP_SIZE': 160, 'TEST_CROP_SIZE': 182, 'TARGET_FPS': 30, 'TRAIN_JITTER_FPS': 0.0, 'DECODING_BACKEND': 'torchvision', 'DECODING_SHORT_SIZE': 256, 'INV_UNIFORM_SAMPLE': False, 'RANDOM_FLIP': True, 'MULTI_LABEL': False, 'ENSEMBLE_METHOD': 'sum', 'REVERSE_INPUT_CHANNEL': False, 'TRAIN_CROP_NUM_TEMPORAL': 1, 'TRAIN_CROP_NUM_SPATIAL': 1, 'COLOR_RND_GRAYSCALE': 0.0, 'LOADER_CHUNK_SIZE': 0, 'LOADER_CHUNK_OVERALL_SIZE': 0, 'SKIP_ROWS': 0, 'TIME_DIFF_PROB': 0.0, 'SSL_COLOR_JITTER': False, 'SSL_COLOR_BRI_CON_SAT': [0.4, 0.4, 0.4], 'SSL_COLOR_HUE': 0.1, 'SSL_MOCOV2_AUG': False, 'SSL_BLUR_SIGMA_MIN': [0.0, 0.1], 'SSL_BLUR_SIGMA_MAX': [0.0, 2.0], 'IN22K_TRAINVAL': False, 'IN22k_VAL_IN1K': '', 'IN_VAL_CROP_RATIO': 0.875, 'DUMMY_LOAD': False}), 'SOLVER': CfgNode({'BASE_LR': 0.1, 'LR_POLICY': 'cosine', 'COSINE_END_LR': 0.0, 'GAMMA': 0.1, 'STEP_SIZE': 1, 'STEPS': [], 'LRS': [], 'MAX_EPOCH': 300, 'MOMENTUM': 0.9, 'DAMPENING': 0.0, 'NESTEROV': True, 'WEIGHT_DECAY': 5e-05, 'WARMUP_FACTOR': 0.1, 'WARMUP_EPOCHS': 35.0, 'WARMUP_START_LR': 0.01, 'OPTIMIZING_METHOD': 'sgd', 'BASE_LR_SCALE_NUM_SHARDS': True, 'COSINE_AFTER_WARMUP': False, 'ZERO_WD_1D_PARAM': False, 'CLIP_GRAD_VAL': None, 'CLIP_GRAD_L2NORM': None, 'LARS_ON': False, 'LAYER_DECAY': 1.0, 'BETAS': (0.9, 0.999)}), 'TASK': '', 'NUM_GPUS': 1, 'NUM_SHARDS': 1, 'SHARD_ID': 0, 'OUTPUT_DIR': '.', 'RNG_SEED': 0, 'LOG_PERIOD': 10, 'LOG_MODEL_INFO': True, 'DIST_BACKEND': 'nccl', 'BENCHMARK': CfgNode({'NUM_EPOCHS': 5, 'LOG_PERIOD': 100, 'SHUFFLE': True}), 'DATA_LOADER': CfgNode({'NUM_WORKERS': 8, 'PIN_MEMORY': True, 'ENABLE_MULTI_THREAD_DECODE': False}), 'DETECTION': CfgNode({'ENABLE': False, 'ALIGNED': True, 'SPATIAL_SCALE_FACTOR': 16, 'ROI_XFORM_RESOLUTION': 7}), 'AVA': CfgNode({'FRAME_DIR': '/mnt/fair-flash3-east/ava_trainval_frames.img/', 'FRAME_LIST_DIR': '/mnt/vol/gfsai-flash3-east/ai-group/users/haoqifan/ava/frame_list/', 'ANNOTATION_DIR': '/mnt/vol/gfsai-flash3-east/ai-group/users/haoqifan/ava/frame_list/', 'TRAIN_LISTS': ['train.csv'], 'TEST_LISTS': ['val.csv'], 'TRAIN_GT_BOX_LISTS': ['ava_train_v2.2.csv'], 'TRAIN_PREDICT_BOX_LISTS': [], 'TEST_PREDICT_BOX_LISTS': ['ava_val_predicted_boxes.csv'], 'DETECTION_SCORE_THRESH': 0.9, 'BGR': False, 'TRAIN_USE_COLOR_AUGMENTATION': False, 'TRAIN_PCA_JITTER_ONLY': True, 'TEST_FORCE_FLIP': False, 'FULL_TEST_ON_VAL': False, 'LABEL_MAP_FILE': 'ava_action_list_v2.2_for_activitynet_2019.pbtxt', 'EXCLUSION_FILE': 'ava_val_excluded_timestamps_v2.2.csv', 'GROUNDTRUTH_FILE': 'ava_val_v2.2.csv', 'IMG_PROC_BACKEND': 'cv2'}), 'MULTIGRID': CfgNode({'EPOCH_FACTOR': 1.5, 'SHORT_CYCLE': False, 'SHORT_CYCLE_FACTORS': [0.5, 0.7071067811865476], 'LONG_CYCLE': False, 'LONG_CYCLE_FACTORS': [(0.25, 0.7071067811865476), (0.5, 0.7071067811865476), (0.5, 1), (1, 1)], 'BN_BASE_SIZE': 8, 'EVAL_FREQ': 3, 'LONG_CYCLE_SAMPLING_RATE': 0, 'DEFAULT_B': 0, 'DEFAULT_T': 0, 'DEFAULT_S': 0}), 'TENSORBOARD': CfgNode({'ENABLE': False, 'PREDICTIONS_PATH': '', 'LOG_DIR': '', 'CLASS_NAMES_PATH': '', 'CATEGORIES_PATH': '', 'CONFUSION_MATRIX': CfgNode({'ENABLE': False, 'FIGSIZE': [8, 8], 'SUBSET_PATH': ''}), 'HISTOGRAM': CfgNode({'ENABLE': False, 'SUBSET_PATH': '', 'TOPK': 10, 'FIGSIZE': [8, 8]}), 'MODEL_VIS': CfgNode({'ENABLE': False, 'MODEL_WEIGHTS': False, 'ACTIVATIONS': False, 'INPUT_VIDEO': False, 'LAYER_LIST': [], 'TOPK_PREDS': 1, 'COLORMAP': 'Pastel2', 'GRAD_CAM': CfgNode({'ENABLE': True, 'LAYER_LIST': [], 'USE_TRUE_LABEL': False, 'COLORMAP': 'viridis'})}), 'WRONG_PRED_VIS': CfgNode({'ENABLE': False, 'TAG': 'Incorrectly classified videos.', 'SUBSET_PATH': ''})}), 'DEMO': CfgNode({'ENABLE': False, 'LABEL_FILE_PATH': '', 'WEBCAM': -1, 'INPUT_VIDEO': '', 'DISPLAY_WIDTH': 0, 'DISPLAY_HEIGHT': 0, 'DETECTRON2_CFG': 'COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml', 'DETECTRON2_WEIGHTS': 'detectron2://COCO-Detection/faster_rcnn_R_50_FPN_3x/137849458/model_final_280758.pkl', 'DETECTRON2_THRESH': 0.9, 'BUFFER_SIZE': 0, 'OUTPUT_FILE': '', 'OUTPUT_FPS': -1, 'INPUT_FORMAT': 'BGR', 'CLIP_VIS_SIZE': 10, 'NUM_VIS_INSTANCES': 2, 'PREDS_BOXES': '', 'THREAD_ENABLE': False, 'NUM_CLIPS_SKIP': 0, 'GT_BOXES': '', 'STARTING_SECOND': 900, 'FPS': 30, 'VIS_MODE': 'thres', 'COMMON_CLASS_THRES': 0.7, 'UNCOMMON_CLASS_THRES': 0.3, 'COMMON_CLASS_NAMES': ['watch (a person)', 'talk to (e.g., self, a person, a group)', 'listen to (a person)', 'touch (an object)', 'carry/hold (an object)', 'walk', 'sit', 'lie/sleep', 'bend/bow (at the waist)'], 'SLOWMO': 1})})
[11/07 13:08:12][INFO] misc.py:  185: Model:
X3D(
  (s1): VideoModelStem(
    (pathway0_stem): X3DStem(
      (conv_xy): Conv3d(3, 24, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1), bias=False)
      (conv): Conv3d(24, 24, kernel_size=(5, 1, 1), stride=(1, 1, 1), padding=(2, 0, 0), groups=24, bias=False)
      (bn): BatchNorm3d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (s2): ResStage(
    (pathway0_res0): ResBlock(
      (branch1): Conv3d(24, 24, kernel_size=(1, 1, 1), stride=(1, 2, 2), bias=False)
      (branch1_bn): BatchNorm3d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (branch2): X3DTransform(
        (a): Conv3d(24, 54, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (a_bn): BatchNorm3d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (a_relu): ReLU(inplace=True)
        (b): Conv3d(54, 54, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1), groups=54, bias=False)
        (b_bn): BatchNorm3d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): SE(
          (avg_pool): AdaptiveAvgPool3d(output_size=(1, 1, 1))
          (fc1): Conv3d(54, 8, kernel_size=(1, 1, 1), stride=(1, 1, 1))
          (fc1_act): ReLU()
          (fc2): Conv3d(8, 54, kernel_size=(1, 1, 1), stride=(1, 1, 1))
          (fc2_sig): Sigmoid()
        )
        (b_relu): Swish()
        (c): Conv3d(54, 24, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (c_bn): BatchNorm3d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (relu): ReLU(inplace=True)
    )
    (pathway0_res1): ResBlock(
      (branch2): X3DTransform(
        (a): Conv3d(24, 54, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (a_bn): BatchNorm3d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (a_relu): ReLU(inplace=True)
        (b): Conv3d(54, 54, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=54, bias=False)
        (b_bn): BatchNorm3d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (b_relu): Swish()
        (c): Conv3d(54, 24, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (c_bn): BatchNorm3d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (relu): ReLU(inplace=True)
    )
    (pathway0_res2): ResBlock(
      (branch2): X3DTransform(
        (a): Conv3d(24, 54, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (a_bn): BatchNorm3d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (a_relu): ReLU(inplace=True)
        (b): Conv3d(54, 54, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=54, bias=False)
        (b_bn): BatchNorm3d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): SE(
          (avg_pool): AdaptiveAvgPool3d(output_size=(1, 1, 1))
          (fc1): Conv3d(54, 8, kernel_size=(1, 1, 1), stride=(1, 1, 1))
          (fc1_act): ReLU()
          (fc2): Conv3d(8, 54, kernel_size=(1, 1, 1), stride=(1, 1, 1))
          (fc2_sig): Sigmoid()
        )
        (b_relu): Swish()
        (c): Conv3d(54, 24, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (c_bn): BatchNorm3d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (relu): ReLU(inplace=True)
    )
  )
  (s3): ResStage(
    (pathway0_res0): ResBlock(
      (branch1): Conv3d(24, 48, kernel_size=(1, 1, 1), stride=(1, 2, 2), bias=False)
      (branch1_bn): BatchNorm3d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (branch2): X3DTransform(
        (a): Conv3d(24, 108, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (a_bn): BatchNorm3d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (a_relu): ReLU(inplace=True)
        (b): Conv3d(108, 108, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1), groups=108, bias=False)
        (b_bn): BatchNorm3d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): SE(
          (avg_pool): AdaptiveAvgPool3d(output_size=(1, 1, 1))
          (fc1): Conv3d(108, 8, kernel_size=(1, 1, 1), stride=(1, 1, 1))
          (fc1_act): ReLU()
          (fc2): Conv3d(8, 108, kernel_size=(1, 1, 1), stride=(1, 1, 1))
          (fc2_sig): Sigmoid()
        )
        (b_relu): Swish()
        (c): Conv3d(108, 48, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (c_bn): BatchNorm3d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (relu): ReLU(inplace=True)
    )
    (pathway0_res1): ResBlock(
      (branch2): X3DTransform(
        (a): Conv3d(48, 108, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (a_bn): BatchNorm3d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (a_relu): ReLU(inplace=True)
        (b): Conv3d(108, 108, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=108, bias=False)
        (b_bn): BatchNorm3d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (b_relu): Swish()
        (c): Conv3d(108, 48, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (c_bn): BatchNorm3d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (relu): ReLU(inplace=True)
    )
    (pathway0_res2): ResBlock(
      (branch2): X3DTransform(
        (a): Conv3d(48, 108, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (a_bn): BatchNorm3d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (a_relu): ReLU(inplace=True)
        (b): Conv3d(108, 108, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=108, bias=False)
        (b_bn): BatchNorm3d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): SE(
          (avg_pool): AdaptiveAvgPool3d(output_size=(1, 1, 1))
          (fc1): Conv3d(108, 8, kernel_size=(1, 1, 1), stride=(1, 1, 1))
          (fc1_act): ReLU()
          (fc2): Conv3d(8, 108, kernel_size=(1, 1, 1), stride=(1, 1, 1))
          (fc2_sig): Sigmoid()
        )
        (b_relu): Swish()
        (c): Conv3d(108, 48, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (c_bn): BatchNorm3d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (relu): ReLU(inplace=True)
    )
    (pathway0_res3): ResBlock(
      (branch2): X3DTransform(
        (a): Conv3d(48, 108, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (a_bn): BatchNorm3d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (a_relu): ReLU(inplace=True)
        (b): Conv3d(108, 108, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=108, bias=False)
        (b_bn): BatchNorm3d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (b_relu): Swish()
        (c): Conv3d(108, 48, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (c_bn): BatchNorm3d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (relu): ReLU(inplace=True)
    )
    (pathway0_res4): ResBlock(
      (branch2): X3DTransform(
        (a): Conv3d(48, 108, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (a_bn): BatchNorm3d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (a_relu): ReLU(inplace=True)
        (b): Conv3d(108, 108, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=108, bias=False)
        (b_bn): BatchNorm3d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): SE(
          (avg_pool): AdaptiveAvgPool3d(output_size=(1, 1, 1))
          (fc1): Conv3d(108, 8, kernel_size=(1, 1, 1), stride=(1, 1, 1))
          (fc1_act): ReLU()
          (fc2): Conv3d(8, 108, kernel_size=(1, 1, 1), stride=(1, 1, 1))
          (fc2_sig): Sigmoid()
        )
        (b_relu): Swish()
        (c): Conv3d(108, 48, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (c_bn): BatchNorm3d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (relu): ReLU(inplace=True)
    )
  )
  (s4): ResStage(
    (pathway0_res0): ResBlock(
      (branch1): Conv3d(48, 96, kernel_size=(1, 1, 1), stride=(1, 2, 2), bias=False)
      (branch1_bn): BatchNorm3d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (branch2): X3DTransform(
        (a): Conv3d(48, 216, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (a_bn): BatchNorm3d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (a_relu): ReLU(inplace=True)
        (b): Conv3d(216, 216, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1), groups=216, bias=False)
        (b_bn): BatchNorm3d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): SE(
          (avg_pool): AdaptiveAvgPool3d(output_size=(1, 1, 1))
          (fc1): Conv3d(216, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1))
          (fc1_act): ReLU()
          (fc2): Conv3d(16, 216, kernel_size=(1, 1, 1), stride=(1, 1, 1))
          (fc2_sig): Sigmoid()
        )
        (b_relu): Swish()
        (c): Conv3d(216, 96, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (c_bn): BatchNorm3d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (relu): ReLU(inplace=True)
    )
    (pathway0_res1): ResBlock(
      (branch2): X3DTransform(
        (a): Conv3d(96, 216, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (a_bn): BatchNorm3d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (a_relu): ReLU(inplace=True)
        (b): Conv3d(216, 216, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=216, bias=False)
        (b_bn): BatchNorm3d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (b_relu): Swish()
        (c): Conv3d(216, 96, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (c_bn): BatchNorm3d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (relu): ReLU(inplace=True)
    )
    (pathway0_res2): ResBlock(
      (branch2): X3DTransform(
        (a): Conv3d(96, 216, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (a_bn): BatchNorm3d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (a_relu): ReLU(inplace=True)
        (b): Conv3d(216, 216, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=216, bias=False)
        (b_bn): BatchNorm3d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): SE(
          (avg_pool): AdaptiveAvgPool3d(output_size=(1, 1, 1))
          (fc1): Conv3d(216, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1))
          (fc1_act): ReLU()
          (fc2): Conv3d(16, 216, kernel_size=(1, 1, 1), stride=(1, 1, 1))
          (fc2_sig): Sigmoid()
        )
        (b_relu): Swish()
        (c): Conv3d(216, 96, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (c_bn): BatchNorm3d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (relu): ReLU(inplace=True)
    )
    (pathway0_res3): ResBlock(
      (branch2): X3DTransform(
        (a): Conv3d(96, 216, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (a_bn): BatchNorm3d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (a_relu): ReLU(inplace=True)
        (b): Conv3d(216, 216, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=216, bias=False)
        (b_bn): BatchNorm3d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (b_relu): Swish()
        (c): Conv3d(216, 96, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (c_bn): BatchNorm3d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (relu): ReLU(inplace=True)
    )
    (pathway0_res4): ResBlock(
      (branch2): X3DTransform(
        (a): Conv3d(96, 216, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (a_bn): BatchNorm3d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (a_relu): ReLU(inplace=True)
        (b): Conv3d(216, 216, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=216, bias=False)
        (b_bn): BatchNorm3d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): SE(
          (avg_pool): AdaptiveAvgPool3d(output_size=(1, 1, 1))
          (fc1): Conv3d(216, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1))
          (fc1_act): ReLU()
          (fc2): Conv3d(16, 216, kernel_size=(1, 1, 1), stride=(1, 1, 1))
          (fc2_sig): Sigmoid()
        )
        (b_relu): Swish()
        (c): Conv3d(216, 96, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (c_bn): BatchNorm3d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (relu): ReLU(inplace=True)
    )
    (pathway0_res5): ResBlock(
      (branch2): X3DTransform(
        (a): Conv3d(96, 216, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (a_bn): BatchNorm3d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (a_relu): ReLU(inplace=True)
        (b): Conv3d(216, 216, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=216, bias=False)
        (b_bn): BatchNorm3d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (b_relu): Swish()
        (c): Conv3d(216, 96, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (c_bn): BatchNorm3d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (relu): ReLU(inplace=True)
    )
    (pathway0_res6): ResBlock(
      (branch2): X3DTransform(
        (a): Conv3d(96, 216, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (a_bn): BatchNorm3d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (a_relu): ReLU(inplace=True)
        (b): Conv3d(216, 216, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=216, bias=False)
        (b_bn): BatchNorm3d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): SE(
          (avg_pool): AdaptiveAvgPool3d(output_size=(1, 1, 1))
          (fc1): Conv3d(216, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1))
          (fc1_act): ReLU()
          (fc2): Conv3d(16, 216, kernel_size=(1, 1, 1), stride=(1, 1, 1))
          (fc2_sig): Sigmoid()
        )
        (b_relu): Swish()
        (c): Conv3d(216, 96, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (c_bn): BatchNorm3d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (relu): ReLU(inplace=True)
    )
    (pathway0_res7): ResBlock(
      (branch2): X3DTransform(
        (a): Conv3d(96, 216, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (a_bn): BatchNorm3d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (a_relu): ReLU(inplace=True)
        (b): Conv3d(216, 216, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=216, bias=False)
        (b_bn): BatchNorm3d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (b_relu): Swish()
        (c): Conv3d(216, 96, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (c_bn): BatchNorm3d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (relu): ReLU(inplace=True)
    )
    (pathway0_res8): ResBlock(
      (branch2): X3DTransform(
        (a): Conv3d(96, 216, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (a_bn): BatchNorm3d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (a_relu): ReLU(inplace=True)
        (b): Conv3d(216, 216, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=216, bias=False)
        (b_bn): BatchNorm3d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): SE(
          (avg_pool): AdaptiveAvgPool3d(output_size=(1, 1, 1))
          (fc1): Conv3d(216, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1))
          (fc1_act): ReLU()
          (fc2): Conv3d(16, 216, kernel_size=(1, 1, 1), stride=(1, 1, 1))
          (fc2_sig): Sigmoid()
        )
        (b_relu): Swish()
        (c): Conv3d(216, 96, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (c_bn): BatchNorm3d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (relu): ReLU(inplace=True)
    )
    (pathway0_res9): ResBlock(
      (branch2): X3DTransform(
        (a): Conv3d(96, 216, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (a_bn): BatchNorm3d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (a_relu): ReLU(inplace=True)
        (b): Conv3d(216, 216, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=216, bias=False)
        (b_bn): BatchNorm3d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (b_relu): Swish()
        (c): Conv3d(216, 96, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (c_bn): BatchNorm3d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (relu): ReLU(inplace=True)
    )
    (pathway0_res10): ResBlock(
      (branch2): X3DTransform(
        (a): Conv3d(96, 216, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (a_bn): BatchNorm3d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (a_relu): ReLU(inplace=True)
        (b): Conv3d(216, 216, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=216, bias=False)
        (b_bn): BatchNorm3d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): SE(
          (avg_pool): AdaptiveAvgPool3d(output_size=(1, 1, 1))
          (fc1): Conv3d(216, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1))
          (fc1_act): ReLU()
          (fc2): Conv3d(16, 216, kernel_size=(1, 1, 1), stride=(1, 1, 1))
          (fc2_sig): Sigmoid()
        )
        (b_relu): Swish()
        (c): Conv3d(216, 96, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (c_bn): BatchNorm3d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (relu): ReLU(inplace=True)
    )
  )
  (s5): ResStage(
    (pathway0_res0): ResBlock(
      (branch1): Conv3d(96, 192, kernel_size=(1, 1, 1), stride=(1, 2, 2), bias=False)
      (branch1_bn): BatchNorm3d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (branch2): X3DTransform(
        (a): Conv3d(96, 432, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (a_bn): BatchNorm3d(432, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (a_relu): ReLU(inplace=True)
        (b): Conv3d(432, 432, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1), groups=432, bias=False)
        (b_bn): BatchNorm3d(432, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): SE(
          (avg_pool): AdaptiveAvgPool3d(output_size=(1, 1, 1))
          (fc1): Conv3d(432, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1))
          (fc1_act): ReLU()
          (fc2): Conv3d(32, 432, kernel_size=(1, 1, 1), stride=(1, 1, 1))
          (fc2_sig): Sigmoid()
        )
        (b_relu): Swish()
        (c): Conv3d(432, 192, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (c_bn): BatchNorm3d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (relu): ReLU(inplace=True)
    )
    (pathway0_res1): ResBlock(
      (branch2): X3DTransform(
        (a): Conv3d(192, 432, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (a_bn): BatchNorm3d(432, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (a_relu): ReLU(inplace=True)
        (b): Conv3d(432, 432, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=432, bias=False)
        (b_bn): BatchNorm3d(432, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (b_relu): Swish()
        (c): Conv3d(432, 192, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (c_bn): BatchNorm3d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (relu): ReLU(inplace=True)
    )
    (pathway0_res2): ResBlock(
      (branch2): X3DTransform(
        (a): Conv3d(192, 432, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (a_bn): BatchNorm3d(432, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (a_relu): ReLU(inplace=True)
        (b): Conv3d(432, 432, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=432, bias=False)
        (b_bn): BatchNorm3d(432, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): SE(
          (avg_pool): AdaptiveAvgPool3d(output_size=(1, 1, 1))
          (fc1): Conv3d(432, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1))
          (fc1_act): ReLU()
          (fc2): Conv3d(32, 432, kernel_size=(1, 1, 1), stride=(1, 1, 1))
          (fc2_sig): Sigmoid()
        )
        (b_relu): Swish()
        (c): Conv3d(432, 192, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (c_bn): BatchNorm3d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (relu): ReLU(inplace=True)
    )
    (pathway0_res3): ResBlock(
      (branch2): X3DTransform(
        (a): Conv3d(192, 432, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (a_bn): BatchNorm3d(432, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (a_relu): ReLU(inplace=True)
        (b): Conv3d(432, 432, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=432, bias=False)
        (b_bn): BatchNorm3d(432, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (b_relu): Swish()
        (c): Conv3d(432, 192, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (c_bn): BatchNorm3d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (relu): ReLU(inplace=True)
    )
    (pathway0_res4): ResBlock(
      (branch2): X3DTransform(
        (a): Conv3d(192, 432, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (a_bn): BatchNorm3d(432, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (a_relu): ReLU(inplace=True)
        (b): Conv3d(432, 432, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=432, bias=False)
        (b_bn): BatchNorm3d(432, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): SE(
          (avg_pool): AdaptiveAvgPool3d(output_size=(1, 1, 1))
          (fc1): Conv3d(432, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1))
          (fc1_act): ReLU()
          (fc2): Conv3d(32, 432, kernel_size=(1, 1, 1), stride=(1, 1, 1))
          (fc2_sig): Sigmoid()
        )
        (b_relu): Swish()
        (c): Conv3d(432, 192, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (c_bn): BatchNorm3d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (relu): ReLU(inplace=True)
    )
    (pathway0_res5): ResBlock(
      (branch2): X3DTransform(
        (a): Conv3d(192, 432, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (a_bn): BatchNorm3d(432, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (a_relu): ReLU(inplace=True)
        (b): Conv3d(432, 432, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=432, bias=False)
        (b_bn): BatchNorm3d(432, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (b_relu): Swish()
        (c): Conv3d(432, 192, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (c_bn): BatchNorm3d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (relu): ReLU(inplace=True)
    )
    (pathway0_res6): ResBlock(
      (branch2): X3DTransform(
        (a): Conv3d(192, 432, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (a_bn): BatchNorm3d(432, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (a_relu): ReLU(inplace=True)
        (b): Conv3d(432, 432, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=432, bias=False)
        (b_bn): BatchNorm3d(432, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): SE(
          (avg_pool): AdaptiveAvgPool3d(output_size=(1, 1, 1))
          (fc1): Conv3d(432, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1))
          (fc1_act): ReLU()
          (fc2): Conv3d(32, 432, kernel_size=(1, 1, 1), stride=(1, 1, 1))
          (fc2_sig): Sigmoid()
        )
        (b_relu): Swish()
        (c): Conv3d(432, 192, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (c_bn): BatchNorm3d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (relu): ReLU(inplace=True)
    )
  )
  (head): X3DHead(
    (conv_5): Conv3d(192, 432, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (conv_5_bn): BatchNorm3d(432, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_5_relu): ReLU(inplace=True)
    (avg_pool): AvgPool3d(kernel_size=[15, 5, 5], stride=1, padding=0)
    (lin_5): Conv3d(432, 2048, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (lin_5_relu): ReLU(inplace=True)
    (dropout): Dropout(p=0.5, inplace=False)
    (projection): Linear(in_features=2048, out_features=13, bias=True)
    (act): Softmax(dim=4)
  )
)
[11/07 13:08:12][INFO] misc.py:  187: Params: 3,001,359
[11/07 13:08:12][INFO] misc.py:  188: Mem: 0.011430740356445312 MB
[11/07 13:08:15][WARNING] jit_analysis.py:  499: Unsupported operator aten::adaptive_avg_pool3d encountered 15 time(s)
[11/07 13:08:15][WARNING] jit_analysis.py:  499: Unsupported operator aten::sigmoid encountered 15 time(s)
[11/07 13:08:15][WARNING] jit_analysis.py:  499: Unsupported operator aten::mul encountered 15 time(s)
[11/07 13:08:15][WARNING] jit_analysis.py:  499: Unsupported operator prim::PythonOp.SwishFunction encountered 26 time(s)
[11/07 13:08:15][WARNING] jit_analysis.py:  499: Unsupported operator aten::add encountered 26 time(s)
[11/07 13:08:15][WARNING] jit_analysis.py:  499: Unsupported operator aten::avg_pool3d encountered 1 time(s)
[11/07 13:08:15][WARNING] jit_analysis.py:  499: Unsupported operator aten::softmax encountered 1 time(s)
[11/07 13:08:15][WARNING] jit_analysis.py:  499: Unsupported operator aten::mean encountered 1 time(s)
[11/07 13:08:15][INFO] misc.py:  190: Flops: 2.3432843360000004 G
[11/07 13:08:15][WARNING] jit_analysis.py:  499: Unsupported operator aten::batch_norm encountered 84 time(s)
[11/07 13:08:15][WARNING] jit_analysis.py:  499: Unsupported operator aten::adaptive_avg_pool3d encountered 15 time(s)
[11/07 13:08:15][WARNING] jit_analysis.py:  499: Unsupported operator aten::sigmoid encountered 15 time(s)
[11/07 13:08:15][WARNING] jit_analysis.py:  499: Unsupported operator aten::mul encountered 15 time(s)
[11/07 13:08:15][WARNING] jit_analysis.py:  499: Unsupported operator prim::PythonOp.SwishFunction encountered 26 time(s)
[11/07 13:08:15][WARNING] jit_analysis.py:  499: Unsupported operator aten::add encountered 26 time(s)
[11/07 13:08:15][WARNING] jit_analysis.py:  499: Unsupported operator aten::avg_pool3d encountered 1 time(s)
[11/07 13:08:15][WARNING] jit_analysis.py:  499: Unsupported operator aten::softmax encountered 1 time(s)
[11/07 13:08:15][WARNING] jit_analysis.py:  499: Unsupported operator aten::mean encountered 1 time(s)
[11/07 13:08:15][INFO] misc.py:  191: Activations: 42.053781 M
[11/07 13:08:15][INFO] misc.py:  196: nvidia-smi
Tue Nov  7 13:08:15 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 525.125.06   Driver Version: 525.125.06   CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Quadro RTX 6000     On   | 00000000:C1:00.0 Off |                  Off |
| 33%   34C    P2    56W / 260W |   1515MiB / 24576MiB |      2%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A     89828      C   python                           1512MiB |
+-----------------------------------------------------------------------------+
bn 168, non bn 148, zero 0, no grad 0
[11/07 13:08:15][INFO] train_net.py:  556: Load from last checkpoint.
[11/07 13:08:15][INFO] checkpoint.py:  222: Loading network weights from ./checkpoints/checkpoint_epoch_00010.pyth.
missing keys: []
unexpected keys: []
[11/07 13:08:16][INFO] kinetics.py:  106: Constructing VRC train...
[11/07 13:08:16][INFO] kinetics.py:  162: Constructing VRC dataloader (size: 24 skip_rows 0) from /cluster/work/cvl/robocup/data/Debug_Dataset/small_train.csv 
[11/07 13:08:16][INFO] kinetics.py:  106: Constructing VRC val...
[11/07 13:08:16][INFO] kinetics.py:  162: Constructing VRC dataloader (size: 23 skip_rows 0) from /cluster/work/cvl/robocup/data/Debug_Dataset/small_val.csv 
[11/07 13:08:16][INFO] kinetics.py:  106: Constructing VRC train...
[11/07 13:08:16][INFO] kinetics.py:  162: Constructing VRC dataloader (size: 24 skip_rows 0) from /cluster/work/cvl/robocup/data/Debug_Dataset/small_train.csv 
[11/07 13:08:16][INFO] train_net.py:  635: Start epoch: 11
[11/07 13:08:29][INFO] logging.py:   99: json_stats: {"RAM": "41.66/501.50G", "_type": "train_epoch", "dt": 0.50238, "dt_data": 0.50238, "dt_net": 0.24182, "epoch": "11/300", "eta": "0:14:31", "gpu_mem": "5.93G", "grad_norm": 7.17628, "loss": 4.19507, "lr": 0.03683, "top1_err": 70.83333, "top5_err": 33.33333}
[11/07 13:08:29][INFO] train_net.py:  696: Epoch 10 takes 13.16s. Epochs from 10 to 10 take 13.16s in average and 13.16s in median.
[11/07 13:08:29][INFO] train_net.py:  702: For epoch 10, each iteraction takes 2.19s in average. From epoch 10 to 10, each iteraction takes 2.19s in average.
[11/07 13:08:33][INFO] logging.py:   99: json_stats: {"RAM": "41.66/501.50G", "_type": "train_epoch", "dt": 0.48572, "dt_data": 0.48572, "dt_net": 0.24226, "epoch": "12/300", "eta": "0:13:59", "gpu_mem": "5.93G", "grad_norm": 6.90232, "loss": 2.72070, "lr": 0.03931, "top1_err": 66.66667, "top5_err": 29.16667}
[11/07 13:08:33][INFO] train_net.py:  696: Epoch 11 takes 4.10s. Epochs from 10 to 11 take 8.63s in average and 8.63s in median.
[11/07 13:08:33][INFO] train_net.py:  702: For epoch 11, each iteraction takes 0.68s in average. From epoch 10 to 11, each iteraction takes 1.44s in average.
[11/07 13:08:37][INFO] logging.py:   99: json_stats: {"RAM": "39.75/501.50G", "_type": "train_epoch", "dt": 0.50738, "dt_data": 0.50738, "dt_net": 0.24170, "epoch": "13/300", "eta": "0:14:33", "gpu_mem": "5.93G", "grad_norm": 6.85902, "loss": 3.62543, "lr": 0.04178, "top1_err": 66.66667, "top5_err": 41.66667}
[11/07 13:08:37][INFO] train_net.py:  696: Epoch 12 takes 4.10s. Epochs from 10 to 12 take 7.12s in average and 4.10s in median.
[11/07 13:08:37][INFO] train_net.py:  702: For epoch 12, each iteraction takes 0.68s in average. From epoch 10 to 12, each iteraction takes 1.19s in average.
[11/07 13:08:41][INFO] logging.py:   99: json_stats: {"RAM": "41.63/501.50G", "_type": "train_epoch", "dt": 0.51557, "dt_data": 0.51557, "dt_net": 0.24257, "epoch": "14/300", "eta": "0:14:44", "gpu_mem": "5.93G", "grad_norm": 3.34955, "loss": 3.03947, "lr": 0.04426, "top1_err": 79.16667, "top5_err": 37.50000}
[11/07 13:08:41][INFO] train_net.py:  696: Epoch 13 takes 4.14s. Epochs from 10 to 13 take 6.38s in average and 4.12s in median.
[11/07 13:08:41][INFO] train_net.py:  702: For epoch 13, each iteraction takes 0.69s in average. From epoch 10 to 13, each iteraction takes 1.06s in average.
[11/07 13:08:46][INFO] logging.py:   99: json_stats: {"RAM": "41.67/501.50G", "_type": "train_epoch", "dt": 0.51004, "dt_data": 0.51004, "dt_net": 0.24266, "epoch": "15/300", "eta": "0:14:32", "gpu_mem": "5.93G", "grad_norm": 4.53318, "loss": 2.96269, "lr": 0.04674, "top1_err": 75.00000, "top5_err": 41.66667}
[11/07 13:08:46][INFO] train_net.py:  696: Epoch 14 takes 4.17s. Epochs from 10 to 14 take 5.93s in average and 4.14s in median.
[11/07 13:08:46][INFO] train_net.py:  702: For epoch 14, each iteraction takes 0.69s in average. From epoch 10 to 14, each iteraction takes 0.99s in average.
[11/07 13:08:50][INFO] logging.py:   99: json_stats: {"RAM": "41.68/501.50G", "_type": "train_epoch", "dt": 0.49298, "dt_data": 0.49298, "dt_net": 0.24210, "epoch": "16/300", "eta": "0:13:59", "gpu_mem": "5.93G", "grad_norm": 4.67242, "loss": 3.01499, "lr": 0.04921, "top1_err": 87.50000, "top5_err": 33.33333}
[11/07 13:08:50][INFO] train_net.py:  696: Epoch 15 takes 4.04s. Epochs from 10 to 15 take 5.62s in average and 4.12s in median.
[11/07 13:08:50][INFO] train_net.py:  702: For epoch 15, each iteraction takes 0.67s in average. From epoch 10 to 15, each iteraction takes 0.94s in average.
[11/07 13:08:54][INFO] logging.py:   99: json_stats: {"RAM": "41.69/501.50G", "_type": "train_epoch", "dt": 0.47658, "dt_data": 0.47658, "dt_net": 0.24278, "epoch": "17/300", "eta": "0:13:29", "gpu_mem": "5.93G", "grad_norm": 2.82715, "loss": 3.12050, "lr": 0.05169, "top1_err": 75.00000, "top5_err": 37.50000}
[11/07 13:08:54][INFO] train_net.py:  696: Epoch 16 takes 4.12s. Epochs from 10 to 16 take 5.40s in average and 4.12s in median.
[11/07 13:08:54][INFO] train_net.py:  702: For epoch 16, each iteraction takes 0.69s in average. From epoch 10 to 16, each iteraction takes 0.90s in average.
[11/07 13:08:58][INFO] logging.py:   99: json_stats: {"RAM": "41.67/501.50G", "_type": "train_epoch", "dt": 0.50083, "dt_data": 0.50083, "dt_net": 0.24237, "epoch": "18/300", "eta": "0:14:07", "gpu_mem": "5.93G", "grad_norm": 4.53830, "loss": 2.41877, "lr": 0.05417, "top1_err": 66.66667, "top5_err": 25.00000}
[11/07 13:08:58][INFO] train_net.py:  696: Epoch 17 takes 4.04s. Epochs from 10 to 17 take 5.23s in average and 4.11s in median.
[11/07 13:08:58][INFO] train_net.py:  702: For epoch 17, each iteraction takes 0.67s in average. From epoch 10 to 17, each iteraction takes 0.87s in average.
[11/07 13:09:02][INFO] logging.py:   99: json_stats: {"RAM": "41.68/501.50G", "_type": "train_epoch", "dt": 0.50739, "dt_data": 0.50739, "dt_net": 0.24288, "epoch": "19/300", "eta": "0:14:15", "gpu_mem": "5.93G", "grad_norm": 2.97081, "loss": 2.09944, "lr": 0.05664, "top1_err": 62.50000, "top5_err": 16.66667}
[11/07 13:09:02][INFO] train_net.py:  696: Epoch 18 takes 4.07s. Epochs from 10 to 18 take 5.10s in average and 4.10s in median.
[11/07 13:09:02][INFO] train_net.py:  702: For epoch 18, each iteraction takes 0.68s in average. From epoch 10 to 18, each iteraction takes 0.85s in average.
[11/07 13:09:06][INFO] logging.py:   99: json_stats: {"RAM": "41.68/501.50G", "_type": "train_epoch", "dt": 0.49748, "dt_data": 0.49748, "dt_net": 0.24280, "epoch": "20/300", "eta": "0:13:55", "gpu_mem": "5.93G", "grad_norm": 2.53153, "loss": 1.83544, "lr": 0.05912, "top1_err": 58.33333, "top5_err": 33.33333}
[11/07 13:09:06][INFO] train_net.py:  696: Epoch 19 takes 4.14s. Epochs from 10 to 19 take 5.01s in average and 4.11s in median.
[11/07 13:09:06][INFO] train_net.py:  702: For epoch 19, each iteraction takes 0.69s in average. From epoch 10 to 19, each iteraction takes 0.83s in average.
[11/07 13:09:06][INFO] precise_bn.py:  129: Computing precise BN statistics for 84 BN layers ...
[11/07 13:09:14][INFO] logging.py:   99: json_stats: {"RAM": "39.81/501.50G", "_type": "val_epoch", "epoch": "20/300", "gpu_mem": "5.93G", "min_top1_err": 73.91304, "min_top5_err": 34.78261, "time_diff": 0.38280, "top1_err": 73.91304, "top5_err": 34.78261}
[11/07 13:09:18][INFO] logging.py:   99: json_stats: {"RAM": "41.69/501.50G", "_type": "train_epoch", "dt": 0.51606, "dt_data": 0.51606, "dt_net": 0.24254, "epoch": "21/300", "eta": "0:14:23", "gpu_mem": "5.93G", "grad_norm": 3.40156, "loss": 2.90235, "lr": 0.06159, "top1_err": 79.16667, "top5_err": 37.50000}
[11/07 13:09:18][INFO] train_net.py:  696: Epoch 20 takes 4.03s. Epochs from 10 to 20 take 4.92s in average and 4.10s in median.
[11/07 13:09:18][INFO] train_net.py:  702: For epoch 20, each iteraction takes 0.67s in average. From epoch 10 to 20, each iteraction takes 0.82s in average.
[11/07 13:09:22][INFO] logging.py:   99: json_stats: {"RAM": "41.67/501.50G", "_type": "train_epoch", "dt": 0.51161, "dt_data": 0.51161, "dt_net": 0.24261, "epoch": "22/300", "eta": "0:14:13", "gpu_mem": "5.93G", "grad_norm": 2.32908, "loss": 2.28456, "lr": 0.06407, "top1_err": 62.50000, "top5_err": 33.33333}
[11/07 13:09:22][INFO] train_net.py:  696: Epoch 21 takes 4.08s. Epochs from 10 to 21 take 4.85s in average and 4.10s in median.
[11/07 13:09:22][INFO] train_net.py:  702: For epoch 21, each iteraction takes 0.68s in average. From epoch 10 to 21, each iteraction takes 0.81s in average.
[11/07 13:09:26][INFO] logging.py:   99: json_stats: {"RAM": "41.71/501.50G", "_type": "train_epoch", "dt": 0.49853, "dt_data": 0.49853, "dt_net": 0.24276, "epoch": "23/300", "eta": "0:13:48", "gpu_mem": "5.93G", "grad_norm": 2.12506, "loss": 2.28551, "lr": 0.06655, "top1_err": 66.66667, "top5_err": 37.50000}
[11/07 13:09:26][INFO] train_net.py:  696: Epoch 22 takes 4.01s. Epochs from 10 to 22 take 4.78s in average and 4.10s in median.
[11/07 13:09:26][INFO] train_net.py:  702: For epoch 22, each iteraction takes 0.67s in average. From epoch 10 to 22, each iteraction takes 0.80s in average.
[11/07 13:09:30][INFO] logging.py:   99: json_stats: {"RAM": "41.68/501.50G", "_type": "train_epoch", "dt": 0.49940, "dt_data": 0.49940, "dt_net": 0.24359, "epoch": "24/300", "eta": "0:13:46", "gpu_mem": "5.93G", "grad_norm": 3.00737, "loss": 2.05185, "lr": 0.06902, "top1_err": 66.66667, "top5_err": 25.00000}
[11/07 13:09:30][INFO] train_net.py:  696: Epoch 23 takes 4.10s. Epochs from 10 to 23 take 4.74s in average and 4.10s in median.
[11/07 13:09:30][INFO] train_net.py:  702: For epoch 23, each iteraction takes 0.68s in average. From epoch 10 to 23, each iteraction takes 0.79s in average.
[11/07 13:09:34][INFO] logging.py:   99: json_stats: {"RAM": "41.67/501.50G", "_type": "train_epoch", "dt": 0.50158, "dt_data": 0.50158, "dt_net": 0.24229, "epoch": "25/300", "eta": "0:13:47", "gpu_mem": "5.93G", "grad_norm": 3.03749, "loss": 2.26961, "lr": 0.07150, "top1_err": 70.83333, "top5_err": 33.33333}
[11/07 13:09:34][INFO] train_net.py:  696: Epoch 24 takes 4.04s. Epochs from 10 to 24 take 4.69s in average and 4.10s in median.
[11/07 13:09:34][INFO] train_net.py:  702: For epoch 24, each iteraction takes 0.67s in average. From epoch 10 to 24, each iteraction takes 0.78s in average.
[11/07 13:09:38][INFO] logging.py:   99: json_stats: {"RAM": "41.67/501.50G", "_type": "train_epoch", "dt": 0.50269, "dt_data": 0.50269, "dt_net": 0.24263, "epoch": "26/300", "eta": "0:13:46", "gpu_mem": "5.93G", "grad_norm": 1.33748, "loss": 2.32160, "lr": 0.07398, "top1_err": 75.00000, "top5_err": 33.33333}
[11/07 13:09:38][INFO] train_net.py:  696: Epoch 25 takes 4.10s. Epochs from 10 to 25 take 4.65s in average and 4.10s in median.
[11/07 13:09:38][INFO] train_net.py:  702: For epoch 25, each iteraction takes 0.68s in average. From epoch 10 to 25, each iteraction takes 0.78s in average.
[11/07 13:09:42][INFO] logging.py:   99: json_stats: {"RAM": "41.67/501.50G", "_type": "train_epoch", "dt": 0.50911, "dt_data": 0.50910, "dt_net": 0.24281, "epoch": "27/300", "eta": "0:13:53", "gpu_mem": "5.93G", "grad_norm": 3.03661, "loss": 2.17125, "lr": 0.07645, "top1_err": 62.50000, "top5_err": 33.33333}
[11/07 13:09:42][INFO] train_net.py:  696: Epoch 26 takes 4.12s. Epochs from 10 to 26 take 4.62s in average and 4.10s in median.
[11/07 13:09:42][INFO] train_net.py:  702: For epoch 26, each iteraction takes 0.69s in average. From epoch 10 to 26, each iteraction takes 0.77s in average.
[11/07 13:09:47][INFO] logging.py:   99: json_stats: {"RAM": "41.65/501.50G", "_type": "train_epoch", "dt": 0.49016, "dt_data": 0.49016, "dt_net": 0.24274, "epoch": "28/300", "eta": "0:13:19", "gpu_mem": "5.93G", "grad_norm": 2.40152, "loss": 2.02072, "lr": 0.07893, "top1_err": 75.00000, "top5_err": 16.66667}
[11/07 13:09:47][INFO] train_net.py:  696: Epoch 27 takes 4.06s. Epochs from 10 to 27 take 4.59s in average and 4.10s in median.
[11/07 13:09:47][INFO] train_net.py:  702: For epoch 27, each iteraction takes 0.68s in average. From epoch 10 to 27, each iteraction takes 0.76s in average.
[11/07 13:09:51][INFO] logging.py:   99: json_stats: {"RAM": "39.84/501.50G", "_type": "train_epoch", "dt": 0.51172, "dt_data": 0.51172, "dt_net": 0.24237, "epoch": "29/300", "eta": "0:13:51", "gpu_mem": "5.93G", "grad_norm": 4.84773, "loss": 2.19542, "lr": 0.08141, "top1_err": 70.83333, "top5_err": 33.33333}
[11/07 13:09:51][INFO] train_net.py:  696: Epoch 28 takes 4.05s. Epochs from 10 to 28 take 4.56s in average and 4.10s in median.
[11/07 13:09:51][INFO] train_net.py:  702: For epoch 28, each iteraction takes 0.67s in average. From epoch 10 to 28, each iteraction takes 0.76s in average.
[11/07 13:09:55][INFO] logging.py:   99: json_stats: {"RAM": "41.64/501.50G", "_type": "train_epoch", "dt": 0.51583, "dt_data": 0.51583, "dt_net": 0.24334, "epoch": "30/300", "eta": "0:13:55", "gpu_mem": "5.93G", "grad_norm": 2.59019, "loss": 2.16508, "lr": 0.08388, "top1_err": 70.83333, "top5_err": 33.33333}
[11/07 13:09:55][INFO] train_net.py:  696: Epoch 29 takes 4.14s. Epochs from 10 to 29 take 4.54s in average and 4.10s in median.
[11/07 13:09:55][INFO] train_net.py:  702: For epoch 29, each iteraction takes 0.69s in average. From epoch 10 to 29, each iteraction takes 0.76s in average.
[11/07 13:09:55][INFO] precise_bn.py:  129: Computing precise BN statistics for 84 BN layers ...
[11/07 13:10:01][INFO] logging.py:   99: json_stats: {"RAM": "41.71/501.50G", "_type": "val_epoch", "epoch": "30/300", "gpu_mem": "5.93G", "min_top1_err": 73.91304, "min_top5_err": 34.78261, "time_diff": 0.37423, "top1_err": 100.00000, "top5_err": 56.52174}
[11/07 13:10:05][INFO] logging.py:   99: json_stats: {"RAM": "41.68/501.50G", "_type": "train_epoch", "dt": 0.51919, "dt_data": 0.51919, "dt_net": 0.24276, "epoch": "31/300", "eta": "0:13:57", "gpu_mem": "5.93G", "grad_norm": 1.11288, "loss": 1.90069, "lr": 0.08636, "top1_err": 58.33333, "top5_err": 25.00000}
[11/07 13:10:05][INFO] train_net.py:  696: Epoch 30 takes 4.16s. Epochs from 10 to 30 take 4.52s in average and 4.10s in median.
[11/07 13:10:05][INFO] train_net.py:  702: For epoch 30, each iteraction takes 0.69s in average. From epoch 10 to 30, each iteraction takes 0.75s in average.
[11/07 13:10:09][INFO] logging.py:   99: json_stats: {"RAM": "41.68/501.50G", "_type": "train_epoch", "dt": 0.50641, "dt_data": 0.50641, "dt_net": 0.24245, "epoch": "32/300", "eta": "0:13:34", "gpu_mem": "5.93G", "grad_norm": 5.59701, "loss": 2.57560, "lr": 0.08884, "top1_err": 66.66667, "top5_err": 29.16667}
[11/07 13:10:09][INFO] train_net.py:  696: Epoch 31 takes 4.07s. Epochs from 10 to 31 take 4.50s in average and 4.10s in median.
[11/07 13:10:09][INFO] train_net.py:  702: For epoch 31, each iteraction takes 0.68s in average. From epoch 10 to 31, each iteraction takes 0.75s in average.
[11/07 13:10:13][INFO] logging.py:   99: json_stats: {"RAM": "41.70/501.50G", "_type": "train_epoch", "dt": 0.50385, "dt_data": 0.50385, "dt_net": 0.24275, "epoch": "33/300", "eta": "0:13:27", "gpu_mem": "5.93G", "grad_norm": 2.67428, "loss": 2.26236, "lr": 0.09131, "top1_err": 62.50000, "top5_err": 29.16667}
[11/07 13:10:13][INFO] train_net.py:  696: Epoch 32 takes 4.13s. Epochs from 10 to 32 take 4.49s in average and 4.10s in median.
[11/07 13:10:13][INFO] train_net.py:  702: For epoch 32, each iteraction takes 0.69s in average. From epoch 10 to 32, each iteraction takes 0.75s in average.
[11/07 13:10:17][INFO] logging.py:   99: json_stats: {"RAM": "41.69/501.50G", "_type": "train_epoch", "dt": 0.49953, "dt_data": 0.49953, "dt_net": 0.24290, "epoch": "34/300", "eta": "0:13:17", "gpu_mem": "5.93G", "grad_norm": 1.38252, "loss": 2.10255, "lr": 0.09379, "top1_err": 66.66667, "top5_err": 25.00000}
[11/07 13:10:17][INFO] train_net.py:  696: Epoch 33 takes 4.09s. Epochs from 10 to 33 take 4.47s in average and 4.10s in median.
[11/07 13:10:17][INFO] train_net.py:  702: For epoch 33, each iteraction takes 0.68s in average. From epoch 10 to 33, each iteraction takes 0.74s in average.
[11/07 13:10:21][INFO] logging.py:   99: json_stats: {"RAM": "41.69/501.50G", "_type": "train_epoch", "dt": 0.48982, "dt_data": 0.48982, "dt_net": 0.24286, "epoch": "35/300", "eta": "0:12:58", "gpu_mem": "5.93G", "grad_norm": 1.02507, "loss": 1.87822, "lr": 0.09627, "top1_err": 54.16667, "top5_err": 33.33333}
[11/07 13:10:21][INFO] train_net.py:  696: Epoch 34 takes 4.08s. Epochs from 10 to 34 take 4.45s in average and 4.10s in median.
[11/07 13:10:21][INFO] train_net.py:  702: For epoch 34, each iteraction takes 0.68s in average. From epoch 10 to 34, each iteraction takes 0.74s in average.
[11/07 13:10:25][INFO] logging.py:   99: json_stats: {"RAM": "38.73/501.50G", "_type": "train_epoch", "dt": 0.51665, "dt_data": 0.51665, "dt_net": 0.24281, "epoch": "36/300", "eta": "0:13:38", "gpu_mem": "5.93G", "grad_norm": 3.54989, "loss": 2.06838, "lr": 0.09652, "top1_err": 58.33333, "top5_err": 20.83333}
[11/07 13:10:25][INFO] train_net.py:  696: Epoch 35 takes 4.08s. Epochs from 10 to 35 take 4.44s in average and 4.09s in median.
[11/07 13:10:25][INFO] train_net.py:  702: For epoch 35, each iteraction takes 0.68s in average. From epoch 10 to 35, each iteraction takes 0.74s in average.
[11/07 13:10:30][INFO] logging.py:   99: json_stats: {"RAM": "41.15/501.50G", "_type": "train_epoch", "dt": 0.49772, "dt_data": 0.49772, "dt_net": 0.24312, "epoch": "37/300", "eta": "0:13:05", "gpu_mem": "5.93G", "grad_norm": 1.67692, "loss": 2.43600, "lr": 0.09633, "top1_err": 79.16667, "top5_err": 25.00000}
[11/07 13:10:30][INFO] train_net.py:  696: Epoch 36 takes 4.16s. Epochs from 10 to 36 take 4.43s in average and 4.10s in median.
[11/07 13:10:30][INFO] train_net.py:  702: For epoch 36, each iteraction takes 0.69s in average. From epoch 10 to 36, each iteraction takes 0.74s in average.
[11/07 13:10:34][INFO] logging.py:   99: json_stats: {"RAM": "41.72/501.50G", "_type": "train_epoch", "dt": 0.51423, "dt_data": 0.51423, "dt_net": 0.24400, "epoch": "38/300", "eta": "0:13:28", "gpu_mem": "5.93G", "grad_norm": 3.16475, "loss": 1.93971, "lr": 0.09613, "top1_err": 62.50000, "top5_err": 25.00000}
[11/07 13:10:34][INFO] train_net.py:  696: Epoch 37 takes 4.21s. Epochs from 10 to 37 take 4.42s in average and 4.10s in median.
[11/07 13:10:34][INFO] train_net.py:  702: For epoch 37, each iteraction takes 0.70s in average. From epoch 10 to 37, each iteraction takes 0.74s in average.
[11/07 13:10:38][INFO] logging.py:   99: json_stats: {"RAM": "41.69/501.50G", "_type": "train_epoch", "dt": 0.49376, "dt_data": 0.49376, "dt_net": 0.24293, "epoch": "39/300", "eta": "0:12:53", "gpu_mem": "5.93G", "grad_norm": 2.41801, "loss": 2.26226, "lr": 0.09592, "top1_err": 75.00000, "top5_err": 33.33333}
[11/07 13:10:38][INFO] train_net.py:  696: Epoch 38 takes 4.04s. Epochs from 10 to 38 take 4.41s in average and 4.10s in median.
[11/07 13:10:38][INFO] train_net.py:  702: For epoch 38, each iteraction takes 0.67s in average. From epoch 10 to 38, each iteraction takes 0.73s in average.
[11/07 13:10:42][INFO] logging.py:   99: json_stats: {"RAM": "41.71/501.50G", "_type": "train_epoch", "dt": 0.50242, "dt_data": 0.50242, "dt_net": 0.24363, "epoch": "40/300", "eta": "0:13:03", "gpu_mem": "5.93G", "grad_norm": 1.69444, "loss": 2.24672, "lr": 0.09571, "top1_err": 66.66667, "top5_err": 25.00000}
[11/07 13:10:42][INFO] train_net.py:  696: Epoch 39 takes 4.07s. Epochs from 10 to 39 take 4.40s in average and 4.09s in median.
[11/07 13:10:42][INFO] train_net.py:  702: For epoch 39, each iteraction takes 0.68s in average. From epoch 10 to 39, each iteraction takes 0.73s in average.
[11/07 13:10:42][INFO] precise_bn.py:  129: Computing precise BN statistics for 84 BN layers ...
[11/07 13:10:48][INFO] logging.py:   99: json_stats: {"RAM": "41.70/501.50G", "_type": "val_epoch", "epoch": "40/300", "gpu_mem": "5.93G", "min_top1_err": 73.91304, "min_top5_err": 34.78261, "time_diff": 0.36795, "top1_err": 91.30435, "top5_err": 52.17391}
[11/07 13:10:52][INFO] logging.py:   99: json_stats: {"RAM": "41.65/501.50G", "_type": "train_epoch", "dt": 0.49931, "dt_data": 0.49931, "dt_net": 0.24318, "epoch": "41/300", "eta": "0:12:55", "gpu_mem": "5.93G", "grad_norm": 1.05857, "loss": 1.99558, "lr": 0.09550, "top1_err": 75.00000, "top5_err": 29.16667}
[11/07 13:10:52][INFO] train_net.py:  696: Epoch 40 takes 4.07s. Epochs from 10 to 40 take 4.39s in average and 4.09s in median.
[11/07 13:10:52][INFO] train_net.py:  702: For epoch 40, each iteraction takes 0.68s in average. From epoch 10 to 40, each iteraction takes 0.73s in average.
[11/07 13:10:56][INFO] logging.py:   99: json_stats: {"RAM": "41.64/501.50G", "_type": "train_epoch", "dt": 0.50449, "dt_data": 0.50449, "dt_net": 0.24301, "epoch": "42/300", "eta": "0:13:00", "gpu_mem": "5.93G", "grad_norm": 0.78392, "loss": 2.04234, "lr": 0.09528, "top1_err": 70.83333, "top5_err": 33.33333}
[11/07 13:10:56][INFO] train_net.py:  696: Epoch 41 takes 4.12s. Epochs from 10 to 41 take 4.38s in average and 4.09s in median.
[11/07 13:10:56][INFO] train_net.py:  702: For epoch 41, each iteraction takes 0.69s in average. From epoch 10 to 41, each iteraction takes 0.73s in average.
[11/07 13:11:00][INFO] logging.py:   99: json_stats: {"RAM": "41.63/501.50G", "_type": "train_epoch", "dt": 0.49149, "dt_data": 0.49149, "dt_net": 0.24278, "epoch": "43/300", "eta": "0:12:37", "gpu_mem": "5.93G", "grad_norm": 1.05542, "loss": 1.86083, "lr": 0.09505, "top1_err": 66.66667, "top5_err": 25.00000}
[11/07 13:11:00][INFO] train_net.py:  696: Epoch 42 takes 4.10s. Epochs from 10 to 42 take 4.37s in average and 4.10s in median.
[11/07 13:11:00][INFO] train_net.py:  702: For epoch 42, each iteraction takes 0.68s in average. From epoch 10 to 42, each iteraction takes 0.73s in average.
[11/07 13:11:04][INFO] logging.py:   99: json_stats: {"RAM": "39.94/501.50G", "_type": "train_epoch", "dt": 0.56111, "dt_data": 0.56111, "dt_net": 0.24265, "epoch": "44/300", "eta": "0:14:21", "gpu_mem": "5.93G", "grad_norm": 1.74910, "loss": 3.21019, "lr": 0.09482, "top1_err": 58.33333, "top5_err": 33.33333}
[11/07 13:11:04][INFO] train_net.py:  696: Epoch 43 takes 4.13s. Epochs from 10 to 43 take 4.36s in average and 4.10s in median.
[11/07 13:11:04][INFO] train_net.py:  702: For epoch 43, each iteraction takes 0.69s in average. From epoch 10 to 43, each iteraction takes 0.73s in average.
[11/07 13:11:08][INFO] logging.py:   99: json_stats: {"RAM": "41.63/501.50G", "_type": "train_epoch", "dt": 0.50157, "dt_data": 0.50157, "dt_net": 0.24272, "epoch": "45/300", "eta": "0:12:47", "gpu_mem": "5.93G", "grad_norm": 2.35865, "loss": 2.21891, "lr": 0.09459, "top1_err": 66.66667, "top5_err": 25.00000}
[11/07 13:11:08][INFO] train_net.py:  696: Epoch 44 takes 4.09s. Epochs from 10 to 44 take 4.35s in average and 4.10s in median.
[11/07 13:11:08][INFO] train_net.py:  702: For epoch 44, each iteraction takes 0.68s in average. From epoch 10 to 44, each iteraction takes 0.73s in average.
[11/07 13:11:12][INFO] logging.py:   99: json_stats: {"RAM": "41.63/501.50G", "_type": "train_epoch", "dt": 0.49875, "dt_data": 0.49875, "dt_net": 0.24354, "epoch": "46/300", "eta": "0:12:40", "gpu_mem": "5.93G", "grad_norm": 2.19506, "loss": 2.17165, "lr": 0.09435, "top1_err": 66.66667, "top5_err": 25.00000}
[11/07 13:11:12][INFO] train_net.py:  696: Epoch 45 takes 4.07s. Epochs from 10 to 45 take 4.35s in average and 4.10s in median.
[11/07 13:11:12][INFO] train_net.py:  702: For epoch 45, each iteraction takes 0.68s in average. From epoch 10 to 45, each iteraction takes 0.72s in average.
[11/07 13:11:16][INFO] logging.py:   99: json_stats: {"RAM": "41.63/501.50G", "_type": "train_epoch", "dt": 0.51082, "dt_data": 0.51082, "dt_net": 0.24235, "epoch": "47/300", "eta": "0:12:55", "gpu_mem": "5.93G", "grad_norm": 1.34848, "loss": 2.04021, "lr": 0.09411, "top1_err": 62.50000, "top5_err": 25.00000}
[11/07 13:11:16][INFO] train_net.py:  696: Epoch 46 takes 4.03s. Epochs from 10 to 46 take 4.34s in average and 4.09s in median.
[11/07 13:11:16][INFO] train_net.py:  702: For epoch 46, each iteraction takes 0.67s in average. From epoch 10 to 46, each iteraction takes 0.72s in average.
[11/07 13:11:20][INFO] logging.py:   99: json_stats: {"RAM": "41.64/501.50G", "_type": "train_epoch", "dt": 0.51090, "dt_data": 0.51090, "dt_net": 0.24372, "epoch": "48/300", "eta": "0:12:52", "gpu_mem": "5.93G", "grad_norm": 0.82318, "loss": 2.10727, "lr": 0.09386, "top1_err": 62.50000, "top5_err": 20.83333}
[11/07 13:11:20][INFO] train_net.py:  696: Epoch 47 takes 4.01s. Epochs from 10 to 47 take 4.33s in average and 4.09s in median.
[11/07 13:11:20][INFO] train_net.py:  702: For epoch 47, each iteraction takes 0.67s in average. From epoch 10 to 47, each iteraction takes 0.72s in average.
[11/07 13:11:24][INFO] logging.py:   99: json_stats: {"RAM": "41.63/501.50G", "_type": "train_epoch", "dt": 0.50242, "dt_data": 0.50242, "dt_net": 0.24292, "epoch": "49/300", "eta": "0:12:36", "gpu_mem": "5.93G", "grad_norm": 0.63017, "loss": 2.03750, "lr": 0.09360, "top1_err": 66.66667, "top5_err": 20.83333}
[11/07 13:11:24][INFO] train_net.py:  696: Epoch 48 takes 4.07s. Epochs from 10 to 48 take 4.32s in average and 4.09s in median.
[11/07 13:11:24][INFO] train_net.py:  702: For epoch 48, each iteraction takes 0.68s in average. From epoch 10 to 48, each iteraction takes 0.72s in average.
[11/07 13:11:28][INFO] logging.py:   99: json_stats: {"RAM": "41.63/501.50G", "_type": "train_epoch", "dt": 0.50034, "dt_data": 0.50034, "dt_net": 0.24343, "epoch": "50/300", "eta": "0:12:30", "gpu_mem": "5.93G", "grad_norm": 0.59620, "loss": 1.94962, "lr": 0.09334, "top1_err": 62.50000, "top5_err": 20.83333}
[11/07 13:11:28][INFO] train_net.py:  696: Epoch 49 takes 4.06s. Epochs from 10 to 49 take 4.32s in average and 4.09s in median.
[11/07 13:11:28][INFO] train_net.py:  702: For epoch 49, each iteraction takes 0.68s in average. From epoch 10 to 49, each iteraction takes 0.72s in average.
[11/07 13:11:28][INFO] precise_bn.py:  129: Computing precise BN statistics for 84 BN layers ...
[11/07 13:11:34][INFO] logging.py:   99: json_stats: {"RAM": "41.63/501.50G", "_type": "val_epoch", "epoch": "50/300", "gpu_mem": "5.93G", "min_top1_err": 73.91304, "min_top5_err": 34.78261, "time_diff": 0.37285, "top1_err": 95.65217, "top5_err": 39.13043}
[11/07 13:11:38][INFO] logging.py:   99: json_stats: {"RAM": "41.61/501.50G", "_type": "train_epoch", "dt": 0.48866, "dt_data": 0.48866, "dt_net": 0.24280, "epoch": "51/300", "eta": "0:12:09", "gpu_mem": "5.93G", "grad_norm": 0.79051, "loss": 1.98335, "lr": 0.09308, "top1_err": 62.50000, "top5_err": 20.83333}
[11/07 13:11:38][INFO] train_net.py:  696: Epoch 50 takes 4.03s. Epochs from 10 to 50 take 4.31s in average and 4.08s in median.
[11/07 13:11:38][INFO] train_net.py:  702: For epoch 50, each iteraction takes 0.67s in average. From epoch 10 to 50, each iteraction takes 0.72s in average.
[11/07 13:11:42][INFO] logging.py:   99: json_stats: {"RAM": "41.05/501.50G", "_type": "train_epoch", "dt": 0.53435, "dt_data": 0.53434, "dt_net": 0.24264, "epoch": "52/300", "eta": "0:13:15", "gpu_mem": "5.93G", "grad_norm": 0.86445, "loss": 1.98335, "lr": 0.09281, "top1_err": 62.50000, "top5_err": 29.16667}
[11/07 13:11:42][INFO] train_net.py:  696: Epoch 51 takes 4.26s. Epochs from 10 to 51 take 4.31s in average and 4.09s in median.
[11/07 13:11:42][INFO] train_net.py:  702: For epoch 51, each iteraction takes 0.71s in average. From epoch 10 to 51, each iteraction takes 0.72s in average.
[11/07 13:11:46][INFO] logging.py:   99: json_stats: {"RAM": "41.64/501.50G", "_type": "train_epoch", "dt": 0.53229, "dt_data": 0.53229, "dt_net": 0.24320, "epoch": "53/300", "eta": "0:13:08", "gpu_mem": "5.93G", "grad_norm": 0.98712, "loss": 1.96513, "lr": 0.09254, "top1_err": 62.50000, "top5_err": 20.83333}
[11/07 13:11:46][INFO] train_net.py:  696: Epoch 52 takes 4.13s. Epochs from 10 to 52 take 4.30s in average and 4.09s in median.
[11/07 13:11:46][INFO] train_net.py:  702: For epoch 52, each iteraction takes 0.69s in average. From epoch 10 to 52, each iteraction takes 0.72s in average.
[11/07 13:11:50][INFO] logging.py:   99: json_stats: {"RAM": "41.65/501.50G", "_type": "train_epoch", "dt": 0.51099, "dt_data": 0.51099, "dt_net": 0.24350, "epoch": "54/300", "eta": "0:12:34", "gpu_mem": "5.93G", "grad_norm": 0.44154, "loss": 1.98525, "lr": 0.09226, "top1_err": 62.50000, "top5_err": 20.83333}
[11/07 13:11:50][INFO] train_net.py:  696: Epoch 53 takes 4.08s. Epochs from 10 to 53 take 4.30s in average and 4.09s in median.
[11/07 13:11:50][INFO] train_net.py:  702: For epoch 53, each iteraction takes 0.68s in average. From epoch 10 to 53, each iteraction takes 0.72s in average.
[11/07 13:11:54][INFO] logging.py:   99: json_stats: {"RAM": "41.65/501.50G", "_type": "train_epoch", "dt": 0.50305, "dt_data": 0.50305, "dt_net": 0.24322, "epoch": "55/300", "eta": "0:12:19", "gpu_mem": "5.93G", "grad_norm": 0.38701, "loss": 2.01772, "lr": 0.09198, "top1_err": 62.50000, "top5_err": 20.83333}
[11/07 13:11:54][INFO] train_net.py:  696: Epoch 54 takes 4.14s. Epochs from 10 to 54 take 4.30s in average and 4.09s in median.
[11/07 13:11:54][INFO] train_net.py:  702: For epoch 54, each iteraction takes 0.69s in average. From epoch 10 to 54, each iteraction takes 0.72s in average.
[11/07 13:11:59][INFO] logging.py:   99: json_stats: {"RAM": "41.65/501.50G", "_type": "train_epoch", "dt": 0.49104, "dt_data": 0.49103, "dt_net": 0.24367, "epoch": "56/300", "eta": "0:11:58", "gpu_mem": "5.93G", "grad_norm": 0.56352, "loss": 2.00456, "lr": 0.09169, "top1_err": 62.50000, "top5_err": 20.83333}
[11/07 13:11:59][INFO] train_net.py:  696: Epoch 55 takes 4.07s. Epochs from 10 to 55 take 4.29s in average and 4.09s in median.
[11/07 13:11:59][INFO] train_net.py:  702: For epoch 55, each iteraction takes 0.68s in average. From epoch 10 to 55, each iteraction takes 0.72s in average.
[11/07 13:12:03][INFO] logging.py:   99: json_stats: {"RAM": "41.65/501.50G", "_type": "train_epoch", "dt": 0.50969, "dt_data": 0.50969, "dt_net": 0.24305, "epoch": "57/300", "eta": "0:12:23", "gpu_mem": "5.93G", "grad_norm": 0.47914, "loss": 1.95250, "lr": 0.09140, "top1_err": 62.50000, "top5_err": 29.16667}
[11/07 13:12:03][INFO] train_net.py:  696: Epoch 56 takes 4.29s. Epochs from 10 to 56 take 4.29s in average and 4.09s in median.
[11/07 13:12:03][INFO] train_net.py:  702: For epoch 56, each iteraction takes 0.72s in average. From epoch 10 to 56, each iteraction takes 0.72s in average.
[11/07 13:12:07][INFO] logging.py:   99: json_stats: {"RAM": "41.65/501.50G", "_type": "train_epoch", "dt": 0.51031, "dt_data": 0.51031, "dt_net": 0.24266, "epoch": "58/300", "eta": "0:12:20", "gpu_mem": "5.93G", "grad_norm": 0.61522, "loss": 1.96631, "lr": 0.09111, "top1_err": 62.50000, "top5_err": 16.66667}
[11/07 13:12:07][INFO] train_net.py:  696: Epoch 57 takes 4.12s. Epochs from 10 to 57 take 4.29s in average and 4.09s in median.
[11/07 13:12:07][INFO] train_net.py:  702: For epoch 57, each iteraction takes 0.69s in average. From epoch 10 to 57, each iteraction takes 0.71s in average.
[11/07 13:12:11][INFO] logging.py:   99: json_stats: {"RAM": "41.64/501.50G", "_type": "train_epoch", "dt": 0.50352, "dt_data": 0.50352, "dt_net": 0.24297, "epoch": "59/300", "eta": "0:12:08", "gpu_mem": "5.93G", "grad_norm": 0.43050, "loss": 2.02467, "lr": 0.09081, "top1_err": 70.83333, "top5_err": 25.00000}
[11/07 13:12:11][INFO] train_net.py:  696: Epoch 58 takes 4.05s. Epochs from 10 to 58 take 4.28s in average and 4.09s in median.
[11/07 13:12:11][INFO] train_net.py:  702: For epoch 58, each iteraction takes 0.67s in average. From epoch 10 to 58, each iteraction takes 0.71s in average.
[11/07 13:12:15][INFO] logging.py:   99: json_stats: {"RAM": "38.90/501.50G", "_type": "train_epoch", "dt": 0.52729, "dt_data": 0.52729, "dt_net": 0.24287, "epoch": "60/300", "eta": "0:12:39", "gpu_mem": "5.93G", "grad_norm": 0.76026, "loss": 2.04290, "lr": 0.09050, "top1_err": 62.50000, "top5_err": 37.50000}
[11/07 13:12:15][INFO] train_net.py:  696: Epoch 59 takes 4.08s. Epochs from 10 to 59 take 4.28s in average and 4.09s in median.
[11/07 13:12:15][INFO] train_net.py:  702: For epoch 59, each iteraction takes 0.68s in average. From epoch 10 to 59, each iteraction takes 0.71s in average.
[11/07 13:12:15][INFO] precise_bn.py:  129: Computing precise BN statistics for 84 BN layers ...
[11/07 13:12:21][INFO] logging.py:   99: json_stats: {"RAM": "41.65/501.50G", "_type": "val_epoch", "epoch": "60/300", "gpu_mem": "5.93G", "min_top1_err": 73.91304, "min_top5_err": 34.78261, "time_diff": 0.38033, "top1_err": 95.65217, "top5_err": 39.13043}
[11/07 13:12:25][INFO] logging.py:   99: json_stats: {"RAM": "41.64/501.50G", "_type": "train_epoch", "dt": 0.51588, "dt_data": 0.51587, "dt_net": 0.24270, "epoch": "61/300", "eta": "0:12:19", "gpu_mem": "5.93G", "grad_norm": 0.57594, "loss": 1.97643, "lr": 0.09019, "top1_err": 62.50000, "top5_err": 29.16667}
[11/07 13:12:25][INFO] train_net.py:  696: Epoch 60 takes 4.12s. Epochs from 10 to 60 take 4.27s in average and 4.09s in median.
[11/07 13:12:25][INFO] train_net.py:  702: For epoch 60, each iteraction takes 0.69s in average. From epoch 10 to 60, each iteraction takes 0.71s in average.
[11/07 13:12:29][INFO] logging.py:   99: json_stats: {"RAM": "41.64/501.50G", "_type": "train_epoch", "dt": 0.50188, "dt_data": 0.50188, "dt_net": 0.24274, "epoch": "62/300", "eta": "0:11:56", "gpu_mem": "5.93G", "grad_norm": 0.87965, "loss": 1.99025, "lr": 0.08988, "top1_err": 83.33333, "top5_err": 20.83333}
[11/07 13:12:29][INFO] train_net.py:  696: Epoch 61 takes 4.00s. Epochs from 10 to 61 take 4.27s in average and 4.09s in median.
[11/07 13:12:29][INFO] train_net.py:  702: For epoch 61, each iteraction takes 0.67s in average. From epoch 10 to 61, each iteraction takes 0.71s in average.
[11/07 13:12:33][INFO] logging.py:   99: json_stats: {"RAM": "41.66/501.50G", "_type": "train_epoch", "dt": 0.52432, "dt_data": 0.52432, "dt_net": 0.24381, "epoch": "63/300", "eta": "0:12:25", "gpu_mem": "5.93G", "grad_norm": 0.63644, "loss": 1.96003, "lr": 0.08956, "top1_err": 62.50000, "top5_err": 20.83333}
[11/07 13:12:33][INFO] train_net.py:  696: Epoch 62 takes 4.18s. Epochs from 10 to 62 take 4.27s in average and 4.09s in median.
[11/07 13:12:33][INFO] train_net.py:  702: For epoch 62, each iteraction takes 0.70s in average. From epoch 10 to 62, each iteraction takes 0.71s in average.
[11/07 13:12:37][INFO] logging.py:   99: json_stats: {"RAM": "41.65/501.50G", "_type": "train_epoch", "dt": 0.51016, "dt_data": 0.51016, "dt_net": 0.24268, "epoch": "64/300", "eta": "0:12:02", "gpu_mem": "5.93G", "grad_norm": 0.69969, "loss": 1.98582, "lr": 0.08924, "top1_err": 62.50000, "top5_err": 29.16667}
[11/07 13:12:37][INFO] train_net.py:  696: Epoch 63 takes 4.07s. Epochs from 10 to 63 take 4.26s in average and 4.09s in median.
[11/07 13:12:37][INFO] train_net.py:  702: For epoch 63, each iteraction takes 0.68s in average. From epoch 10 to 63, each iteraction takes 0.71s in average.
[11/07 13:12:41][INFO] logging.py:   99: json_stats: {"RAM": "41.68/501.50G", "_type": "train_epoch", "dt": 0.52250, "dt_data": 0.52250, "dt_net": 0.24471, "epoch": "65/300", "eta": "0:12:16", "gpu_mem": "5.93G", "grad_norm": 0.51154, "loss": 1.90881, "lr": 0.08891, "top1_err": 62.50000, "top5_err": 12.50000}
[11/07 13:12:41][INFO] train_net.py:  696: Epoch 64 takes 4.22s. Epochs from 10 to 64 take 4.26s in average and 4.09s in median.
[11/07 13:12:41][INFO] train_net.py:  702: For epoch 64, each iteraction takes 0.70s in average. From epoch 10 to 64, each iteraction takes 0.71s in average.
[11/07 13:12:46][INFO] logging.py:   99: json_stats: {"RAM": "41.65/501.50G", "_type": "train_epoch", "dt": 0.50472, "dt_data": 0.50472, "dt_net": 0.24274, "epoch": "66/300", "eta": "0:11:48", "gpu_mem": "5.93G", "grad_norm": 0.92696, "loss": 1.94351, "lr": 0.08858, "top1_err": 62.50000, "top5_err": 20.83333}
[11/07 13:12:46][INFO] train_net.py:  696: Epoch 65 takes 4.12s. Epochs from 10 to 65 take 4.26s in average and 4.09s in median.
[11/07 13:12:46][INFO] train_net.py:  702: For epoch 65, each iteraction takes 0.69s in average. From epoch 10 to 65, each iteraction takes 0.71s in average.
[11/07 13:12:50][INFO] logging.py:   99: json_stats: {"RAM": "41.63/501.50G", "_type": "train_epoch", "dt": 0.51001, "dt_data": 0.51001, "dt_net": 0.24276, "epoch": "67/300", "eta": "0:11:52", "gpu_mem": "5.93G", "grad_norm": 1.29211, "loss": 1.83281, "lr": 0.08825, "top1_err": 62.50000, "top5_err": 12.50000}
[11/07 13:12:50][INFO] train_net.py:  696: Epoch 66 takes 4.06s. Epochs from 10 to 66 take 4.26s in average and 4.09s in median.
[11/07 13:12:50][INFO] train_net.py:  702: For epoch 66, each iteraction takes 0.68s in average. From epoch 10 to 66, each iteraction takes 0.71s in average.
[11/07 13:12:54][INFO] logging.py:   99: json_stats: {"RAM": "40.68/501.50G", "_type": "train_epoch", "dt": 0.51542, "dt_data": 0.51542, "dt_net": 0.24350, "epoch": "68/300", "eta": "0:11:57", "gpu_mem": "5.93G", "grad_norm": 2.16351, "loss": 2.07546, "lr": 0.08791, "top1_err": 66.66667, "top5_err": 20.83333}
[11/07 13:12:54][INFO] train_net.py:  696: Epoch 67 takes 4.23s. Epochs from 10 to 67 take 4.26s in average and 4.09s in median.
[11/07 13:12:54][INFO] train_net.py:  702: For epoch 67, each iteraction takes 0.70s in average. From epoch 10 to 67, each iteraction takes 0.71s in average.
[11/07 13:12:58][INFO] logging.py:   99: json_stats: {"RAM": "41.64/501.50G", "_type": "train_epoch", "dt": 0.50295, "dt_data": 0.50295, "dt_net": 0.24321, "epoch": "69/300", "eta": "0:11:37", "gpu_mem": "5.93G", "grad_norm": 0.85565, "loss": 2.00553, "lr": 0.08756, "top1_err": 79.16667, "top5_err": 25.00000}
[11/07 13:12:58][INFO] train_net.py:  696: Epoch 68 takes 4.01s. Epochs from 10 to 68 take 4.25s in average and 4.09s in median.
[11/07 13:12:58][INFO] train_net.py:  702: For epoch 68, each iteraction takes 0.67s in average. From epoch 10 to 68, each iteraction takes 0.71s in average.
[11/07 13:13:02][INFO] logging.py:   99: json_stats: {"RAM": "41.65/501.50G", "_type": "train_epoch", "dt": 0.49872, "dt_data": 0.49872, "dt_net": 0.24363, "epoch": "70/300", "eta": "0:11:28", "gpu_mem": "5.93G", "grad_norm": 0.34427, "loss": 1.94571, "lr": 0.08722, "top1_err": 62.50000, "top5_err": 20.83333}
[11/07 13:13:02][INFO] train_net.py:  696: Epoch 69 takes 4.07s. Epochs from 10 to 69 take 4.25s in average and 4.09s in median.
[11/07 13:13:02][INFO] train_net.py:  702: For epoch 69, each iteraction takes 0.68s in average. From epoch 10 to 69, each iteraction takes 0.71s in average.
[11/07 13:13:02][INFO] precise_bn.py:  129: Computing precise BN statistics for 84 BN layers ...
[11/07 13:13:07][INFO] logging.py:   99: json_stats: {"RAM": "41.65/501.50G", "_type": "val_epoch", "epoch": "70/300", "gpu_mem": "5.93G", "min_top1_err": 73.91304, "min_top5_err": 34.78261, "time_diff": 0.36758, "top1_err": 91.30435, "top5_err": 34.78261}
[11/07 13:13:11][INFO] logging.py:   99: json_stats: {"RAM": "41.65/501.50G", "_type": "train_epoch", "dt": 0.50596, "dt_data": 0.50596, "dt_net": 0.24294, "epoch": "71/300", "eta": "0:11:35", "gpu_mem": "5.93G", "grad_norm": 0.71903, "loss": 1.90180, "lr": 0.08686, "top1_err": 62.50000, "top5_err": 20.83333}
[11/07 13:13:11][INFO] train_net.py:  696: Epoch 70 takes 4.05s. Epochs from 10 to 70 take 4.25s in average and 4.08s in median.
[11/07 13:13:11][INFO] train_net.py:  702: For epoch 70, each iteraction takes 0.67s in average. From epoch 10 to 70, each iteraction takes 0.71s in average.
[11/07 13:13:15][INFO] logging.py:   99: json_stats: {"RAM": "41.64/501.50G", "_type": "train_epoch", "dt": 0.51548, "dt_data": 0.51548, "dt_net": 0.24282, "epoch": "72/300", "eta": "0:11:45", "gpu_mem": "5.93G", "grad_norm": 0.86907, "loss": 1.98637, "lr": 0.08651, "top1_err": 62.50000, "top5_err": 16.66667}
[11/07 13:13:15][INFO] train_net.py:  696: Epoch 71 takes 4.01s. Epochs from 10 to 71 take 4.24s in average and 4.08s in median.
[11/07 13:13:15][INFO] train_net.py:  702: For epoch 71, each iteraction takes 0.67s in average. From epoch 10 to 71, each iteraction takes 0.71s in average.
[11/07 13:13:19][INFO] logging.py:   99: json_stats: {"RAM": "41.65/501.50G", "_type": "train_epoch", "dt": 0.49199, "dt_data": 0.49199, "dt_net": 0.24317, "epoch": "73/300", "eta": "0:11:10", "gpu_mem": "5.93G", "grad_norm": 1.06252, "loss": 1.91019, "lr": 0.08615, "top1_err": 62.50000, "top5_err": 12.50000}
[11/07 13:13:19][INFO] train_net.py:  696: Epoch 72 takes 4.03s. Epochs from 10 to 72 take 4.24s in average and 4.08s in median.
[11/07 13:13:19][INFO] train_net.py:  702: For epoch 72, each iteraction takes 0.67s in average. From epoch 10 to 72, each iteraction takes 0.71s in average.
[11/07 13:13:24][INFO] logging.py:   99: json_stats: {"RAM": "41.64/501.50G", "_type": "train_epoch", "dt": 0.50639, "dt_data": 0.50639, "dt_net": 0.24266, "epoch": "74/300", "eta": "0:11:26", "gpu_mem": "5.93G", "grad_norm": 0.59208, "loss": 1.98444, "lr": 0.08578, "top1_err": 62.50000, "top5_err": 33.33333}
[11/07 13:13:24][INFO] train_net.py:  696: Epoch 73 takes 4.02s. Epochs from 10 to 73 take 4.24s in average and 4.08s in median.
[11/07 13:13:24][INFO] train_net.py:  702: For epoch 73, each iteraction takes 0.67s in average. From epoch 10 to 73, each iteraction takes 0.71s in average.
[11/07 13:13:27][INFO] logging.py:   99: json_stats: {"RAM": "38.73/501.50G", "_type": "train_epoch", "dt": 0.52182, "dt_data": 0.52182, "dt_net": 0.24302, "epoch": "75/300", "eta": "0:11:44", "gpu_mem": "5.93G", "grad_norm": 0.42962, "loss": 2.01082, "lr": 0.08542, "top1_err": 79.16667, "top5_err": 25.00000}
[11/07 13:13:27][INFO] train_net.py:  696: Epoch 74 takes 3.98s. Epochs from 10 to 74 take 4.23s in average and 4.08s in median.
[11/07 13:13:27][INFO] train_net.py:  702: For epoch 74, each iteraction takes 0.66s in average. From epoch 10 to 74, each iteraction takes 0.71s in average.
[11/07 13:13:32][INFO] logging.py:   99: json_stats: {"RAM": "41.35/501.50G", "_type": "train_epoch", "dt": 0.51344, "dt_data": 0.51344, "dt_net": 0.24276, "epoch": "76/300", "eta": "0:11:29", "gpu_mem": "5.93G", "grad_norm": 0.39847, "loss": 1.97131, "lr": 0.08505, "top1_err": 62.50000, "top5_err": 25.00000}
[11/07 13:13:32][INFO] train_net.py:  696: Epoch 75 takes 4.20s. Epochs from 10 to 75 take 4.23s in average and 4.08s in median.
[11/07 13:13:32][INFO] train_net.py:  702: For epoch 75, each iteraction takes 0.70s in average. From epoch 10 to 75, each iteraction takes 0.71s in average.
[11/07 13:13:36][INFO] logging.py:   99: json_stats: {"RAM": "41.72/501.50G", "_type": "train_epoch", "dt": 0.48696, "dt_data": 0.48696, "dt_net": 0.24288, "epoch": "77/300", "eta": "0:10:51", "gpu_mem": "5.93G", "grad_norm": 0.57330, "loss": 1.93062, "lr": 0.08467, "top1_err": 62.50000, "top5_err": 25.00000}
[11/07 13:13:36][INFO] train_net.py:  696: Epoch 76 takes 4.08s. Epochs from 10 to 76 take 4.23s in average and 4.08s in median.
[11/07 13:13:36][INFO] train_net.py:  702: For epoch 76, each iteraction takes 0.68s in average. From epoch 10 to 76, each iteraction takes 0.70s in average.
[11/07 13:13:40][INFO] logging.py:   99: json_stats: {"RAM": "41.68/501.50G", "_type": "train_epoch", "dt": 0.52238, "dt_data": 0.52238, "dt_net": 0.24314, "epoch": "78/300", "eta": "0:11:35", "gpu_mem": "5.93G", "grad_norm": 0.43046, "loss": 1.93274, "lr": 0.08429, "top1_err": 62.50000, "top5_err": 20.83333}
[11/07 13:13:40][INFO] train_net.py:  696: Epoch 77 takes 4.15s. Epochs from 10 to 77 take 4.23s in average and 4.08s in median.
[11/07 13:13:40][INFO] train_net.py:  702: For epoch 77, each iteraction takes 0.69s in average. From epoch 10 to 77, each iteraction takes 0.70s in average.
[11/07 13:13:44][INFO] logging.py:   99: json_stats: {"RAM": "41.68/501.50G", "_type": "train_epoch", "dt": 0.52452, "dt_data": 0.52452, "dt_net": 0.24283, "epoch": "79/300", "eta": "0:11:35", "gpu_mem": "5.93G", "grad_norm": 0.79298, "loss": 1.98436, "lr": 0.08391, "top1_err": 62.50000, "top5_err": 25.00000}
[11/07 13:13:44][INFO] train_net.py:  696: Epoch 78 takes 4.14s. Epochs from 10 to 78 take 4.23s in average and 4.08s in median.
[11/07 13:13:44][INFO] train_net.py:  702: For epoch 78, each iteraction takes 0.69s in average. From epoch 10 to 78, each iteraction takes 0.70s in average.
[11/07 13:13:48][INFO] logging.py:   99: json_stats: {"RAM": "41.68/501.50G", "_type": "train_epoch", "dt": 0.49779, "dt_data": 0.49779, "dt_net": 0.24350, "epoch": "80/300", "eta": "0:10:57", "gpu_mem": "5.93G", "grad_norm": 1.39173, "loss": 1.88723, "lr": 0.08352, "top1_err": 62.50000, "top5_err": 16.66667}
[11/07 13:13:48][INFO] train_net.py:  696: Epoch 79 takes 4.08s. Epochs from 10 to 79 take 4.22s in average and 4.08s in median.
[11/07 13:13:48][INFO] train_net.py:  702: For epoch 79, each iteraction takes 0.68s in average. From epoch 10 to 79, each iteraction takes 0.70s in average.
[11/07 13:13:48][INFO] precise_bn.py:  129: Computing precise BN statistics for 84 BN layers ...
[11/07 13:13:54][INFO] logging.py:   99: json_stats: {"RAM": "41.72/501.50G", "_type": "val_epoch", "epoch": "80/300", "gpu_mem": "5.93G", "min_top1_err": 73.91304, "min_top5_err": 34.78261, "time_diff": 0.37635, "top1_err": 95.65217, "top5_err": 39.13043}
[11/07 13:13:58][INFO] logging.py:   99: json_stats: {"RAM": "41.68/501.50G", "_type": "train_epoch", "dt": 0.49612, "dt_data": 0.49612, "dt_net": 0.24328, "epoch": "81/300", "eta": "0:10:51", "gpu_mem": "5.93G", "grad_norm": 1.11504, "loss": 1.79662, "lr": 0.08313, "top1_err": 62.50000, "top5_err": 16.66667}
[11/07 13:13:58][INFO] train_net.py:  696: Epoch 80 takes 4.06s. Epochs from 10 to 80 take 4.22s in average and 4.08s in median.
[11/07 13:13:58][INFO] train_net.py:  702: For epoch 80, each iteraction takes 0.68s in average. From epoch 10 to 80, each iteraction takes 0.70s in average.
[11/07 13:14:02][INFO] logging.py:   99: json_stats: {"RAM": "41.66/501.50G", "_type": "train_epoch", "dt": 0.49753, "dt_data": 0.49753, "dt_net": 0.24276, "epoch": "82/300", "eta": "0:10:50", "gpu_mem": "5.93G", "grad_norm": 0.49869, "loss": 1.93478, "lr": 0.08274, "top1_err": 70.83333, "top5_err": 20.83333}
[11/07 13:14:02][INFO] train_net.py:  696: Epoch 81 takes 3.97s. Epochs from 10 to 81 take 4.22s in average and 4.08s in median.
[11/07 13:14:02][INFO] train_net.py:  702: For epoch 81, each iteraction takes 0.66s in average. From epoch 10 to 81, each iteraction takes 0.70s in average.
[11/07 13:14:06][INFO] logging.py:   99: json_stats: {"RAM": "39.96/501.50G", "_type": "train_epoch", "dt": 0.52647, "dt_data": 0.52647, "dt_net": 0.24258, "epoch": "83/300", "eta": "0:11:25", "gpu_mem": "5.93G", "grad_norm": 0.96821, "loss": 1.82488, "lr": 0.08234, "top1_err": 54.16667, "top5_err": 12.50000}
[11/07 13:14:06][INFO] train_net.py:  696: Epoch 82 takes 4.16s. Epochs from 10 to 82 take 4.22s in average and 4.08s in median.
[11/07 13:14:06][INFO] train_net.py:  702: For epoch 82, each iteraction takes 0.69s in average. From epoch 10 to 82, each iteraction takes 0.70s in average.
[11/07 13:14:10][INFO] logging.py:   99: json_stats: {"RAM": "41.65/501.50G", "_type": "train_epoch", "dt": 0.53616, "dt_data": 0.53616, "dt_net": 0.24303, "epoch": "84/300", "eta": "0:11:34", "gpu_mem": "5.93G", "grad_norm": 1.68630, "loss": 1.90331, "lr": 0.08194, "top1_err": 70.83333, "top5_err": 20.83333}
[11/07 13:14:10][INFO] train_net.py:  696: Epoch 83 takes 4.17s. Epochs from 10 to 83 take 4.22s in average and 4.08s in median.
[11/07 13:14:10][INFO] train_net.py:  702: For epoch 83, each iteraction takes 0.69s in average. From epoch 10 to 83, each iteraction takes 0.70s in average.
[11/07 13:14:14][INFO] logging.py:   99: json_stats: {"RAM": "41.66/501.50G", "_type": "train_epoch", "dt": 0.49575, "dt_data": 0.49575, "dt_net": 0.24229, "epoch": "85/300", "eta": "0:10:39", "gpu_mem": "5.93G", "grad_norm": 0.65715, "loss": 1.81330, "lr": 0.08153, "top1_err": 62.50000, "top5_err": 25.00000}
[11/07 13:14:14][INFO] train_net.py:  696: Epoch 84 takes 4.11s. Epochs from 10 to 84 take 4.22s in average and 4.08s in median.
[11/07 13:14:14][INFO] train_net.py:  702: For epoch 84, each iteraction takes 0.69s in average. From epoch 10 to 84, each iteraction takes 0.70s in average.
[11/07 13:14:18][INFO] logging.py:   99: json_stats: {"RAM": "41.68/501.50G", "_type": "train_epoch", "dt": 0.50188, "dt_data": 0.50188, "dt_net": 0.24349, "epoch": "86/300", "eta": "0:10:44", "gpu_mem": "5.93G", "grad_norm": 0.70723, "loss": 1.99702, "lr": 0.08113, "top1_err": 66.66667, "top5_err": 29.16667}
[11/07 13:14:18][INFO] train_net.py:  696: Epoch 85 takes 4.09s. Epochs from 10 to 85 take 4.21s in average and 4.08s in median.
[11/07 13:14:18][INFO] train_net.py:  702: For epoch 85, each iteraction takes 0.68s in average. From epoch 10 to 85, each iteraction takes 0.70s in average.
[11/07 13:14:22][INFO] logging.py:   99: json_stats: {"RAM": "41.67/501.50G", "_type": "train_epoch", "dt": 0.49677, "dt_data": 0.49677, "dt_net": 0.24311, "epoch": "87/300", "eta": "0:10:34", "gpu_mem": "5.93G", "grad_norm": 0.54787, "loss": 1.97693, "lr": 0.08071, "top1_err": 62.50000, "top5_err": 20.83333}
[11/07 13:14:22][INFO] train_net.py:  696: Epoch 86 takes 4.01s. Epochs from 10 to 86 take 4.21s in average and 4.08s in median.
[11/07 13:14:22][INFO] train_net.py:  702: For epoch 86, each iteraction takes 0.67s in average. From epoch 10 to 86, each iteraction takes 0.70s in average.
[11/07 13:14:26][INFO] logging.py:   99: json_stats: {"RAM": "41.67/501.50G", "_type": "train_epoch", "dt": 0.50088, "dt_data": 0.50088, "dt_net": 0.24301, "epoch": "88/300", "eta": "0:10:37", "gpu_mem": "5.93G", "grad_norm": 0.54720, "loss": 1.94864, "lr": 0.08030, "top1_err": 62.50000, "top5_err": 25.00000}
[11/07 13:14:26][INFO] train_net.py:  696: Epoch 87 takes 4.06s. Epochs from 10 to 87 take 4.21s in average and 4.08s in median.
[11/07 13:14:26][INFO] train_net.py:  702: For epoch 87, each iteraction takes 0.68s in average. From epoch 10 to 87, each iteraction takes 0.70s in average.
[11/07 13:14:31][INFO] logging.py:   99: json_stats: {"RAM": "41.67/501.50G", "_type": "train_epoch", "dt": 0.52706, "dt_data": 0.52706, "dt_net": 0.24309, "epoch": "89/300", "eta": "0:11:07", "gpu_mem": "5.93G", "grad_norm": 0.43586, "loss": 1.94524, "lr": 0.07988, "top1_err": 62.50000, "top5_err": 20.83333}
[11/07 13:14:31][INFO] train_net.py:  696: Epoch 88 takes 4.07s. Epochs from 10 to 88 take 4.21s in average and 4.08s in median.
[11/07 13:14:31][INFO] train_net.py:  702: For epoch 88, each iteraction takes 0.68s in average. From epoch 10 to 88, each iteraction takes 0.70s in average.
[11/07 13:14:35][INFO] logging.py:   99: json_stats: {"RAM": "41.68/501.50G", "_type": "train_epoch", "dt": 0.49032, "dt_data": 0.49032, "dt_net": 0.24259, "epoch": "90/300", "eta": "0:10:17", "gpu_mem": "5.93G", "grad_norm": 1.67098, "loss": 1.93293, "lr": 0.07946, "top1_err": 62.50000, "top5_err": 16.66667}
[11/07 13:14:35][INFO] train_net.py:  696: Epoch 89 takes 4.04s. Epochs from 10 to 89 take 4.21s in average and 4.08s in median.
[11/07 13:14:35][INFO] train_net.py:  702: For epoch 89, each iteraction takes 0.67s in average. From epoch 10 to 89, each iteraction takes 0.70s in average.
[11/07 13:14:35][INFO] precise_bn.py:  129: Computing precise BN statistics for 84 BN layers ...
[11/07 13:14:40][INFO] logging.py:   99: json_stats: {"RAM": "38.76/501.50G", "_type": "val_epoch", "epoch": "90/300", "gpu_mem": "5.93G", "min_top1_err": 73.91304, "min_top5_err": 34.78261, "time_diff": 0.40604, "top1_err": 91.30435, "top5_err": 34.78261}
[11/07 13:14:44][INFO] logging.py:   99: json_stats: {"RAM": "41.48/501.50G", "_type": "train_epoch", "dt": 0.53286, "dt_data": 0.53286, "dt_net": 0.24276, "epoch": "91/300", "eta": "0:11:08", "gpu_mem": "5.93G", "grad_norm": 0.87183, "loss": 1.94553, "lr": 0.07904, "top1_err": 62.50000, "top5_err": 25.00000}
[11/07 13:14:44][INFO] train_net.py:  696: Epoch 90 takes 4.21s. Epochs from 10 to 90 take 4.21s in average and 4.08s in median.
[11/07 13:14:44][INFO] train_net.py:  702: For epoch 90, each iteraction takes 0.70s in average. From epoch 10 to 90, each iteraction takes 0.70s in average.
[11/07 13:14:49][INFO] logging.py:   99: json_stats: {"RAM": "41.68/501.50G", "_type": "train_epoch", "dt": 0.53503, "dt_data": 0.53503, "dt_net": 0.24340, "epoch": "92/300", "eta": "0:11:07", "gpu_mem": "5.93G", "grad_norm": 1.06751, "loss": 1.94562, "lr": 0.07861, "top1_err": 58.33333, "top5_err": 20.83333}
[11/07 13:14:49][INFO] train_net.py:  696: Epoch 91 takes 4.18s. Epochs from 10 to 91 take 4.21s in average and 4.08s in median.
[11/07 13:14:49][INFO] train_net.py:  702: For epoch 91, each iteraction takes 0.70s in average. From epoch 10 to 91, each iteraction takes 0.70s in average.
[11/07 13:14:53][INFO] logging.py:   99: json_stats: {"RAM": "41.68/501.50G", "_type": "train_epoch", "dt": 0.50042, "dt_data": 0.50042, "dt_net": 0.24278, "epoch": "93/300", "eta": "0:10:21", "gpu_mem": "5.93G", "grad_norm": 1.21613, "loss": 1.88888, "lr": 0.07818, "top1_err": 62.50000, "top5_err": 20.83333}
[11/07 13:14:53][INFO] train_net.py:  696: Epoch 92 takes 4.04s. Epochs from 10 to 92 take 4.20s in average and 4.08s in median.
[11/07 13:14:53][INFO] train_net.py:  702: For epoch 92, each iteraction takes 0.67s in average. From epoch 10 to 92, each iteraction takes 0.70s in average.
[11/07 13:14:57][INFO] logging.py:   99: json_stats: {"RAM": "41.68/501.50G", "_type": "train_epoch", "dt": 0.50035, "dt_data": 0.50035, "dt_net": 0.24336, "epoch": "94/300", "eta": "0:10:18", "gpu_mem": "5.93G", "grad_norm": 0.64293, "loss": 1.88050, "lr": 0.07774, "top1_err": 66.66667, "top5_err": 16.66667}
[11/07 13:14:57][INFO] train_net.py:  696: Epoch 93 takes 4.07s. Epochs from 10 to 93 take 4.20s in average and 4.08s in median.
[11/07 13:14:57][INFO] train_net.py:  702: For epoch 93, each iteraction takes 0.68s in average. From epoch 10 to 93, each iteraction takes 0.70s in average.
[11/07 13:15:01][INFO] logging.py:   99: json_stats: {"RAM": "41.68/501.50G", "_type": "train_epoch", "dt": 0.60155, "dt_data": 0.60155, "dt_net": 0.24344, "epoch": "95/300", "eta": "0:12:19", "gpu_mem": "5.93G", "grad_norm": 1.13088, "loss": 1.89778, "lr": 0.07731, "top1_err": 62.50000, "top5_err": 25.00000}
[11/07 13:15:01][INFO] train_net.py:  696: Epoch 94 takes 4.20s. Epochs from 10 to 94 take 4.20s in average and 4.08s in median.
[11/07 13:15:01][INFO] train_net.py:  702: For epoch 94, each iteraction takes 0.70s in average. From epoch 10 to 94, each iteraction takes 0.70s in average.
[11/07 13:15:05][INFO] logging.py:   99: json_stats: {"RAM": "41.68/501.50G", "_type": "train_epoch", "dt": 0.49374, "dt_data": 0.49374, "dt_net": 0.24276, "epoch": "96/300", "eta": "0:10:04", "gpu_mem": "5.93G", "grad_norm": 0.55679, "loss": 2.02417, "lr": 0.07686, "top1_err": 58.33333, "top5_err": 20.83333}
[11/07 13:15:05][INFO] train_net.py:  696: Epoch 95 takes 4.04s. Epochs from 10 to 95 take 4.20s in average and 4.08s in median.
[11/07 13:15:05][INFO] train_net.py:  702: For epoch 95, each iteraction takes 0.67s in average. From epoch 10 to 95, each iteraction takes 0.70s in average.
[11/07 13:15:09][INFO] logging.py:   99: json_stats: {"RAM": "41.68/501.50G", "_type": "train_epoch", "dt": 0.51619, "dt_data": 0.51619, "dt_net": 0.24294, "epoch": "97/300", "eta": "0:10:28", "gpu_mem": "5.93G", "grad_norm": 0.49418, "loss": 1.90022, "lr": 0.07642, "top1_err": 62.50000, "top5_err": 16.66667}
[11/07 13:15:09][INFO] train_net.py:  696: Epoch 96 takes 4.08s. Epochs from 10 to 96 take 4.20s in average and 4.08s in median.
[11/07 13:15:09][INFO] train_net.py:  702: For epoch 96, each iteraction takes 0.68s in average. From epoch 10 to 96, each iteraction takes 0.70s in average.
[11/07 13:15:13][INFO] logging.py:   99: json_stats: {"RAM": "41.67/501.50G", "_type": "train_epoch", "dt": 0.49696, "dt_data": 0.49695, "dt_net": 0.24326, "epoch": "98/300", "eta": "0:10:02", "gpu_mem": "5.93G", "grad_norm": 1.49920, "loss": 1.84404, "lr": 0.07598, "top1_err": 66.66667, "top5_err": 12.50000}
[11/07 13:15:13][INFO] train_net.py:  696: Epoch 97 takes 4.07s. Epochs from 10 to 97 take 4.20s in average and 4.08s in median.
[11/07 13:15:13][INFO] train_net.py:  702: For epoch 97, each iteraction takes 0.68s in average. From epoch 10 to 97, each iteraction takes 0.70s in average.
[11/07 13:15:17][INFO] logging.py:   99: json_stats: {"RAM": "39.73/501.50G", "_type": "train_epoch", "dt": 0.51541, "dt_data": 0.51541, "dt_net": 0.24299, "epoch": "99/300", "eta": "0:10:21", "gpu_mem": "5.93G", "grad_norm": 3.69052, "loss": 1.90354, "lr": 0.07553, "top1_err": 58.33333, "top5_err": 16.66667}
[11/07 13:15:17][INFO] train_net.py:  696: Epoch 98 takes 3.96s. Epochs from 10 to 98 take 4.19s in average and 4.08s in median.
[11/07 13:15:17][INFO] train_net.py:  702: For epoch 98, each iteraction takes 0.66s in average. From epoch 10 to 98, each iteraction takes 0.70s in average.
[11/07 13:15:21][INFO] logging.py:   99: json_stats: {"RAM": "41.33/501.50G", "_type": "train_epoch", "dt": 0.51568, "dt_data": 0.51568, "dt_net": 0.24285, "epoch": "100/300", "eta": "0:10:18", "gpu_mem": "5.93G", "grad_norm": 0.83762, "loss": 1.89200, "lr": 0.07508, "top1_err": 54.16667, "top5_err": 25.00000}
[11/07 13:15:21][INFO] train_net.py:  696: Epoch 99 takes 4.16s. Epochs from 10 to 99 take 4.19s in average and 4.08s in median.
[11/07 13:15:21][INFO] train_net.py:  702: For epoch 99, each iteraction takes 0.69s in average. From epoch 10 to 99, each iteraction takes 0.70s in average.
[11/07 13:15:21][INFO] precise_bn.py:  129: Computing precise BN statistics for 84 BN layers ...
[11/07 13:15:27][INFO] logging.py:   99: json_stats: {"RAM": "41.69/501.50G", "_type": "val_epoch", "epoch": "100/300", "gpu_mem": "5.93G", "min_top1_err": 73.91304, "min_top5_err": 34.78261, "time_diff": 0.35741, "top1_err": 86.95652, "top5_err": 39.13043}
[11/07 13:15:31][INFO] logging.py:   99: json_stats: {"RAM": "41.68/501.50G", "_type": "train_epoch", "dt": 0.53636, "dt_data": 0.53636, "dt_net": 0.24307, "epoch": "101/300", "eta": "0:10:40", "gpu_mem": "5.93G", "grad_norm": 1.80225, "loss": 2.05777, "lr": 0.07462, "top1_err": 58.33333, "top5_err": 29.16667}
[11/07 13:15:31][INFO] train_net.py:  696: Epoch 100 takes 4.02s. Epochs from 10 to 100 take 4.19s in average and 4.08s in median.
[11/07 13:15:31][INFO] train_net.py:  702: For epoch 100, each iteraction takes 0.67s in average. From epoch 10 to 100, each iteraction takes 0.70s in average.
[11/07 13:15:35][INFO] logging.py:   99: json_stats: {"RAM": "41.68/501.50G", "_type": "train_epoch", "dt": 0.49147, "dt_data": 0.49147, "dt_net": 0.24288, "epoch": "102/300", "eta": "0:09:43", "gpu_mem": "5.93G", "grad_norm": 1.46624, "loss": 1.89494, "lr": 0.07416, "top1_err": 54.16667, "top5_err": 25.00000}
[11/07 13:15:35][INFO] train_net.py:  696: Epoch 101 takes 4.01s. Epochs from 10 to 101 take 4.19s in average and 4.08s in median.
[11/07 13:15:35][INFO] train_net.py:  702: For epoch 101, each iteraction takes 0.67s in average. From epoch 10 to 101, each iteraction takes 0.70s in average.
[11/07 13:15:39][INFO] logging.py:   99: json_stats: {"RAM": "41.68/501.50G", "_type": "train_epoch", "dt": 0.50494, "dt_data": 0.50494, "dt_net": 0.24362, "epoch": "103/300", "eta": "0:09:56", "gpu_mem": "5.93G", "grad_norm": 0.93662, "loss": 1.87969, "lr": 0.07370, "top1_err": 58.33333, "top5_err": 16.66667}
[11/07 13:15:39][INFO] train_net.py:  696: Epoch 102 takes 4.07s. Epochs from 10 to 102 take 4.19s in average and 4.08s in median.
[11/07 13:15:39][INFO] train_net.py:  702: For epoch 102, each iteraction takes 0.68s in average. From epoch 10 to 102, each iteraction takes 0.70s in average.
[11/07 13:15:43][INFO] logging.py:   99: json_stats: {"RAM": "41.68/501.50G", "_type": "train_epoch", "dt": 0.48740, "dt_data": 0.48740, "dt_net": 0.24310, "epoch": "104/300", "eta": "0:09:33", "gpu_mem": "5.93G", "grad_norm": 2.25158, "loss": 1.83499, "lr": 0.07324, "top1_err": 58.33333, "top5_err": 25.00000}
[11/07 13:15:43][INFO] train_net.py:  696: Epoch 103 takes 4.01s. Epochs from 10 to 103 take 4.19s in average and 4.08s in median.
[11/07 13:15:43][INFO] train_net.py:  702: For epoch 103, each iteraction takes 0.67s in average. From epoch 10 to 103, each iteraction takes 0.70s in average.
[11/07 13:15:47][INFO] logging.py:   99: json_stats: {"RAM": "41.68/501.50G", "_type": "train_epoch", "dt": 0.50748, "dt_data": 0.50748, "dt_net": 0.24280, "epoch": "105/300", "eta": "0:09:53", "gpu_mem": "5.93G", "grad_norm": 2.75510, "loss": 1.73835, "lr": 0.07278, "top1_err": 50.00000, "top5_err": 25.00000}
[11/07 13:15:47][INFO] train_net.py:  696: Epoch 104 takes 4.03s. Epochs from 10 to 104 take 4.19s in average and 4.08s in median.
[11/07 13:15:47][INFO] train_net.py:  702: For epoch 104, each iteraction takes 0.67s in average. From epoch 10 to 104, each iteraction takes 0.70s in average.
[11/07 13:15:51][INFO] logging.py:   99: json_stats: {"RAM": "41.66/501.50G", "_type": "train_epoch", "dt": 0.50534, "dt_data": 0.50534, "dt_net": 0.24311, "epoch": "106/300", "eta": "0:09:48", "gpu_mem": "5.93G", "grad_norm": 2.44382, "loss": 1.64192, "lr": 0.07231, "top1_err": 45.83333, "top5_err": 16.66667}
[11/07 13:15:51][INFO] train_net.py:  696: Epoch 105 takes 4.01s. Epochs from 10 to 105 take 4.18s in average and 4.08s in median.
[11/07 13:15:51][INFO] train_net.py:  702: For epoch 105, each iteraction takes 0.67s in average. From epoch 10 to 105, each iteraction takes 0.70s in average.
[11/07 13:15:55][INFO] logging.py:   99: json_stats: {"RAM": "39.93/501.50G", "_type": "train_epoch", "dt": 0.53099, "dt_data": 0.53099, "dt_net": 0.24285, "epoch": "107/300", "eta": "0:10:14", "gpu_mem": "5.93G", "grad_norm": 1.65467, "loss": 1.96459, "lr": 0.07184, "top1_err": 54.16667, "top5_err": 29.16667}
[11/07 13:15:55][INFO] train_net.py:  696: Epoch 106 takes 4.03s. Epochs from 10 to 106 take 4.18s in average and 4.08s in median.
[11/07 13:15:55][INFO] train_net.py:  702: For epoch 106, each iteraction takes 0.67s in average. From epoch 10 to 106, each iteraction takes 0.70s in average.
[11/07 13:15:59][INFO] logging.py:   99: json_stats: {"RAM": "41.67/501.50G", "_type": "train_epoch", "dt": 0.52357, "dt_data": 0.52357, "dt_net": 0.24290, "epoch": "108/300", "eta": "0:10:03", "gpu_mem": "5.93G", "grad_norm": 1.28093, "loss": 1.98928, "lr": 0.07137, "top1_err": 62.50000, "top5_err": 29.16667}
[11/07 13:15:59][INFO] train_net.py:  696: Epoch 107 takes 4.16s. Epochs from 10 to 107 take 4.18s in average and 4.08s in median.
[11/07 13:15:59][INFO] train_net.py:  702: For epoch 107, each iteraction takes 0.69s in average. From epoch 10 to 107, each iteraction takes 0.70s in average.
[11/07 13:16:03][INFO] logging.py:   99: json_stats: {"RAM": "41.68/501.50G", "_type": "train_epoch", "dt": 0.51321, "dt_data": 0.51321, "dt_net": 0.24292, "epoch": "109/300", "eta": "0:09:48", "gpu_mem": "5.93G", "grad_norm": 1.01687, "loss": 1.82741, "lr": 0.07089, "top1_err": 58.33333, "top5_err": 16.66667}
[11/07 13:16:03][INFO] train_net.py:  696: Epoch 108 takes 4.09s. Epochs from 10 to 108 take 4.18s in average and 4.08s in median.
[11/07 13:16:03][INFO] train_net.py:  702: For epoch 108, each iteraction takes 0.68s in average. From epoch 10 to 108, each iteraction takes 0.70s in average.
[11/07 13:16:07][INFO] logging.py:   99: json_stats: {"RAM": "41.68/501.50G", "_type": "train_epoch", "dt": 0.51715, "dt_data": 0.51715, "dt_net": 0.24272, "epoch": "110/300", "eta": "0:09:49", "gpu_mem": "5.93G", "grad_norm": 0.99122, "loss": 1.80681, "lr": 0.07042, "top1_err": 58.33333, "top5_err": 25.00000}
[11/07 13:16:07][INFO] train_net.py:  696: Epoch 109 takes 4.13s. Epochs from 10 to 109 take 4.18s in average and 4.08s in median.
[11/07 13:16:07][INFO] train_net.py:  702: For epoch 109, each iteraction takes 0.69s in average. From epoch 10 to 109, each iteraction takes 0.70s in average.
[11/07 13:16:07][INFO] precise_bn.py:  129: Computing precise BN statistics for 84 BN layers ...
[11/07 13:16:13][INFO] logging.py:   99: json_stats: {"RAM": "41.69/501.50G", "_type": "val_epoch", "epoch": "110/300", "gpu_mem": "5.93G", "min_top1_err": 73.91304, "min_top5_err": 26.08696, "time_diff": 0.37522, "top1_err": 86.95652, "top5_err": 26.08696}
[11/07 13:16:17][INFO] logging.py:   99: json_stats: {"RAM": "41.71/501.50G", "_type": "train_epoch", "dt": 0.51972, "dt_data": 0.51971, "dt_net": 0.24286, "epoch": "111/300", "eta": "0:09:49", "gpu_mem": "5.93G", "grad_norm": 1.51113, "loss": 1.65515, "lr": 0.06994, "top1_err": 45.83333, "top5_err": 25.00000}
[11/07 13:16:17][INFO] train_net.py:  696: Epoch 110 takes 4.07s. Epochs from 10 to 110 take 4.18s in average and 4.08s in median.
[11/07 13:16:17][INFO] train_net.py:  702: For epoch 110, each iteraction takes 0.68s in average. From epoch 10 to 110, each iteraction takes 0.70s in average.
[11/07 13:16:21][INFO] logging.py:   99: json_stats: {"RAM": "41.69/501.50G", "_type": "train_epoch", "dt": 0.49902, "dt_data": 0.49902, "dt_net": 0.24310, "epoch": "112/300", "eta": "0:09:22", "gpu_mem": "5.93G", "grad_norm": 1.68430, "loss": 1.80845, "lr": 0.06946, "top1_err": 45.83333, "top5_err": 29.16667}
[11/07 13:16:21][INFO] train_net.py:  696: Epoch 111 takes 4.11s. Epochs from 10 to 111 take 4.18s in average and 4.08s in median.
[11/07 13:16:21][INFO] train_net.py:  702: For epoch 111, each iteraction takes 0.68s in average. From epoch 10 to 111, each iteraction takes 0.70s in average.
[11/07 13:16:25][INFO] logging.py:   99: json_stats: {"RAM": "41.67/501.50G", "_type": "train_epoch", "dt": 0.49765, "dt_data": 0.49765, "dt_net": 0.24291, "epoch": "113/300", "eta": "0:09:18", "gpu_mem": "5.93G", "grad_norm": 8.35537, "loss": 1.82963, "lr": 0.06897, "top1_err": 50.00000, "top5_err": 25.00000}
[11/07 13:16:25][INFO] train_net.py:  696: Epoch 112 takes 4.01s. Epochs from 10 to 112 take 4.18s in average and 4.08s in median.
[11/07 13:16:25][INFO] train_net.py:  702: For epoch 112, each iteraction takes 0.67s in average. From epoch 10 to 112, each iteraction takes 0.70s in average.
[11/07 13:16:29][INFO] logging.py:   99: json_stats: {"RAM": "38.73/501.50G", "_type": "train_epoch", "dt": 0.51879, "dt_data": 0.51879, "dt_net": 0.24294, "epoch": "114/300", "eta": "0:09:38", "gpu_mem": "5.93G", "grad_norm": 0.70629, "loss": 1.90711, "lr": 0.06849, "top1_err": 58.33333, "top5_err": 20.83333}
[11/07 13:16:29][INFO] train_net.py:  696: Epoch 113 takes 4.05s. Epochs from 10 to 113 take 4.18s in average and 4.08s in median.
[11/07 13:16:29][INFO] train_net.py:  702: For epoch 113, each iteraction takes 0.67s in average. From epoch 10 to 113, each iteraction takes 0.70s in average.
[11/07 13:16:33][INFO] logging.py:   99: json_stats: {"RAM": "41.15/501.50G", "_type": "train_epoch", "dt": 0.53080, "dt_data": 0.53080, "dt_net": 0.24323, "epoch": "115/300", "eta": "0:09:49", "gpu_mem": "5.93G", "grad_norm": 0.87039, "loss": 1.65346, "lr": 0.06800, "top1_err": 50.00000, "top5_err": 16.66667}
[11/07 13:16:33][INFO] train_net.py:  696: Epoch 114 takes 4.26s. Epochs from 10 to 114 take 4.18s in average and 4.08s in median.
[11/07 13:16:33][INFO] train_net.py:  702: For epoch 114, each iteraction takes 0.71s in average. From epoch 10 to 114, each iteraction takes 0.70s in average.
[11/07 13:16:38][INFO] logging.py:   99: json_stats: {"RAM": "41.71/501.50G", "_type": "train_epoch", "dt": 0.50796, "dt_data": 0.50796, "dt_net": 0.24287, "epoch": "116/300", "eta": "0:09:20", "gpu_mem": "5.93G", "grad_norm": 1.13904, "loss": 1.72579, "lr": 0.06751, "top1_err": 45.83333, "top5_err": 16.66667}
[11/07 13:16:38][INFO] train_net.py:  696: Epoch 115 takes 4.15s. Epochs from 10 to 115 take 4.18s in average and 4.08s in median.
[11/07 13:16:38][INFO] train_net.py:  702: For epoch 115, each iteraction takes 0.69s in average. From epoch 10 to 115, each iteraction takes 0.70s in average.
[11/07 13:16:42][INFO] logging.py:   99: json_stats: {"RAM": "41.68/501.50G", "_type": "train_epoch", "dt": 0.49995, "dt_data": 0.49995, "dt_net": 0.24357, "epoch": "117/300", "eta": "0:09:08", "gpu_mem": "5.93G", "grad_norm": 1.35917, "loss": 1.82792, "lr": 0.06702, "top1_err": 54.16667, "top5_err": 20.83333}
[11/07 13:16:42][INFO] train_net.py:  696: Epoch 116 takes 4.09s. Epochs from 10 to 116 take 4.18s in average and 4.08s in median.
[11/07 13:16:42][INFO] train_net.py:  702: For epoch 116, each iteraction takes 0.68s in average. From epoch 10 to 116, each iteraction takes 0.70s in average.
[11/07 13:16:46][INFO] logging.py:   99: json_stats: {"RAM": "41.68/501.50G", "_type": "train_epoch", "dt": 0.50403, "dt_data": 0.50403, "dt_net": 0.24315, "epoch": "118/300", "eta": "0:09:10", "gpu_mem": "5.93G", "grad_norm": 2.25341, "loss": 2.07834, "lr": 0.06653, "top1_err": 70.83333, "top5_err": 16.66667}
[11/07 13:16:46][INFO] train_net.py:  696: Epoch 117 takes 4.09s. Epochs from 10 to 117 take 4.17s in average and 4.08s in median.
[11/07 13:16:46][INFO] train_net.py:  702: For epoch 117, each iteraction takes 0.68s in average. From epoch 10 to 117, each iteraction takes 0.70s in average.
[11/07 13:16:50][INFO] logging.py:   99: json_stats: {"RAM": "41.71/501.50G", "_type": "train_epoch", "dt": 0.50092, "dt_data": 0.50092, "dt_net": 0.24345, "epoch": "119/300", "eta": "0:09:03", "gpu_mem": "5.93G", "grad_norm": 1.01814, "loss": 1.49539, "lr": 0.06603, "top1_err": 58.33333, "top5_err": 8.33333}
[11/07 13:16:50][INFO] train_net.py:  696: Epoch 118 takes 4.06s. Epochs from 10 to 118 take 4.17s in average and 4.08s in median.
[11/07 13:16:50][INFO] train_net.py:  702: For epoch 118, each iteraction takes 0.68s in average. From epoch 10 to 118, each iteraction takes 0.70s in average.
[11/07 13:16:54][INFO] logging.py:   99: json_stats: {"RAM": "41.72/501.50G", "_type": "train_epoch", "dt": 0.52424, "dt_data": 0.52424, "dt_net": 0.24345, "epoch": "120/300", "eta": "0:09:26", "gpu_mem": "5.93G", "grad_norm": 1.64159, "loss": 1.98362, "lr": 0.06553, "top1_err": 54.16667, "top5_err": 16.66667}
[11/07 13:16:54][INFO] train_net.py:  696: Epoch 119 takes 4.12s. Epochs from 10 to 119 take 4.17s in average and 4.08s in median.
[11/07 13:16:54][INFO] train_net.py:  702: For epoch 119, each iteraction takes 0.69s in average. From epoch 10 to 119, each iteraction takes 0.70s in average.
[11/07 13:16:54][INFO] precise_bn.py:  129: Computing precise BN statistics for 84 BN layers ...
[11/07 13:17:00][INFO] logging.py:   99: json_stats: {"RAM": "41.71/501.50G", "_type": "val_epoch", "epoch": "120/300", "gpu_mem": "5.93G", "min_top1_err": 73.91304, "min_top5_err": 26.08696, "time_diff": 0.35730, "top1_err": 91.30435, "top5_err": 26.08696}
[11/07 13:17:04][INFO] logging.py:   99: json_stats: {"RAM": "41.70/501.50G", "_type": "train_epoch", "dt": 0.50828, "dt_data": 0.50828, "dt_net": 0.24286, "epoch": "121/300", "eta": "0:09:05", "gpu_mem": "5.93G", "grad_norm": 1.21540, "loss": 1.69354, "lr": 0.06504, "top1_err": 50.00000, "top5_err": 16.66667}
[11/07 13:17:04][INFO] train_net.py:  696: Epoch 120 takes 4.09s. Epochs from 10 to 120 take 4.17s in average and 4.08s in median.
[11/07 13:17:04][INFO] train_net.py:  702: For epoch 120, each iteraction takes 0.68s in average. From epoch 10 to 120, each iteraction takes 0.70s in average.
[11/07 13:17:08][INFO] logging.py:   99: json_stats: {"RAM": "39.99/501.50G", "_type": "train_epoch", "dt": 0.52481, "dt_data": 0.52481, "dt_net": 0.24264, "epoch": "122/300", "eta": "0:09:20", "gpu_mem": "5.93G", "grad_norm": 1.01981, "loss": 1.64102, "lr": 0.06454, "top1_err": 45.83333, "top5_err": 12.50000}
[11/07 13:17:08][INFO] train_net.py:  696: Epoch 121 takes 4.09s. Epochs from 10 to 121 take 4.17s in average and 4.08s in median.
[11/07 13:17:08][INFO] train_net.py:  702: For epoch 121, each iteraction takes 0.68s in average. From epoch 10 to 121, each iteraction takes 0.70s in average.
[11/07 13:17:12][INFO] logging.py:   99: json_stats: {"RAM": "41.70/501.50G", "_type": "train_epoch", "dt": 0.52880, "dt_data": 0.52880, "dt_net": 0.24316, "epoch": "123/300", "eta": "0:09:21", "gpu_mem": "5.93G", "grad_norm": 1.45973, "loss": 1.59526, "lr": 0.06403, "top1_err": 45.83333, "top5_err": 4.16667}
[11/07 13:17:12][INFO] train_net.py:  696: Epoch 122 takes 4.09s. Epochs from 10 to 122 take 4.17s in average and 4.08s in median.
[11/07 13:17:12][INFO] train_net.py:  702: For epoch 122, each iteraction takes 0.68s in average. From epoch 10 to 122, each iteraction takes 0.70s in average.
[11/07 13:17:16][INFO] logging.py:   99: json_stats: {"RAM": "41.71/501.50G", "_type": "train_epoch", "dt": 0.49711, "dt_data": 0.49711, "dt_net": 0.24259, "epoch": "124/300", "eta": "0:08:44", "gpu_mem": "5.93G", "grad_norm": 3.07556, "loss": 1.70857, "lr": 0.06353, "top1_err": 54.16667, "top5_err": 12.50000}
[11/07 13:17:16][INFO] train_net.py:  696: Epoch 123 takes 4.01s. Epochs from 10 to 123 take 4.17s in average and 4.08s in median.
[11/07 13:17:16][INFO] train_net.py:  702: For epoch 123, each iteraction takes 0.67s in average. From epoch 10 to 123, each iteraction takes 0.69s in average.
[11/07 13:17:20][INFO] logging.py:   99: json_stats: {"RAM": "41.72/501.50G", "_type": "train_epoch", "dt": 0.49652, "dt_data": 0.49652, "dt_net": 0.24296, "epoch": "125/300", "eta": "0:08:41", "gpu_mem": "5.93G", "grad_norm": 1.34543, "loss": 1.65804, "lr": 0.06303, "top1_err": 41.66667, "top5_err": 25.00000}
[11/07 13:17:20][INFO] train_net.py:  696: Epoch 124 takes 4.03s. Epochs from 10 to 124 take 4.17s in average and 4.08s in median.
[11/07 13:17:20][INFO] train_net.py:  702: For epoch 124, each iteraction takes 0.67s in average. From epoch 10 to 124, each iteraction takes 0.69s in average.
[11/07 13:17:24][INFO] logging.py:   99: json_stats: {"RAM": "41.71/501.50G", "_type": "train_epoch", "dt": 0.50427, "dt_data": 0.50427, "dt_net": 0.24311, "epoch": "126/300", "eta": "0:08:46", "gpu_mem": "5.93G", "grad_norm": 1.90535, "loss": 1.66463, "lr": 0.06252, "top1_err": 45.83333, "top5_err": 25.00000}
[11/07 13:17:24][INFO] train_net.py:  696: Epoch 125 takes 3.99s. Epochs from 10 to 125 take 4.17s in average and 4.08s in median.
[11/07 13:17:24][INFO] train_net.py:  702: For epoch 125, each iteraction takes 0.67s in average. From epoch 10 to 125, each iteraction takes 0.69s in average.
[11/07 13:17:28][INFO] logging.py:   99: json_stats: {"RAM": "41.66/501.50G", "_type": "train_epoch", "dt": 0.48684, "dt_data": 0.48684, "dt_net": 0.24336, "epoch": "127/300", "eta": "0:08:25", "gpu_mem": "5.93G", "grad_norm": 1.00782, "loss": 1.39221, "lr": 0.06201, "top1_err": 45.83333, "top5_err": 12.50000}
[11/07 13:17:28][INFO] train_net.py:  696: Epoch 126 takes 4.01s. Epochs from 10 to 126 take 4.17s in average and 4.08s in median.
[11/07 13:17:28][INFO] train_net.py:  702: For epoch 126, each iteraction takes 0.67s in average. From epoch 10 to 126, each iteraction takes 0.69s in average.
[11/07 13:17:32][INFO] logging.py:   99: json_stats: {"RAM": "41.70/501.50G", "_type": "train_epoch", "dt": 0.49568, "dt_data": 0.49568, "dt_net": 0.24313, "epoch": "128/300", "eta": "0:08:31", "gpu_mem": "5.93G", "grad_norm": 3.35714, "loss": 1.52535, "lr": 0.06150, "top1_err": 45.83333, "top5_err": 12.50000}
[11/07 13:17:32][INFO] train_net.py:  696: Epoch 127 takes 3.97s. Epochs from 10 to 127 take 4.16s in average and 4.08s in median.
[11/07 13:17:32][INFO] train_net.py:  702: For epoch 127, each iteraction takes 0.66s in average. From epoch 10 to 127, each iteraction takes 0.69s in average.
[11/07 13:17:36][INFO] logging.py:   99: json_stats: {"RAM": "41.70/501.50G", "_type": "train_epoch", "dt": 0.50032, "dt_data": 0.50032, "dt_net": 0.24328, "epoch": "129/300", "eta": "0:08:33", "gpu_mem": "5.93G", "grad_norm": 3.03612, "loss": 1.59507, "lr": 0.06099, "top1_err": 41.66667, "top5_err": 20.83333}
[11/07 13:17:36][INFO] train_net.py:  696: Epoch 128 takes 4.04s. Epochs from 10 to 128 take 4.16s in average and 4.08s in median.
[11/07 13:17:36][INFO] train_net.py:  702: For epoch 128, each iteraction takes 0.67s in average. From epoch 10 to 128, each iteraction takes 0.69s in average.
[11/07 13:17:40][INFO] logging.py:   99: json_stats: {"RAM": "41.68/501.50G", "_type": "train_epoch", "dt": 0.49783, "dt_data": 0.49783, "dt_net": 0.24327, "epoch": "130/300", "eta": "0:08:27", "gpu_mem": "5.93G", "grad_norm": 1.75338, "loss": 1.48844, "lr": 0.06048, "top1_err": 50.00000, "top5_err": 16.66667}
[11/07 13:17:40][INFO] train_net.py:  696: Epoch 129 takes 4.05s. Epochs from 10 to 129 take 4.16s in average and 4.08s in median.
[11/07 13:17:40][INFO] train_net.py:  702: For epoch 129, each iteraction takes 0.67s in average. From epoch 10 to 129, each iteraction takes 0.69s in average.
[11/07 13:17:40][INFO] precise_bn.py:  129: Computing precise BN statistics for 84 BN layers ...
[11/07 13:17:46][INFO] logging.py:   99: json_stats: {"RAM": "40.97/501.50G", "_type": "val_epoch", "epoch": "130/300", "gpu_mem": "5.93G", "min_top1_err": 73.91304, "min_top5_err": 26.08696, "time_diff": 0.36524, "top1_err": 100.00000, "top5_err": 26.08696}
[11/07 13:17:50][INFO] logging.py:   99: json_stats: {"RAM": "41.70/501.50G", "_type": "train_epoch", "dt": 0.50959, "dt_data": 0.50958, "dt_net": 0.24286, "epoch": "131/300", "eta": "0:08:36", "gpu_mem": "5.93G", "grad_norm": 1.08978, "loss": 1.37219, "lr": 0.05997, "top1_err": 45.83333, "top5_err": 12.50000}
[11/07 13:17:50][INFO] train_net.py:  696: Epoch 130 takes 4.10s. Epochs from 10 to 130 take 4.16s in average and 4.08s in median.
[11/07 13:17:50][INFO] train_net.py:  702: For epoch 130, each iteraction takes 0.68s in average. From epoch 10 to 130, each iteraction takes 0.69s in average.
[11/07 13:17:54][INFO] logging.py:   99: json_stats: {"RAM": "41.72/501.50G", "_type": "train_epoch", "dt": 0.49698, "dt_data": 0.49698, "dt_net": 0.24286, "epoch": "132/300", "eta": "0:08:20", "gpu_mem": "5.93G", "grad_norm": 2.74733, "loss": 1.43039, "lr": 0.05945, "top1_err": 45.83333, "top5_err": 12.50000}
[11/07 13:17:54][INFO] train_net.py:  696: Epoch 131 takes 4.09s. Epochs from 10 to 131 take 4.16s in average and 4.08s in median.
[11/07 13:17:54][INFO] train_net.py:  702: For epoch 131, each iteraction takes 0.68s in average. From epoch 10 to 131, each iteraction takes 0.69s in average.
[11/07 13:17:58][INFO] logging.py:   99: json_stats: {"RAM": "41.75/501.50G", "_type": "train_epoch", "dt": 0.51336, "dt_data": 0.51335, "dt_net": 0.24388, "epoch": "133/300", "eta": "0:08:34", "gpu_mem": "5.93G", "grad_norm": 1.25973, "loss": 1.26466, "lr": 0.05894, "top1_err": 37.50000, "top5_err": 12.50000}
[11/07 13:17:58][INFO] train_net.py:  696: Epoch 132 takes 4.07s. Epochs from 10 to 132 take 4.16s in average and 4.08s in median.
[11/07 13:17:58][INFO] train_net.py:  702: For epoch 132, each iteraction takes 0.68s in average. From epoch 10 to 132, each iteraction takes 0.69s in average.
[11/07 13:18:02][INFO] logging.py:   99: json_stats: {"RAM": "41.72/501.50G", "_type": "train_epoch", "dt": 0.50336, "dt_data": 0.50336, "dt_net": 0.24272, "epoch": "134/300", "eta": "0:08:21", "gpu_mem": "5.93G", "grad_norm": 2.08389, "loss": 1.63409, "lr": 0.05842, "top1_err": 50.00000, "top5_err": 20.83333}
[11/07 13:18:02][INFO] train_net.py:  696: Epoch 133 takes 4.04s. Epochs from 10 to 133 take 4.16s in average and 4.08s in median.
[11/07 13:18:02][INFO] train_net.py:  702: For epoch 133, each iteraction takes 0.67s in average. From epoch 10 to 133, each iteraction takes 0.69s in average.
[11/07 13:18:06][INFO] logging.py:   99: json_stats: {"RAM": "41.72/501.50G", "_type": "train_epoch", "dt": 0.50019, "dt_data": 0.50019, "dt_net": 0.24334, "epoch": "135/300", "eta": "0:08:15", "gpu_mem": "5.93G", "grad_norm": 1.31124, "loss": 1.26665, "lr": 0.05791, "top1_err": 41.66667, "top5_err": 12.50000}
[11/07 13:18:06][INFO] train_net.py:  696: Epoch 134 takes 4.11s. Epochs from 10 to 134 take 4.16s in average and 4.08s in median.
[11/07 13:18:06][INFO] train_net.py:  702: For epoch 134, each iteraction takes 0.69s in average. From epoch 10 to 134, each iteraction takes 0.69s in average.
[11/07 13:18:10][INFO] logging.py:   99: json_stats: {"RAM": "41.71/501.50G", "_type": "train_epoch", "dt": 0.49175, "dt_data": 0.49175, "dt_net": 0.24271, "epoch": "136/300", "eta": "0:08:03", "gpu_mem": "5.93G", "grad_norm": 4.62083, "loss": 1.65646, "lr": 0.05739, "top1_err": 54.16667, "top5_err": 12.50000}
[11/07 13:18:10][INFO] train_net.py:  696: Epoch 135 takes 3.98s. Epochs from 10 to 135 take 4.16s in average and 4.08s in median.
[11/07 13:18:10][INFO] train_net.py:  702: For epoch 135, each iteraction takes 0.66s in average. From epoch 10 to 135, each iteraction takes 0.69s in average.
[11/07 13:18:14][INFO] logging.py:   99: json_stats: {"RAM": "41.71/501.50G", "_type": "train_epoch", "dt": 0.50069, "dt_data": 0.50069, "dt_net": 0.24304, "epoch": "137/300", "eta": "0:08:09", "gpu_mem": "5.93G", "grad_norm": 1.97614, "loss": 1.59658, "lr": 0.05687, "top1_err": 50.00000, "top5_err": 20.83333}
[11/07 13:18:14][INFO] train_net.py:  696: Epoch 136 takes 4.09s. Epochs from 10 to 136 take 4.16s in average and 4.08s in median.
[11/07 13:18:14][INFO] train_net.py:  702: For epoch 136, each iteraction takes 0.68s in average. From epoch 10 to 136, each iteraction takes 0.69s in average.
[11/07 13:18:18][INFO] logging.py:   99: json_stats: {"RAM": "41.68/501.50G", "_type": "train_epoch", "dt": 0.51285, "dt_data": 0.51285, "dt_net": 0.24360, "epoch": "138/300", "eta": "0:08:18", "gpu_mem": "5.93G", "grad_norm": 1.78466, "loss": 1.38184, "lr": 0.05635, "top1_err": 45.83333, "top5_err": 12.50000}
[11/07 13:18:18][INFO] train_net.py:  696: Epoch 137 takes 3.98s. Epochs from 10 to 137 take 4.16s in average and 4.08s in median.
[11/07 13:18:18][INFO] train_net.py:  702: For epoch 137, each iteraction takes 0.66s in average. From epoch 10 to 137, each iteraction takes 0.69s in average.
[11/07 13:18:22][INFO] logging.py:   99: json_stats: {"RAM": "41.12/501.50G", "_type": "train_epoch", "dt": 0.50873, "dt_data": 0.50873, "dt_net": 0.24358, "epoch": "139/300", "eta": "0:08:11", "gpu_mem": "5.93G", "grad_norm": 1.73024, "loss": 1.55354, "lr": 0.05583, "top1_err": 50.00000, "top5_err": 12.50000}
[11/07 13:18:22][INFO] train_net.py:  696: Epoch 138 takes 4.22s. Epochs from 10 to 138 take 4.16s in average and 4.08s in median.
[11/07 13:18:22][INFO] train_net.py:  702: For epoch 138, each iteraction takes 0.70s in average. From epoch 10 to 138, each iteraction takes 0.69s in average.
[11/07 13:18:26][INFO] logging.py:   99: json_stats: {"RAM": "41.71/501.50G", "_type": "train_epoch", "dt": 0.51724, "dt_data": 0.51724, "dt_net": 0.24288, "epoch": "140/300", "eta": "0:08:16", "gpu_mem": "5.93G", "grad_norm": 5.05713, "loss": 1.58642, "lr": 0.05531, "top1_err": 54.16667, "top5_err": 12.50000}
[11/07 13:18:26][INFO] train_net.py:  696: Epoch 139 takes 4.10s. Epochs from 10 to 139 take 4.16s in average and 4.08s in median.
[11/07 13:18:26][INFO] train_net.py:  702: For epoch 139, each iteraction takes 0.68s in average. From epoch 10 to 139, each iteraction takes 0.69s in average.
[11/07 13:18:26][INFO] precise_bn.py:  129: Computing precise BN statistics for 84 BN layers ...
[11/07 13:18:32][INFO] logging.py:   99: json_stats: {"RAM": "41.75/501.50G", "_type": "val_epoch", "epoch": "140/300", "gpu_mem": "5.93G", "min_top1_err": 73.91304, "min_top5_err": 26.08696, "time_diff": 0.36253, "top1_err": 86.95652, "top5_err": 30.43478}
[11/07 13:18:36][INFO] logging.py:   99: json_stats: {"RAM": "41.71/501.50G", "_type": "train_epoch", "dt": 0.53532, "dt_data": 0.53532, "dt_net": 0.24317, "epoch": "141/300", "eta": "0:08:30", "gpu_mem": "5.93G", "grad_norm": 1.32246, "loss": 1.44153, "lr": 0.05479, "top1_err": 41.66667, "top5_err": 12.50000}
[11/07 13:18:36][INFO] train_net.py:  696: Epoch 140 takes 4.08s. Epochs from 10 to 140 take 4.15s in average and 4.08s in median.
[11/07 13:18:36][INFO] train_net.py:  702: For epoch 140, each iteraction takes 0.68s in average. From epoch 10 to 140, each iteraction takes 0.69s in average.
[11/07 13:18:40][INFO] logging.py:   99: json_stats: {"RAM": "41.71/501.50G", "_type": "train_epoch", "dt": 0.50238, "dt_data": 0.50238, "dt_net": 0.24292, "epoch": "142/300", "eta": "0:07:56", "gpu_mem": "5.93G", "grad_norm": 1.90661, "loss": 1.39838, "lr": 0.05427, "top1_err": 41.66667, "top5_err": 20.83333}
[11/07 13:18:40][INFO] train_net.py:  696: Epoch 141 takes 4.09s. Epochs from 10 to 141 take 4.15s in average and 4.08s in median.
[11/07 13:18:40][INFO] train_net.py:  702: For epoch 141, each iteraction takes 0.68s in average. From epoch 10 to 141, each iteraction takes 0.69s in average.
[11/07 13:18:44][INFO] logging.py:   99: json_stats: {"RAM": "41.70/501.50G", "_type": "train_epoch", "dt": 0.51461, "dt_data": 0.51461, "dt_net": 0.24262, "epoch": "143/300", "eta": "0:08:04", "gpu_mem": "5.93G", "grad_norm": 3.63711, "loss": 1.33480, "lr": 0.05375, "top1_err": 45.83333, "top5_err": 16.66667}
[11/07 13:18:44][INFO] train_net.py:  696: Epoch 142 takes 4.04s. Epochs from 10 to 142 take 4.15s in average and 4.08s in median.
[11/07 13:18:44][INFO] train_net.py:  702: For epoch 142, each iteraction takes 0.67s in average. From epoch 10 to 142, each iteraction takes 0.69s in average.
[11/07 13:18:48][INFO] logging.py:   99: json_stats: {"RAM": "41.72/501.50G", "_type": "train_epoch", "dt": 0.50886, "dt_data": 0.50886, "dt_net": 0.24326, "epoch": "144/300", "eta": "0:07:56", "gpu_mem": "5.93G", "grad_norm": 1.10981, "loss": 1.60296, "lr": 0.05323, "top1_err": 41.66667, "top5_err": 12.50000}
[11/07 13:18:48][INFO] train_net.py:  696: Epoch 143 takes 4.11s. Epochs from 10 to 143 take 4.15s in average and 4.08s in median.
[11/07 13:18:48][INFO] train_net.py:  702: For epoch 143, each iteraction takes 0.69s in average. From epoch 10 to 143, each iteraction takes 0.69s in average.
[11/07 13:18:52][INFO] logging.py:   99: json_stats: {"RAM": "41.70/501.50G", "_type": "train_epoch", "dt": 0.50110, "dt_data": 0.50110, "dt_net": 0.24305, "epoch": "145/300", "eta": "0:07:45", "gpu_mem": "5.93G", "grad_norm": 1.47193, "loss": 1.35327, "lr": 0.05270, "top1_err": 41.66667, "top5_err": 8.33333}
[11/07 13:18:52][INFO] train_net.py:  696: Epoch 144 takes 4.09s. Epochs from 10 to 144 take 4.15s in average and 4.08s in median.
[11/07 13:18:52][INFO] train_net.py:  702: For epoch 144, each iteraction takes 0.68s in average. From epoch 10 to 144, each iteraction takes 0.69s in average.
[11/07 13:18:56][INFO] logging.py:   99: json_stats: {"RAM": "39.86/501.50G", "_type": "train_epoch", "dt": 0.51202, "dt_data": 0.51202, "dt_net": 0.24286, "epoch": "146/300", "eta": "0:07:53", "gpu_mem": "5.93G", "grad_norm": 0.77921, "loss": 1.30906, "lr": 0.05218, "top1_err": 41.66667, "top5_err": 20.83333}
[11/07 13:18:56][INFO] train_net.py:  696: Epoch 145 takes 4.09s. Epochs from 10 to 145 take 4.15s in average and 4.08s in median.
[11/07 13:18:56][INFO] train_net.py:  702: For epoch 145, each iteraction takes 0.68s in average. From epoch 10 to 145, each iteraction takes 0.69s in average.
[11/07 13:19:01][INFO] logging.py:   99: json_stats: {"RAM": "41.68/501.50G", "_type": "train_epoch", "dt": 0.59131, "dt_data": 0.59131, "dt_net": 0.24366, "epoch": "147/300", "eta": "0:09:02", "gpu_mem": "5.93G", "grad_norm": 2.30117, "loss": 1.13627, "lr": 0.05166, "top1_err": 37.50000, "top5_err": 4.16667}
[11/07 13:19:01][INFO] train_net.py:  696: Epoch 146 takes 4.24s. Epochs from 10 to 146 take 4.15s in average and 4.08s in median.
[11/07 13:19:01][INFO] train_net.py:  702: For epoch 146, each iteraction takes 0.71s in average. From epoch 10 to 146, each iteraction takes 0.69s in average.
[11/07 13:19:05][INFO] logging.py:   99: json_stats: {"RAM": "41.73/501.50G", "_type": "train_epoch", "dt": 0.51030, "dt_data": 0.51030, "dt_net": 0.24248, "epoch": "148/300", "eta": "0:07:45", "gpu_mem": "5.93G", "grad_norm": 2.67747, "loss": 1.28706, "lr": 0.05113, "top1_err": 41.66667, "top5_err": 12.50000}
[11/07 13:19:05][INFO] train_net.py:  696: Epoch 147 takes 4.04s. Epochs from 10 to 147 take 4.15s in average and 4.08s in median.
[11/07 13:19:05][INFO] train_net.py:  702: For epoch 147, each iteraction takes 0.67s in average. From epoch 10 to 147, each iteraction takes 0.69s in average.
[11/07 13:19:09][INFO] logging.py:   99: json_stats: {"RAM": "41.71/501.50G", "_type": "train_epoch", "dt": 0.51066, "dt_data": 0.51066, "dt_net": 0.24287, "epoch": "149/300", "eta": "0:07:42", "gpu_mem": "5.93G", "grad_norm": 3.57096, "loss": 1.24526, "lr": 0.05061, "top1_err": 41.66667, "top5_err": 4.16667}
[11/07 13:19:09][INFO] train_net.py:  696: Epoch 148 takes 4.13s. Epochs from 10 to 148 take 4.15s in average and 4.08s in median.
[11/07 13:19:09][INFO] train_net.py:  702: For epoch 148, each iteraction takes 0.69s in average. From epoch 10 to 148, each iteraction takes 0.69s in average.
[11/07 13:19:13][INFO] logging.py:   99: json_stats: {"RAM": "41.70/501.50G", "_type": "train_epoch", "dt": 0.50860, "dt_data": 0.50860, "dt_net": 0.24340, "epoch": "150/300", "eta": "0:07:37", "gpu_mem": "5.93G", "grad_norm": 4.75523, "loss": 1.20490, "lr": 0.05009, "top1_err": 45.83333, "top5_err": 4.16667}
[11/07 13:19:13][INFO] train_net.py:  696: Epoch 149 takes 4.05s. Epochs from 10 to 149 take 4.15s in average and 4.08s in median.
[11/07 13:19:13][INFO] train_net.py:  702: For epoch 149, each iteraction takes 0.68s in average. From epoch 10 to 149, each iteraction takes 0.69s in average.
[11/07 13:19:13][INFO] precise_bn.py:  129: Computing precise BN statistics for 84 BN layers ...
[11/07 13:19:18][INFO] logging.py:   99: json_stats: {"RAM": "41.72/501.50G", "_type": "val_epoch", "epoch": "150/300", "gpu_mem": "5.93G", "min_top1_err": 73.91304, "min_top5_err": 13.04348, "time_diff": 0.37745, "top1_err": 86.95652, "top5_err": 13.04348}
[11/07 13:19:23][INFO] logging.py:   99: json_stats: {"RAM": "41.70/501.50G", "_type": "train_epoch", "dt": 0.51078, "dt_data": 0.51077, "dt_net": 0.24339, "epoch": "151/300", "eta": "0:07:36", "gpu_mem": "5.93G", "grad_norm": 1.69810, "loss": 1.24777, "lr": 0.04956, "top1_err": 45.83333, "top5_err": 12.50000}
[11/07 13:19:23][INFO] train_net.py:  696: Epoch 150 takes 4.04s. Epochs from 10 to 150 take 4.15s in average and 4.08s in median.
[11/07 13:19:23][INFO] train_net.py:  702: For epoch 150, each iteraction takes 0.67s in average. From epoch 10 to 150, each iteraction takes 0.69s in average.
[11/07 13:19:27][INFO] logging.py:   99: json_stats: {"RAM": "41.69/501.50G", "_type": "train_epoch", "dt": 0.49848, "dt_data": 0.49848, "dt_net": 0.24261, "epoch": "152/300", "eta": "0:07:22", "gpu_mem": "5.93G", "grad_norm": 1.59534, "loss": 1.13946, "lr": 0.04904, "top1_err": 41.66667, "top5_err": 12.50000}
[11/07 13:19:27][INFO] train_net.py:  696: Epoch 151 takes 4.09s. Epochs from 10 to 151 take 4.15s in average and 4.08s in median.
[11/07 13:19:27][INFO] train_net.py:  702: For epoch 151, each iteraction takes 0.68s in average. From epoch 10 to 151, each iteraction takes 0.69s in average.
[11/07 13:19:31][INFO] logging.py:   99: json_stats: {"RAM": "41.67/501.50G", "_type": "train_epoch", "dt": 0.54167, "dt_data": 0.54167, "dt_net": 0.24332, "epoch": "153/300", "eta": "0:07:57", "gpu_mem": "5.93G", "grad_norm": 1.07306, "loss": 1.36776, "lr": 0.04852, "top1_err": 41.66667, "top5_err": 8.33333}
[11/07 13:19:31][INFO] train_net.py:  696: Epoch 152 takes 4.08s. Epochs from 10 to 152 take 4.15s in average and 4.08s in median.
[11/07 13:19:31][INFO] train_net.py:  702: For epoch 152, each iteraction takes 0.68s in average. From epoch 10 to 152, each iteraction takes 0.69s in average.
[11/07 13:19:35][INFO] logging.py:   99: json_stats: {"RAM": "41.06/501.50G", "_type": "train_epoch", "dt": 0.51768, "dt_data": 0.51768, "dt_net": 0.24342, "epoch": "154/300", "eta": "0:07:33", "gpu_mem": "5.93G", "grad_norm": 3.52088, "loss": 1.51509, "lr": 0.04799, "top1_err": 45.83333, "top5_err": 20.83333}
[11/07 13:19:35][INFO] train_net.py:  696: Epoch 153 takes 4.16s. Epochs from 10 to 153 take 4.15s in average and 4.08s in median.
[11/07 13:19:35][INFO] train_net.py:  702: For epoch 153, each iteraction takes 0.69s in average. From epoch 10 to 153, each iteraction takes 0.69s in average.
[11/07 13:19:39][INFO] logging.py:   99: json_stats: {"RAM": "41.65/501.50G", "_type": "train_epoch", "dt": 0.50750, "dt_data": 0.50750, "dt_net": 0.24319, "epoch": "155/300", "eta": "0:07:21", "gpu_mem": "5.93G", "grad_norm": 1.60992, "loss": 1.25356, "lr": 0.04747, "top1_err": 41.66667, "top5_err": 12.50000}
[11/07 13:19:39][INFO] train_net.py:  696: Epoch 154 takes 4.11s. Epochs from 10 to 154 take 4.15s in average and 4.08s in median.
[11/07 13:19:39][INFO] train_net.py:  702: For epoch 154, each iteraction takes 0.68s in average. From epoch 10 to 154, each iteraction takes 0.69s in average.
[11/07 13:19:43][INFO] logging.py:   99: json_stats: {"RAM": "41.66/501.50G", "_type": "train_epoch", "dt": 0.52261, "dt_data": 0.52261, "dt_net": 0.24231, "epoch": "156/300", "eta": "0:07:31", "gpu_mem": "5.93G", "grad_norm": 1.46695, "loss": 1.30099, "lr": 0.04695, "top1_err": 41.66667, "top5_err": 8.33333}
[11/07 13:19:43][INFO] train_net.py:  696: Epoch 155 takes 4.05s. Epochs from 10 to 155 take 4.15s in average and 4.08s in median.
[11/07 13:19:43][INFO] train_net.py:  702: For epoch 155, each iteraction takes 0.68s in average. From epoch 10 to 155, each iteraction takes 0.69s in average.
[11/07 13:19:47][INFO] logging.py:   99: json_stats: {"RAM": "41.66/501.50G", "_type": "train_epoch", "dt": 0.50202, "dt_data": 0.50202, "dt_net": 0.24278, "epoch": "157/300", "eta": "0:07:10", "gpu_mem": "5.93G", "grad_norm": 1.85177, "loss": 1.15062, "lr": 0.04643, "top1_err": 33.33333, "top5_err": 4.16667}
[11/07 13:19:47][INFO] train_net.py:  696: Epoch 156 takes 4.09s. Epochs from 10 to 156 take 4.15s in average and 4.08s in median.
[11/07 13:19:47][INFO] train_net.py:  702: For epoch 156, each iteraction takes 0.68s in average. From epoch 10 to 156, each iteraction takes 0.69s in average.
[11/07 13:19:51][INFO] logging.py:   99: json_stats: {"RAM": "41.66/501.50G", "_type": "train_epoch", "dt": 0.52444, "dt_data": 0.52444, "dt_net": 0.24288, "epoch": "158/300", "eta": "0:07:26", "gpu_mem": "5.93G", "grad_norm": 1.88455, "loss": 1.28009, "lr": 0.04590, "top1_err": 50.00000, "top5_err": 12.50000}
[11/07 13:19:51][INFO] train_net.py:  696: Epoch 157 takes 4.03s. Epochs from 10 to 157 take 4.15s in average and 4.08s in median.
[11/07 13:19:51][INFO] train_net.py:  702: For epoch 157, each iteraction takes 0.67s in average. From epoch 10 to 157, each iteraction takes 0.69s in average.
[11/07 13:19:55][INFO] logging.py:   99: json_stats: {"RAM": "41.65/501.50G", "_type": "train_epoch", "dt": 0.50712, "dt_data": 0.50712, "dt_net": 0.24269, "epoch": "159/300", "eta": "0:07:08", "gpu_mem": "5.93G", "grad_norm": 0.88076, "loss": 1.19629, "lr": 0.04538, "top1_err": 41.66667, "top5_err": 8.33333}
[11/07 13:19:55][INFO] train_net.py:  696: Epoch 158 takes 4.10s. Epochs from 10 to 158 take 4.15s in average and 4.08s in median.
[11/07 13:19:55][INFO] train_net.py:  702: For epoch 158, each iteraction takes 0.68s in average. From epoch 10 to 158, each iteraction takes 0.69s in average.
[11/07 13:19:59][INFO] logging.py:   99: json_stats: {"RAM": "41.66/501.50G", "_type": "train_epoch", "dt": 0.51168, "dt_data": 0.51168, "dt_net": 0.24329, "epoch": "160/300", "eta": "0:07:09", "gpu_mem": "5.93G", "grad_norm": 3.38245, "loss": 1.25274, "lr": 0.04486, "top1_err": 41.66667, "top5_err": 8.33333}
[11/07 13:19:59][INFO] train_net.py:  696: Epoch 159 takes 4.01s. Epochs from 10 to 159 take 4.15s in average and 4.08s in median.
[11/07 13:19:59][INFO] train_net.py:  702: For epoch 159, each iteraction takes 0.67s in average. From epoch 10 to 159, each iteraction takes 0.69s in average.
[11/07 13:19:59][INFO] precise_bn.py:  129: Computing precise BN statistics for 84 BN layers ...
[11/07 13:20:05][INFO] logging.py:   99: json_stats: {"RAM": "41.65/501.50G", "_type": "val_epoch", "epoch": "160/300", "gpu_mem": "5.93G", "min_top1_err": 73.91304, "min_top5_err": 13.04348, "time_diff": 0.36915, "top1_err": 86.95652, "top5_err": 21.73913}
[11/07 13:20:09][INFO] logging.py:   99: json_stats: {"RAM": "39.81/501.50G", "_type": "train_epoch", "dt": 0.52751, "dt_data": 0.52752, "dt_net": 0.24364, "epoch": "161/300", "eta": "0:07:19", "gpu_mem": "5.93G", "grad_norm": 3.08627, "loss": 1.26170, "lr": 0.04434, "top1_err": 41.66667, "top5_err": 8.33333}
[11/07 13:20:09][INFO] train_net.py:  696: Epoch 160 takes 4.02s. Epochs from 10 to 160 take 4.15s in average and 4.08s in median.
[11/07 13:20:09][INFO] train_net.py:  702: For epoch 160, each iteraction takes 0.67s in average. From epoch 10 to 160, each iteraction takes 0.69s in average.
[11/07 13:20:13][INFO] logging.py:   99: json_stats: {"RAM": "41.63/501.50G", "_type": "train_epoch", "dt": 0.51548, "dt_data": 0.51548, "dt_net": 0.24337, "epoch": "162/300", "eta": "0:07:06", "gpu_mem": "5.93G", "grad_norm": 1.04566, "loss": 1.16621, "lr": 0.04382, "top1_err": 41.66667, "top5_err": 12.50000}
[11/07 13:20:13][INFO] train_net.py:  696: Epoch 161 takes 4.16s. Epochs from 10 to 161 take 4.15s in average and 4.08s in median.
[11/07 13:20:13][INFO] train_net.py:  702: For epoch 161, each iteraction takes 0.69s in average. From epoch 10 to 161, each iteraction takes 0.69s in average.
[11/07 13:20:17][INFO] logging.py:   99: json_stats: {"RAM": "41.68/501.50G", "_type": "train_epoch", "dt": 0.49725, "dt_data": 0.49725, "dt_net": 0.24267, "epoch": "163/300", "eta": "0:06:48", "gpu_mem": "5.93G", "grad_norm": 0.90177, "loss": 1.13297, "lr": 0.04330, "top1_err": 41.66667, "top5_err": 12.50000}
[11/07 13:20:17][INFO] train_net.py:  696: Epoch 162 takes 4.15s. Epochs from 10 to 162 take 4.15s in average and 4.08s in median.
[11/07 13:20:17][INFO] train_net.py:  702: For epoch 162, each iteraction takes 0.69s in average. From epoch 10 to 162, each iteraction takes 0.69s in average.
[11/07 13:20:21][INFO] logging.py:   99: json_stats: {"RAM": "41.71/501.50G", "_type": "train_epoch", "dt": 0.53241, "dt_data": 0.53241, "dt_net": 0.24326, "epoch": "164/300", "eta": "0:07:14", "gpu_mem": "5.93G", "grad_norm": 2.59119, "loss": 1.54687, "lr": 0.04278, "top1_err": 41.66667, "top5_err": 8.33333}
[11/07 13:20:21][INFO] train_net.py:  696: Epoch 163 takes 4.22s. Epochs from 10 to 163 take 4.15s in average and 4.08s in median.
[11/07 13:20:21][INFO] train_net.py:  702: For epoch 163, each iteraction takes 0.70s in average. From epoch 10 to 163, each iteraction takes 0.69s in average.
[11/07 13:20:25][INFO] logging.py:   99: json_stats: {"RAM": "41.73/501.50G", "_type": "train_epoch", "dt": 0.49945, "dt_data": 0.49945, "dt_net": 0.24255, "epoch": "165/300", "eta": "0:06:44", "gpu_mem": "5.93G", "grad_norm": 3.83835, "loss": 1.51797, "lr": 0.04226, "top1_err": 54.16667, "top5_err": 4.16667}
[11/07 13:20:25][INFO] train_net.py:  696: Epoch 164 takes 4.02s. Epochs from 10 to 164 take 4.15s in average and 4.08s in median.
[11/07 13:20:25][INFO] train_net.py:  702: For epoch 164, each iteraction takes 0.67s in average. From epoch 10 to 164, each iteraction takes 0.69s in average.
[11/07 13:20:30][INFO] logging.py:   99: json_stats: {"RAM": "41.73/501.50G", "_type": "train_epoch", "dt": 0.51275, "dt_data": 0.51275, "dt_net": 0.24341, "epoch": "166/300", "eta": "0:06:52", "gpu_mem": "5.93G", "grad_norm": 1.90318, "loss": 1.35538, "lr": 0.04175, "top1_err": 45.83333, "top5_err": 20.83333}
[11/07 13:20:30][INFO] train_net.py:  696: Epoch 165 takes 4.11s. Epochs from 10 to 165 take 4.14s in average and 4.08s in median.
[11/07 13:20:30][INFO] train_net.py:  702: For epoch 165, each iteraction takes 0.69s in average. From epoch 10 to 165, each iteraction takes 0.69s in average.
[11/07 13:20:34][INFO] logging.py:   99: json_stats: {"RAM": "41.73/501.50G", "_type": "train_epoch", "dt": 0.53438, "dt_data": 0.53438, "dt_net": 0.24314, "epoch": "167/300", "eta": "0:07:06", "gpu_mem": "5.93G", "grad_norm": 1.86340, "loss": 1.19072, "lr": 0.04123, "top1_err": 41.66667, "top5_err": 8.33333}
[11/07 13:20:34][INFO] train_net.py:  696: Epoch 166 takes 4.12s. Epochs from 10 to 166 take 4.14s in average and 4.08s in median.
[11/07 13:20:34][INFO] train_net.py:  702: For epoch 166, each iteraction takes 0.69s in average. From epoch 10 to 166, each iteraction takes 0.69s in average.
[11/07 13:20:38][INFO] logging.py:   99: json_stats: {"RAM": "41.73/501.50G", "_type": "train_epoch", "dt": 0.49042, "dt_data": 0.49042, "dt_net": 0.24297, "epoch": "168/300", "eta": "0:06:28", "gpu_mem": "5.93G", "grad_norm": 1.76625, "loss": 1.21226, "lr": 0.04072, "top1_err": 50.00000, "top5_err": 8.33333}
[11/07 13:20:38][INFO] train_net.py:  696: Epoch 167 takes 4.12s. Epochs from 10 to 167 take 4.14s in average and 4.08s in median.
[11/07 13:20:38][INFO] train_net.py:  702: For epoch 167, each iteraction takes 0.69s in average. From epoch 10 to 167, each iteraction takes 0.69s in average.
[11/07 13:20:42][INFO] logging.py:   99: json_stats: {"RAM": "41.71/501.50G", "_type": "train_epoch", "dt": 0.50819, "dt_data": 0.50819, "dt_net": 0.24259, "epoch": "169/300", "eta": "0:06:39", "gpu_mem": "5.93G", "grad_norm": 4.95393, "loss": 1.07929, "lr": 0.04020, "top1_err": 37.50000, "top5_err": 8.33333}
[11/07 13:20:42][INFO] train_net.py:  696: Epoch 168 takes 4.10s. Epochs from 10 to 168 take 4.14s in average and 4.08s in median.
[11/07 13:20:42][INFO] train_net.py:  702: For epoch 168, each iteraction takes 0.68s in average. From epoch 10 to 168, each iteraction takes 0.69s in average.
[11/07 13:20:46][INFO] logging.py:   99: json_stats: {"RAM": "39.97/501.50G", "_type": "train_epoch", "dt": 0.53879, "dt_data": 0.53879, "dt_net": 0.24323, "epoch": "170/300", "eta": "0:07:00", "gpu_mem": "5.93G", "grad_norm": 2.47043, "loss": 1.20811, "lr": 0.03969, "top1_err": 45.83333, "top5_err": 16.66667}
[11/07 13:20:46][INFO] train_net.py:  696: Epoch 169 takes 4.09s. Epochs from 10 to 169 take 4.14s in average and 4.08s in median.
[11/07 13:20:46][INFO] train_net.py:  702: For epoch 169, each iteraction takes 0.68s in average. From epoch 10 to 169, each iteraction takes 0.69s in average.
[11/07 13:20:46][INFO] precise_bn.py:  129: Computing precise BN statistics for 84 BN layers ...
[11/07 13:20:52][INFO] logging.py:   99: json_stats: {"RAM": "41.73/501.50G", "_type": "val_epoch", "epoch": "170/300", "gpu_mem": "5.93G", "min_top1_err": 73.91304, "min_top5_err": 13.04348, "time_diff": 0.36926, "top1_err": 86.95652, "top5_err": 26.08696}
[11/07 13:20:56][INFO] logging.py:   99: json_stats: {"RAM": "41.72/501.50G", "_type": "train_epoch", "dt": 0.51265, "dt_data": 0.51265, "dt_net": 0.24294, "epoch": "171/300", "eta": "0:06:36", "gpu_mem": "5.93G", "grad_norm": 3.79223, "loss": 0.84921, "lr": 0.03918, "top1_err": 29.16667, "top5_err": 4.16667}
[11/07 13:20:56][INFO] train_net.py:  696: Epoch 170 takes 3.98s. Epochs from 10 to 170 take 4.14s in average and 4.08s in median.
[11/07 13:20:56][INFO] train_net.py:  702: For epoch 170, each iteraction takes 0.66s in average. From epoch 10 to 170, each iteraction takes 0.69s in average.
[11/07 13:21:00][INFO] logging.py:   99: json_stats: {"RAM": "41.76/501.50G", "_type": "train_epoch", "dt": 0.52440, "dt_data": 0.52440, "dt_net": 0.24285, "epoch": "172/300", "eta": "0:06:42", "gpu_mem": "5.93G", "grad_norm": 1.22499, "loss": 0.86041, "lr": 0.03867, "top1_err": 33.33333, "top5_err": 0.00000}
[11/07 13:21:00][INFO] train_net.py:  696: Epoch 171 takes 4.09s. Epochs from 10 to 171 take 4.14s in average and 4.08s in median.
[11/07 13:21:00][INFO] train_net.py:  702: For epoch 171, each iteraction takes 0.68s in average. From epoch 10 to 171, each iteraction takes 0.69s in average.
[11/07 13:21:04][INFO] logging.py:   99: json_stats: {"RAM": "41.75/501.50G", "_type": "train_epoch", "dt": 0.48918, "dt_data": 0.48918, "dt_net": 0.24285, "epoch": "173/300", "eta": "0:06:12", "gpu_mem": "5.93G", "grad_norm": 2.78333, "loss": 1.14742, "lr": 0.03816, "top1_err": 41.66667, "top5_err": 4.16667}
[11/07 13:21:04][INFO] train_net.py:  696: Epoch 172 takes 4.01s. Epochs from 10 to 172 take 4.14s in average and 4.08s in median.
[11/07 13:21:04][INFO] train_net.py:  702: For epoch 172, each iteraction takes 0.67s in average. From epoch 10 to 172, each iteraction takes 0.69s in average.
[11/07 13:21:08][INFO] logging.py:   99: json_stats: {"RAM": "41.73/501.50G", "_type": "train_epoch", "dt": 0.50916, "dt_data": 0.50916, "dt_net": 0.24300, "epoch": "174/300", "eta": "0:06:24", "gpu_mem": "5.93G", "grad_norm": 2.79001, "loss": 1.10413, "lr": 0.03765, "top1_err": 33.33333, "top5_err": 8.33333}
[11/07 13:21:08][INFO] train_net.py:  696: Epoch 173 takes 4.06s. Epochs from 10 to 173 take 4.14s in average and 4.08s in median.
[11/07 13:21:08][INFO] train_net.py:  702: For epoch 173, each iteraction takes 0.68s in average. From epoch 10 to 173, each iteraction takes 0.69s in average.
[11/07 13:21:12][INFO] logging.py:   99: json_stats: {"RAM": "41.77/501.50G", "_type": "train_epoch", "dt": 0.49859, "dt_data": 0.49859, "dt_net": 0.24237, "epoch": "175/300", "eta": "0:06:13", "gpu_mem": "5.93G", "grad_norm": 2.19517, "loss": 0.83659, "lr": 0.03714, "top1_err": 33.33333, "top5_err": 4.16667}
[11/07 13:21:12][INFO] train_net.py:  696: Epoch 174 takes 4.00s. Epochs from 10 to 174 take 4.14s in average and 4.08s in median.
[11/07 13:21:12][INFO] train_net.py:  702: For epoch 174, each iteraction takes 0.67s in average. From epoch 10 to 174, each iteraction takes 0.69s in average.
[11/07 13:21:16][INFO] logging.py:   99: json_stats: {"RAM": "41.74/501.50G", "_type": "train_epoch", "dt": 0.50780, "dt_data": 0.50780, "dt_net": 0.24430, "epoch": "176/300", "eta": "0:06:17", "gpu_mem": "5.93G", "grad_norm": 1.87977, "loss": 0.85474, "lr": 0.03664, "top1_err": 29.16667, "top5_err": 8.33333}
[11/07 13:21:16][INFO] train_net.py:  696: Epoch 175 takes 4.05s. Epochs from 10 to 175 take 4.14s in average and 4.08s in median.
[11/07 13:21:16][INFO] train_net.py:  702: For epoch 175, each iteraction takes 0.68s in average. From epoch 10 to 175, each iteraction takes 0.69s in average.
[11/07 13:21:20][INFO] logging.py:   99: json_stats: {"RAM": "41.71/501.50G", "_type": "train_epoch", "dt": 0.49442, "dt_data": 0.49442, "dt_net": 0.24269, "epoch": "177/300", "eta": "0:06:04", "gpu_mem": "5.93G", "grad_norm": 3.57118, "loss": 0.79586, "lr": 0.03613, "top1_err": 25.00000, "top5_err": 8.33333}
[11/07 13:21:20][INFO] train_net.py:  696: Epoch 176 takes 4.03s. Epochs from 10 to 176 take 4.14s in average and 4.08s in median.
[11/07 13:21:20][INFO] train_net.py:  702: For epoch 176, each iteraction takes 0.67s in average. From epoch 10 to 176, each iteraction takes 0.69s in average.
[11/07 13:21:24][INFO] logging.py:   99: json_stats: {"RAM": "41.09/501.50G", "_type": "train_epoch", "dt": 0.51787, "dt_data": 0.51787, "dt_net": 0.24319, "epoch": "178/300", "eta": "0:06:19", "gpu_mem": "5.93G", "grad_norm": 1.34958, "loss": 0.85060, "lr": 0.03563, "top1_err": 25.00000, "top5_err": 4.16667}
[11/07 13:21:24][INFO] train_net.py:  696: Epoch 177 takes 4.19s. Epochs from 10 to 177 take 4.14s in average and 4.08s in median.
[11/07 13:21:24][INFO] train_net.py:  702: For epoch 177, each iteraction takes 0.70s in average. From epoch 10 to 177, each iteraction takes 0.69s in average.
[11/07 13:21:28][INFO] logging.py:   99: json_stats: {"RAM": "41.74/501.50G", "_type": "train_epoch", "dt": 0.51706, "dt_data": 0.51705, "dt_net": 0.24342, "epoch": "179/300", "eta": "0:06:15", "gpu_mem": "5.93G", "grad_norm": 0.94106, "loss": 1.22165, "lr": 0.03513, "top1_err": 29.16667, "top5_err": 4.16667}
[11/07 13:21:28][INFO] train_net.py:  696: Epoch 178 takes 4.11s. Epochs from 10 to 178 take 4.14s in average and 4.08s in median.
[11/07 13:21:28][INFO] train_net.py:  702: For epoch 178, each iteraction takes 0.69s in average. From epoch 10 to 178, each iteraction takes 0.69s in average.
[11/07 13:21:32][INFO] logging.py:   99: json_stats: {"RAM": "41.78/501.50G", "_type": "train_epoch", "dt": 0.51900, "dt_data": 0.51900, "dt_net": 0.24286, "epoch": "180/300", "eta": "0:06:13", "gpu_mem": "5.93G", "grad_norm": 1.41458, "loss": 1.12969, "lr": 0.03463, "top1_err": 41.66667, "top5_err": 4.16667}
[11/07 13:21:32][INFO] train_net.py:  696: Epoch 179 takes 4.15s. Epochs from 10 to 179 take 4.14s in average and 4.08s in median.
[11/07 13:21:32][INFO] train_net.py:  702: For epoch 179, each iteraction takes 0.69s in average. From epoch 10 to 179, each iteraction takes 0.69s in average.
[11/07 13:21:32][INFO] precise_bn.py:  129: Computing precise BN statistics for 84 BN layers ...
[11/07 13:21:38][INFO] logging.py:   99: json_stats: {"RAM": "41.76/501.50G", "_type": "val_epoch", "epoch": "180/300", "gpu_mem": "5.93G", "min_top1_err": 73.91304, "min_top5_err": 13.04348, "time_diff": 0.38127, "top1_err": 86.95652, "top5_err": 30.43478}
[11/07 13:21:42][INFO] logging.py:   99: json_stats: {"RAM": "41.74/501.50G", "_type": "train_epoch", "dt": 0.51972, "dt_data": 0.51972, "dt_net": 0.24276, "epoch": "181/300", "eta": "0:06:11", "gpu_mem": "5.93G", "grad_norm": 2.14714, "loss": 0.94235, "lr": 0.03413, "top1_err": 25.00000, "top5_err": 4.16667}
[11/07 13:21:42][INFO] train_net.py:  696: Epoch 180 takes 4.05s. Epochs from 10 to 180 take 4.14s in average and 4.08s in median.
[11/07 13:21:42][INFO] train_net.py:  702: For epoch 180, each iteraction takes 0.67s in average. From epoch 10 to 180, each iteraction takes 0.69s in average.
[11/07 13:21:46][INFO] logging.py:   99: json_stats: {"RAM": "41.77/501.50G", "_type": "train_epoch", "dt": 0.50142, "dt_data": 0.50142, "dt_net": 0.24323, "epoch": "182/300", "eta": "0:05:54", "gpu_mem": "5.93G", "grad_norm": 1.08890, "loss": 0.82962, "lr": 0.03364, "top1_err": 45.83333, "top5_err": 0.00000}
[11/07 13:21:46][INFO] train_net.py:  696: Epoch 181 takes 4.07s. Epochs from 10 to 181 take 4.14s in average and 4.08s in median.
[11/07 13:21:46][INFO] train_net.py:  702: For epoch 181, each iteraction takes 0.68s in average. From epoch 10 to 181, each iteraction takes 0.69s in average.
[11/07 13:21:50][INFO] logging.py:   99: json_stats: {"RAM": "41.74/501.50G", "_type": "train_epoch", "dt": 0.50701, "dt_data": 0.50701, "dt_net": 0.24261, "epoch": "183/300", "eta": "0:05:55", "gpu_mem": "5.93G", "grad_norm": 2.76261, "loss": 0.92014, "lr": 0.03315, "top1_err": 41.66667, "top5_err": 8.33333}
[11/07 13:21:50][INFO] train_net.py:  696: Epoch 182 takes 4.15s. Epochs from 10 to 182 take 4.14s in average and 4.08s in median.
[11/07 13:21:50][INFO] train_net.py:  702: For epoch 182, each iteraction takes 0.69s in average. From epoch 10 to 182, each iteraction takes 0.69s in average.
[11/07 13:21:54][INFO] logging.py:   99: json_stats: {"RAM": "41.72/501.50G", "_type": "train_epoch", "dt": 0.51101, "dt_data": 0.51101, "dt_net": 0.24290, "epoch": "184/300", "eta": "0:05:55", "gpu_mem": "5.93G", "grad_norm": 2.00050, "loss": 0.79764, "lr": 0.03265, "top1_err": 33.33333, "top5_err": 0.00000}
[11/07 13:21:54][INFO] train_net.py:  696: Epoch 183 takes 4.14s. Epochs from 10 to 183 take 4.14s in average and 4.08s in median.
[11/07 13:21:54][INFO] train_net.py:  702: For epoch 183, each iteraction takes 0.69s in average. From epoch 10 to 183, each iteraction takes 0.69s in average.
[11/07 13:21:59][INFO] logging.py:   99: json_stats: {"RAM": "39.96/501.50G", "_type": "train_epoch", "dt": 0.51898, "dt_data": 0.51898, "dt_net": 0.24278, "epoch": "185/300", "eta": "0:05:58", "gpu_mem": "5.93G", "grad_norm": 2.39458, "loss": 0.87748, "lr": 0.03216, "top1_err": 45.83333, "top5_err": 0.00000}
[11/07 13:21:59][INFO] train_net.py:  696: Epoch 184 takes 4.15s. Epochs from 10 to 184 take 4.14s in average and 4.08s in median.
[11/07 13:21:59][INFO] train_net.py:  702: For epoch 184, each iteraction takes 0.69s in average. From epoch 10 to 184, each iteraction takes 0.69s in average.
[11/07 13:22:03][INFO] logging.py:   99: json_stats: {"RAM": "41.72/501.50G", "_type": "train_epoch", "dt": 0.53893, "dt_data": 0.53893, "dt_net": 0.24248, "epoch": "186/300", "eta": "0:06:08", "gpu_mem": "5.93G", "grad_norm": 3.61517, "loss": 0.80839, "lr": 0.03167, "top1_err": 37.50000, "top5_err": 0.00000}
[11/07 13:22:03][INFO] train_net.py:  696: Epoch 185 takes 4.30s. Epochs from 10 to 185 take 4.14s in average and 4.08s in median.
[11/07 13:22:03][INFO] train_net.py:  702: For epoch 185, each iteraction takes 0.72s in average. From epoch 10 to 185, each iteraction takes 0.69s in average.
[11/07 13:22:07][INFO] logging.py:   99: json_stats: {"RAM": "41.74/501.50G", "_type": "train_epoch", "dt": 0.49802, "dt_data": 0.49802, "dt_net": 0.24328, "epoch": "187/300", "eta": "0:05:37", "gpu_mem": "5.93G", "grad_norm": 1.42065, "loss": 1.07426, "lr": 0.03119, "top1_err": 37.50000, "top5_err": 0.00000}
[11/07 13:22:07][INFO] train_net.py:  696: Epoch 186 takes 4.08s. Epochs from 10 to 186 take 4.14s in average and 4.08s in median.
[11/07 13:22:07][INFO] train_net.py:  702: For epoch 186, each iteraction takes 0.68s in average. From epoch 10 to 186, each iteraction takes 0.69s in average.
[11/07 13:22:11][INFO] logging.py:   99: json_stats: {"RAM": "41.73/501.50G", "_type": "train_epoch", "dt": 0.50357, "dt_data": 0.50357, "dt_net": 0.24294, "epoch": "188/300", "eta": "0:05:38", "gpu_mem": "5.93G", "grad_norm": 2.06601, "loss": 0.89729, "lr": 0.03070, "top1_err": 29.16667, "top5_err": 0.00000}
[11/07 13:22:11][INFO] train_net.py:  696: Epoch 187 takes 4.07s. Epochs from 10 to 187 take 4.14s in average and 4.08s in median.
[11/07 13:22:11][INFO] train_net.py:  702: For epoch 187, each iteraction takes 0.68s in average. From epoch 10 to 187, each iteraction takes 0.69s in average.
[11/07 13:22:15][INFO] logging.py:   99: json_stats: {"RAM": "41.75/501.50G", "_type": "train_epoch", "dt": 0.51740, "dt_data": 0.51740, "dt_net": 0.24364, "epoch": "189/300", "eta": "0:05:44", "gpu_mem": "5.93G", "grad_norm": 2.49431, "loss": 0.92255, "lr": 0.03022, "top1_err": 33.33333, "top5_err": 0.00000}
[11/07 13:22:15][INFO] train_net.py:  696: Epoch 188 takes 4.14s. Epochs from 10 to 188 take 4.14s in average and 4.08s in median.
[11/07 13:22:15][INFO] train_net.py:  702: For epoch 188, each iteraction takes 0.69s in average. From epoch 10 to 188, each iteraction takes 0.69s in average.
[11/07 13:22:19][INFO] logging.py:   99: json_stats: {"RAM": "41.73/501.50G", "_type": "train_epoch", "dt": 0.50149, "dt_data": 0.50149, "dt_net": 0.24316, "epoch": "190/300", "eta": "0:05:30", "gpu_mem": "5.93G", "grad_norm": 2.04887, "loss": 0.64798, "lr": 0.02974, "top1_err": 20.83333, "top5_err": 0.00000}
[11/07 13:22:19][INFO] train_net.py:  696: Epoch 189 takes 4.04s. Epochs from 10 to 189 take 4.14s in average and 4.08s in median.
[11/07 13:22:19][INFO] train_net.py:  702: For epoch 189, each iteraction takes 0.67s in average. From epoch 10 to 189, each iteraction takes 0.69s in average.
[11/07 13:22:19][INFO] precise_bn.py:  129: Computing precise BN statistics for 84 BN layers ...
[11/07 13:22:25][INFO] logging.py:   99: json_stats: {"RAM": "41.75/501.50G", "_type": "val_epoch", "epoch": "190/300", "gpu_mem": "5.93G", "min_top1_err": 73.91304, "min_top5_err": 13.04348, "time_diff": 0.38302, "top1_err": 86.95652, "top5_err": 26.08696}
[11/07 13:22:29][INFO] logging.py:   99: json_stats: {"RAM": "41.73/501.50G", "_type": "train_epoch", "dt": 0.51145, "dt_data": 0.51145, "dt_net": 0.24286, "epoch": "191/300", "eta": "0:05:34", "gpu_mem": "5.93G", "grad_norm": 2.15287, "loss": 0.68352, "lr": 0.02927, "top1_err": 33.33333, "top5_err": 0.00000}
[11/07 13:22:29][INFO] train_net.py:  696: Epoch 190 takes 4.08s. Epochs from 10 to 190 take 4.14s in average and 4.08s in median.
[11/07 13:22:29][INFO] train_net.py:  702: For epoch 190, each iteraction takes 0.68s in average. From epoch 10 to 190, each iteraction takes 0.69s in average.
[11/07 13:22:33][INFO] logging.py:   99: json_stats: {"RAM": "38.78/501.50G", "_type": "train_epoch", "dt": 0.50885, "dt_data": 0.50885, "dt_net": 0.24265, "epoch": "192/300", "eta": "0:05:29", "gpu_mem": "5.93G", "grad_norm": 0.97549, "loss": 0.69102, "lr": 0.02879, "top1_err": 25.00000, "top5_err": 0.00000}
[11/07 13:22:33][INFO] train_net.py:  696: Epoch 191 takes 4.03s. Epochs from 10 to 191 take 4.14s in average and 4.08s in median.
[11/07 13:22:33][INFO] train_net.py:  702: For epoch 191, each iteraction takes 0.67s in average. From epoch 10 to 191, each iteraction takes 0.69s in average.
[11/07 13:22:37][INFO] logging.py:   99: json_stats: {"RAM": "41.19/501.50G", "_type": "train_epoch", "dt": 0.50661, "dt_data": 0.50661, "dt_net": 0.24317, "epoch": "193/300", "eta": "0:05:25", "gpu_mem": "5.93G", "grad_norm": 1.74756, "loss": 0.75946, "lr": 0.02832, "top1_err": 37.50000, "top5_err": 4.16667}
[11/07 13:22:37][INFO] train_net.py:  696: Epoch 192 takes 4.18s. Epochs from 10 to 192 take 4.14s in average and 4.08s in median.
[11/07 13:22:37][INFO] train_net.py:  702: For epoch 192, each iteraction takes 0.70s in average. From epoch 10 to 192, each iteraction takes 0.69s in average.
[11/07 13:22:41][INFO] logging.py:   99: json_stats: {"RAM": "41.72/501.50G", "_type": "train_epoch", "dt": 0.50874, "dt_data": 0.50874, "dt_net": 0.24282, "epoch": "194/300", "eta": "0:05:23", "gpu_mem": "5.93G", "grad_norm": 3.41970, "loss": 0.80843, "lr": 0.02785, "top1_err": 29.16667, "top5_err": 4.16667}
[11/07 13:22:41][INFO] train_net.py:  696: Epoch 193 takes 4.18s. Epochs from 10 to 193 take 4.14s in average and 4.08s in median.
[11/07 13:22:41][INFO] train_net.py:  702: For epoch 193, each iteraction takes 0.70s in average. From epoch 10 to 193, each iteraction takes 0.69s in average.
[11/07 13:22:46][INFO] logging.py:   99: json_stats: {"RAM": "41.74/501.50G", "_type": "train_epoch", "dt": 0.51267, "dt_data": 0.51267, "dt_net": 0.24287, "epoch": "195/300", "eta": "0:05:22", "gpu_mem": "5.93G", "grad_norm": 1.02571, "loss": 0.72842, "lr": 0.02738, "top1_err": 20.83333, "top5_err": 0.00000}
[11/07 13:22:46][INFO] train_net.py:  696: Epoch 194 takes 4.17s. Epochs from 10 to 194 take 4.14s in average and 4.08s in median.
[11/07 13:22:46][INFO] train_net.py:  702: For epoch 194, each iteraction takes 0.70s in average. From epoch 10 to 194, each iteraction takes 0.69s in average.
[11/07 13:22:50][INFO] logging.py:   99: json_stats: {"RAM": "41.73/501.50G", "_type": "train_epoch", "dt": 0.51662, "dt_data": 0.51662, "dt_net": 0.24329, "epoch": "196/300", "eta": "0:05:22", "gpu_mem": "5.93G", "grad_norm": 0.54054, "loss": 0.47199, "lr": 0.02691, "top1_err": 16.66667, "top5_err": 0.00000}
[11/07 13:22:50][INFO] train_net.py:  696: Epoch 195 takes 4.16s. Epochs from 10 to 195 take 4.14s in average and 4.08s in median.
[11/07 13:22:50][INFO] train_net.py:  702: For epoch 195, each iteraction takes 0.69s in average. From epoch 10 to 195, each iteraction takes 0.69s in average.
[11/07 13:22:54][INFO] logging.py:   99: json_stats: {"RAM": "41.73/501.50G", "_type": "train_epoch", "dt": 0.52736, "dt_data": 0.52736, "dt_net": 0.24260, "epoch": "197/300", "eta": "0:05:25", "gpu_mem": "5.93G", "grad_norm": 1.98369, "loss": 0.86942, "lr": 0.02645, "top1_err": 25.00000, "top5_err": 4.16667}
[11/07 13:22:54][INFO] train_net.py:  696: Epoch 196 takes 4.13s. Epochs from 10 to 196 take 4.14s in average and 4.08s in median.
[11/07 13:22:54][INFO] train_net.py:  702: For epoch 196, each iteraction takes 0.69s in average. From epoch 10 to 196, each iteraction takes 0.69s in average.
[11/07 13:22:58][INFO] logging.py:   99: json_stats: {"RAM": "41.74/501.50G", "_type": "train_epoch", "dt": 0.50936, "dt_data": 0.50936, "dt_net": 0.24282, "epoch": "198/300", "eta": "0:05:11", "gpu_mem": "5.93G", "grad_norm": 6.55407, "loss": 0.62657, "lr": 0.02599, "top1_err": 25.00000, "top5_err": 0.00000}
[11/07 13:22:58][INFO] train_net.py:  696: Epoch 197 takes 4.16s. Epochs from 10 to 197 take 4.14s in average and 4.08s in median.
[11/07 13:22:58][INFO] train_net.py:  702: For epoch 197, each iteraction takes 0.69s in average. From epoch 10 to 197, each iteraction takes 0.69s in average.
[11/07 13:23:02][INFO] logging.py:   99: json_stats: {"RAM": "41.73/501.50G", "_type": "train_epoch", "dt": 0.50950, "dt_data": 0.50950, "dt_net": 0.24307, "epoch": "199/300", "eta": "0:05:08", "gpu_mem": "5.93G", "grad_norm": 1.04569, "loss": 0.46884, "lr": 0.02553, "top1_err": 12.50000, "top5_err": 0.00000}
[11/07 13:23:02][INFO] train_net.py:  696: Epoch 198 takes 4.14s. Epochs from 10 to 198 take 4.14s in average and 4.08s in median.
[11/07 13:23:02][INFO] train_net.py:  702: For epoch 198, each iteraction takes 0.69s in average. From epoch 10 to 198, each iteraction takes 0.69s in average.
[11/07 13:23:06][INFO] logging.py:   99: json_stats: {"RAM": "41.73/501.50G", "_type": "train_epoch", "dt": 0.49190, "dt_data": 0.49190, "dt_net": 0.24307, "epoch": "200/300", "eta": "0:04:55", "gpu_mem": "5.93G", "grad_norm": 2.00637, "loss": 0.74906, "lr": 0.02508, "top1_err": 25.00000, "top5_err": 0.00000}
[11/07 13:23:06][INFO] train_net.py:  696: Epoch 199 takes 4.14s. Epochs from 10 to 199 take 4.14s in average and 4.09s in median.
[11/07 13:23:06][INFO] train_net.py:  702: For epoch 199, each iteraction takes 0.69s in average. From epoch 10 to 199, each iteraction takes 0.69s in average.
[11/07 13:23:06][INFO] precise_bn.py:  129: Computing precise BN statistics for 84 BN layers ...
[11/07 13:23:12][INFO] logging.py:   99: json_stats: {"RAM": "40.86/501.50G", "_type": "val_epoch", "epoch": "200/300", "gpu_mem": "5.93G", "min_top1_err": 73.91304, "min_top5_err": 13.04348, "time_diff": 0.39934, "top1_err": 86.95652, "top5_err": 26.08696}
[11/07 13:23:16][INFO] logging.py:   99: json_stats: {"RAM": "41.75/501.50G", "_type": "train_epoch", "dt": 0.51761, "dt_data": 0.51761, "dt_net": 0.24292, "epoch": "201/300", "eta": "0:05:07", "gpu_mem": "5.93G", "grad_norm": 3.20413, "loss": 0.65355, "lr": 0.02462, "top1_err": 16.66667, "top5_err": 0.00000}
[11/07 13:23:16][INFO] train_net.py:  696: Epoch 200 takes 4.05s. Epochs from 10 to 200 take 4.14s in average and 4.08s in median.
[11/07 13:23:16][INFO] train_net.py:  702: For epoch 200, each iteraction takes 0.67s in average. From epoch 10 to 200, each iteraction takes 0.69s in average.
[11/07 13:23:20][INFO] logging.py:   99: json_stats: {"RAM": "41.75/501.50G", "_type": "train_epoch", "dt": 0.50124, "dt_data": 0.50124, "dt_net": 0.24342, "epoch": "202/300", "eta": "0:04:54", "gpu_mem": "5.93G", "grad_norm": 1.92790, "loss": 0.58830, "lr": 0.02417, "top1_err": 25.00000, "top5_err": 0.00000}
[11/07 13:23:20][INFO] train_net.py:  696: Epoch 201 takes 4.08s. Epochs from 10 to 201 take 4.14s in average and 4.08s in median.
[11/07 13:23:20][INFO] train_net.py:  702: For epoch 201, each iteraction takes 0.68s in average. From epoch 10 to 201, each iteraction takes 0.69s in average.
[11/07 13:23:24][INFO] logging.py:   99: json_stats: {"RAM": "41.74/501.50G", "_type": "train_epoch", "dt": 0.50518, "dt_data": 0.50518, "dt_net": 0.24292, "epoch": "203/300", "eta": "0:04:53", "gpu_mem": "5.93G", "grad_norm": 1.36806, "loss": 0.99310, "lr": 0.02373, "top1_err": 29.16667, "top5_err": 4.16667}
[11/07 13:23:24][INFO] train_net.py:  696: Epoch 202 takes 4.04s. Epochs from 10 to 202 take 4.14s in average and 4.08s in median.
[11/07 13:23:24][INFO] train_net.py:  702: For epoch 202, each iteraction takes 0.67s in average. From epoch 10 to 202, each iteraction takes 0.69s in average.
[11/07 13:23:28][INFO] logging.py:   99: json_stats: {"RAM": "41.74/501.50G", "_type": "train_epoch", "dt": 0.51467, "dt_data": 0.51467, "dt_net": 0.24310, "epoch": "204/300", "eta": "0:04:56", "gpu_mem": "5.93G", "grad_norm": 6.01137, "loss": 0.76306, "lr": 0.02328, "top1_err": 25.00000, "top5_err": 4.16667}
[11/07 13:23:28][INFO] train_net.py:  696: Epoch 203 takes 4.04s. Epochs from 10 to 203 take 4.14s in average and 4.08s in median.
[11/07 13:23:28][INFO] train_net.py:  702: For epoch 203, each iteraction takes 0.67s in average. From epoch 10 to 203, each iteraction takes 0.69s in average.
[11/07 13:23:32][INFO] logging.py:   99: json_stats: {"RAM": "41.74/501.50G", "_type": "train_epoch", "dt": 0.51104, "dt_data": 0.51104, "dt_net": 0.24280, "epoch": "205/300", "eta": "0:04:51", "gpu_mem": "5.93G", "grad_norm": 1.44555, "loss": 0.92739, "lr": 0.02284, "top1_err": 20.83333, "top5_err": 8.33333}
[11/07 13:23:32][INFO] train_net.py:  696: Epoch 204 takes 4.04s. Epochs from 10 to 204 take 4.14s in average and 4.08s in median.
[11/07 13:23:32][INFO] train_net.py:  702: For epoch 204, each iteraction takes 0.67s in average. From epoch 10 to 204, each iteraction takes 0.69s in average.
[11/07 13:23:37][INFO] logging.py:   99: json_stats: {"RAM": "41.75/501.50G", "_type": "train_epoch", "dt": 0.50395, "dt_data": 0.50395, "dt_net": 0.24312, "epoch": "206/300", "eta": "0:04:44", "gpu_mem": "5.93G", "grad_norm": 1.55972, "loss": 0.89647, "lr": 0.02240, "top1_err": 20.83333, "top5_err": 8.33333}
[11/07 13:23:37][INFO] train_net.py:  696: Epoch 205 takes 4.06s. Epochs from 10 to 205 take 4.14s in average and 4.08s in median.
[11/07 13:23:37][INFO] train_net.py:  702: For epoch 205, each iteraction takes 0.68s in average. From epoch 10 to 205, each iteraction takes 0.69s in average.
[11/07 13:23:41][INFO] logging.py:   99: json_stats: {"RAM": "41.73/501.50G", "_type": "train_epoch", "dt": 0.51173, "dt_data": 0.51173, "dt_net": 0.24304, "epoch": "207/300", "eta": "0:04:45", "gpu_mem": "5.93G", "grad_norm": 6.00980, "loss": 0.75311, "lr": 0.02197, "top1_err": 29.16667, "top5_err": 0.00000}
[11/07 13:23:41][INFO] train_net.py:  696: Epoch 206 takes 4.01s. Epochs from 10 to 206 take 4.13s in average and 4.08s in median.
[11/07 13:23:41][INFO] train_net.py:  702: For epoch 206, each iteraction takes 0.67s in average. From epoch 10 to 206, each iteraction takes 0.69s in average.
[11/07 13:23:45][INFO] logging.py:   99: json_stats: {"RAM": "41.71/501.50G", "_type": "train_epoch", "dt": 0.50450, "dt_data": 0.50450, "dt_net": 0.24255, "epoch": "208/300", "eta": "0:04:38", "gpu_mem": "5.93G", "grad_norm": 2.06992, "loss": 0.82999, "lr": 0.02154, "top1_err": 25.00000, "top5_err": 8.33333}
[11/07 13:23:45][INFO] train_net.py:  696: Epoch 207 takes 4.08s. Epochs from 10 to 207 take 4.13s in average and 4.08s in median.
[11/07 13:23:45][INFO] train_net.py:  702: For epoch 207, each iteraction takes 0.68s in average. From epoch 10 to 207, each iteraction takes 0.69s in average.
[11/07 13:23:49][INFO] logging.py:   99: json_stats: {"RAM": "40.87/501.50G", "_type": "train_epoch", "dt": 0.52390, "dt_data": 0.52390, "dt_net": 0.24367, "epoch": "209/300", "eta": "0:04:46", "gpu_mem": "5.93G", "grad_norm": 1.93234, "loss": 0.53237, "lr": 0.02111, "top1_err": 16.66667, "top5_err": 0.00000}
[11/07 13:23:49][INFO] train_net.py:  696: Epoch 208 takes 4.24s. Epochs from 10 to 208 take 4.14s in average and 4.08s in median.
[11/07 13:23:49][INFO] train_net.py:  702: For epoch 208, each iteraction takes 0.71s in average. From epoch 10 to 208, each iteraction takes 0.69s in average.
[11/07 13:23:53][INFO] logging.py:   99: json_stats: {"RAM": "41.74/501.50G", "_type": "train_epoch", "dt": 0.52768, "dt_data": 0.52768, "dt_net": 0.24336, "epoch": "210/300", "eta": "0:04:44", "gpu_mem": "5.93G", "grad_norm": 1.01921, "loss": 0.52522, "lr": 0.02068, "top1_err": 12.50000, "top5_err": 0.00000}
[11/07 13:23:53][INFO] train_net.py:  696: Epoch 209 takes 4.14s. Epochs from 10 to 209 take 4.14s in average and 4.08s in median.
[11/07 13:23:53][INFO] train_net.py:  702: For epoch 209, each iteraction takes 0.69s in average. From epoch 10 to 209, each iteraction takes 0.69s in average.
[11/07 13:23:53][INFO] precise_bn.py:  129: Computing precise BN statistics for 84 BN layers ...
[11/07 13:23:59][INFO] logging.py:   99: json_stats: {"RAM": "41.76/501.50G", "_type": "val_epoch", "epoch": "210/300", "gpu_mem": "5.93G", "min_top1_err": 73.91304, "min_top5_err": 13.04348, "time_diff": 0.36268, "top1_err": 86.95652, "top5_err": 21.73913}
[11/07 13:24:03][INFO] logging.py:   99: json_stats: {"RAM": "41.76/501.50G", "_type": "train_epoch", "dt": 0.50691, "dt_data": 0.50691, "dt_net": 0.24284, "epoch": "211/300", "eta": "0:04:30", "gpu_mem": "5.93G", "grad_norm": 6.32489, "loss": 0.61118, "lr": 0.02026, "top1_err": 20.83333, "top5_err": 0.00000}
[11/07 13:24:03][INFO] train_net.py:  696: Epoch 210 takes 4.13s. Epochs from 10 to 210 take 4.14s in average and 4.08s in median.
[11/07 13:24:03][INFO] train_net.py:  702: For epoch 210, each iteraction takes 0.69s in average. From epoch 10 to 210, each iteraction takes 0.69s in average.
[11/07 13:24:07][INFO] logging.py:   99: json_stats: {"RAM": "41.79/501.50G", "_type": "train_epoch", "dt": 0.51080, "dt_data": 0.51080, "dt_net": 0.24287, "epoch": "212/300", "eta": "0:04:29", "gpu_mem": "5.93G", "grad_norm": 2.54677, "loss": 0.54238, "lr": 0.01984, "top1_err": 16.66667, "top5_err": 0.00000}
[11/07 13:24:07][INFO] train_net.py:  696: Epoch 211 takes 4.04s. Epochs from 10 to 211 take 4.13s in average and 4.08s in median.
[11/07 13:24:07][INFO] train_net.py:  702: For epoch 211, each iteraction takes 0.67s in average. From epoch 10 to 211, each iteraction takes 0.69s in average.
[11/07 13:24:11][INFO] logging.py:   99: json_stats: {"RAM": "41.76/501.50G", "_type": "train_epoch", "dt": 0.51720, "dt_data": 0.51720, "dt_net": 0.24401, "epoch": "213/300", "eta": "0:04:29", "gpu_mem": "5.93G", "grad_norm": 2.12008, "loss": 0.58330, "lr": 0.01942, "top1_err": 16.66667, "top5_err": 0.00000}
[11/07 13:24:11][INFO] train_net.py:  696: Epoch 212 takes 4.05s. Epochs from 10 to 212 take 4.13s in average and 4.08s in median.
[11/07 13:24:11][INFO] train_net.py:  702: For epoch 212, each iteraction takes 0.68s in average. From epoch 10 to 212, each iteraction takes 0.69s in average.
[11/07 13:24:15][INFO] logging.py:   99: json_stats: {"RAM": "41.76/501.50G", "_type": "train_epoch", "dt": 0.49718, "dt_data": 0.49718, "dt_net": 0.24282, "epoch": "214/300", "eta": "0:04:16", "gpu_mem": "5.93G", "grad_norm": 1.19266, "loss": 0.51364, "lr": 0.01901, "top1_err": 16.66667, "top5_err": 0.00000}
[11/07 13:24:15][INFO] train_net.py:  696: Epoch 213 takes 3.99s. Epochs from 10 to 213 take 4.13s in average and 4.08s in median.
[11/07 13:24:15][INFO] train_net.py:  702: For epoch 213, each iteraction takes 0.67s in average. From epoch 10 to 213, each iteraction takes 0.69s in average.
[11/07 13:24:19][INFO] logging.py:   99: json_stats: {"RAM": "41.74/501.50G", "_type": "train_epoch", "dt": 0.51110, "dt_data": 0.51110, "dt_net": 0.24253, "epoch": "215/300", "eta": "0:04:20", "gpu_mem": "5.93G", "grad_norm": 1.38795, "loss": 0.57507, "lr": 0.01860, "top1_err": 20.83333, "top5_err": 0.00000}
[11/07 13:24:19][INFO] train_net.py:  696: Epoch 214 takes 4.03s. Epochs from 10 to 214 take 4.13s in average and 4.08s in median.
[11/07 13:24:19][INFO] train_net.py:  702: For epoch 214, each iteraction takes 0.67s in average. From epoch 10 to 214, each iteraction takes 0.69s in average.
[11/07 13:24:23][INFO] logging.py:   99: json_stats: {"RAM": "39.61/501.50G", "_type": "train_epoch", "dt": 0.50366, "dt_data": 0.50366, "dt_net": 0.24267, "epoch": "216/300", "eta": "0:04:13", "gpu_mem": "5.93G", "grad_norm": 1.79421, "loss": 0.56187, "lr": 0.01820, "top1_err": 20.83333, "top5_err": 4.16667}
[11/07 13:24:23][INFO] train_net.py:  696: Epoch 215 takes 4.00s. Epochs from 10 to 215 take 4.13s in average and 4.08s in median.
[11/07 13:24:23][INFO] train_net.py:  702: For epoch 215, each iteraction takes 0.67s in average. From epoch 10 to 215, each iteraction takes 0.69s in average.
[11/07 13:24:27][INFO] logging.py:   99: json_stats: {"RAM": "41.64/501.50G", "_type": "train_epoch", "dt": 0.53740, "dt_data": 0.53739, "dt_net": 0.24331, "epoch": "217/300", "eta": "0:04:27", "gpu_mem": "5.93G", "grad_norm": 1.47461, "loss": 0.23390, "lr": 0.01779, "top1_err": 4.16667, "top5_err": 0.00000}
[11/07 13:24:27][INFO] train_net.py:  696: Epoch 216 takes 4.17s. Epochs from 10 to 216 take 4.13s in average and 4.08s in median.
[11/07 13:24:27][INFO] train_net.py:  702: For epoch 216, each iteraction takes 0.70s in average. From epoch 10 to 216, each iteraction takes 0.69s in average.
[11/07 13:24:31][INFO] logging.py:   99: json_stats: {"RAM": "41.74/501.50G", "_type": "train_epoch", "dt": 0.51693, "dt_data": 0.51693, "dt_net": 0.24314, "epoch": "218/300", "eta": "0:04:14", "gpu_mem": "5.93G", "grad_norm": 1.60661, "loss": 0.61360, "lr": 0.01740, "top1_err": 16.66667, "top5_err": 0.00000}
[11/07 13:24:31][INFO] train_net.py:  696: Epoch 217 takes 4.20s. Epochs from 10 to 217 take 4.13s in average and 4.08s in median.
[11/07 13:24:31][INFO] train_net.py:  702: For epoch 217, each iteraction takes 0.70s in average. From epoch 10 to 217, each iteraction takes 0.69s in average.
[11/07 13:24:35][INFO] logging.py:   99: json_stats: {"RAM": "41.75/501.50G", "_type": "train_epoch", "dt": 0.50813, "dt_data": 0.50813, "dt_net": 0.24346, "epoch": "219/300", "eta": "0:04:06", "gpu_mem": "5.93G", "grad_norm": 5.20520, "loss": 0.44027, "lr": 0.01700, "top1_err": 12.50000, "top5_err": 0.00000}
[11/07 13:24:35][INFO] train_net.py:  696: Epoch 218 takes 4.11s. Epochs from 10 to 218 take 4.13s in average and 4.08s in median.
[11/07 13:24:35][INFO] train_net.py:  702: For epoch 218, each iteraction takes 0.68s in average. From epoch 10 to 218, each iteraction takes 0.69s in average.
[11/07 13:24:40][INFO] logging.py:   99: json_stats: {"RAM": "41.77/501.50G", "_type": "train_epoch", "dt": 0.52357, "dt_data": 0.52357, "dt_net": 0.24241, "epoch": "220/300", "eta": "0:04:11", "gpu_mem": "5.93G", "grad_norm": 2.71992, "loss": 0.48801, "lr": 0.01661, "top1_err": 16.66667, "top5_err": 0.00000}
[11/07 13:24:40][INFO] train_net.py:  696: Epoch 219 takes 4.14s. Epochs from 10 to 219 take 4.13s in average and 4.08s in median.
[11/07 13:24:40][INFO] train_net.py:  702: For epoch 219, each iteraction takes 0.69s in average. From epoch 10 to 219, each iteraction takes 0.69s in average.
[11/07 13:24:40][INFO] precise_bn.py:  129: Computing precise BN statistics for 84 BN layers ...
[11/07 13:24:45][INFO] logging.py:   99: json_stats: {"RAM": "41.77/501.50G", "_type": "val_epoch", "epoch": "220/300", "gpu_mem": "5.93G", "min_top1_err": 73.91304, "min_top5_err": 13.04348, "time_diff": 0.38564, "top1_err": 86.95652, "top5_err": 30.43478}
[11/07 13:24:49][INFO] logging.py:   99: json_stats: {"RAM": "41.77/501.50G", "_type": "train_epoch", "dt": 0.50871, "dt_data": 0.50871, "dt_net": 0.24294, "epoch": "221/300", "eta": "0:04:01", "gpu_mem": "5.93G", "grad_norm": 2.52485, "loss": 0.45307, "lr": 0.01622, "top1_err": 12.50000, "top5_err": 0.00000}
[11/07 13:24:49][INFO] train_net.py:  696: Epoch 220 takes 4.12s. Epochs from 10 to 220 take 4.13s in average and 4.08s in median.
[11/07 13:24:49][INFO] train_net.py:  702: For epoch 220, each iteraction takes 0.69s in average. From epoch 10 to 220, each iteraction takes 0.69s in average.
[11/07 13:24:54][INFO] logging.py:   99: json_stats: {"RAM": "41.75/501.50G", "_type": "train_epoch", "dt": 0.52491, "dt_data": 0.52491, "dt_net": 0.24283, "epoch": "222/300", "eta": "0:04:05", "gpu_mem": "5.93G", "grad_norm": 1.05119, "loss": 0.38581, "lr": 0.01584, "top1_err": 12.50000, "top5_err": 0.00000}
[11/07 13:24:54][INFO] train_net.py:  696: Epoch 221 takes 4.12s. Epochs from 10 to 221 take 4.13s in average and 4.08s in median.
[11/07 13:24:54][INFO] train_net.py:  702: For epoch 221, each iteraction takes 0.69s in average. From epoch 10 to 221, each iteraction takes 0.69s in average.
[11/07 13:24:58][INFO] logging.py:   99: json_stats: {"RAM": "41.73/501.50G", "_type": "train_epoch", "dt": 0.50137, "dt_data": 0.50137, "dt_net": 0.24290, "epoch": "223/300", "eta": "0:03:51", "gpu_mem": "5.93G", "grad_norm": 1.87224, "loss": 0.30527, "lr": 0.01546, "top1_err": 4.16667, "top5_err": 0.00000}
[11/07 13:24:58][INFO] train_net.py:  696: Epoch 222 takes 4.06s. Epochs from 10 to 222 take 4.13s in average and 4.08s in median.
[11/07 13:24:58][INFO] train_net.py:  702: For epoch 222, each iteraction takes 0.68s in average. From epoch 10 to 222, each iteraction takes 0.69s in average.
[11/07 13:25:02][INFO] logging.py:   99: json_stats: {"RAM": "40.96/501.50G", "_type": "train_epoch", "dt": 0.52470, "dt_data": 0.52470, "dt_net": 0.24245, "epoch": "224/300", "eta": "0:03:59", "gpu_mem": "5.93G", "grad_norm": 0.13757, "loss": 0.41885, "lr": 0.01508, "top1_err": 4.16667, "top5_err": 0.00000}
[11/07 13:25:02][INFO] train_net.py:  696: Epoch 223 takes 4.25s. Epochs from 10 to 223 take 4.13s in average and 4.08s in median.
[11/07 13:25:02][INFO] train_net.py:  702: For epoch 223, each iteraction takes 0.71s in average. From epoch 10 to 223, each iteraction takes 0.69s in average.
[11/07 13:25:06][INFO] logging.py:   99: json_stats: {"RAM": "41.83/501.50G", "_type": "train_epoch", "dt": 0.50397, "dt_data": 0.50397, "dt_net": 0.24284, "epoch": "225/300", "eta": "0:03:46", "gpu_mem": "5.93G", "grad_norm": 0.14735, "loss": 0.35265, "lr": 0.01471, "top1_err": 8.33333, "top5_err": 0.00000}
[11/07 13:25:06][INFO] train_net.py:  696: Epoch 224 takes 4.06s. Epochs from 10 to 224 take 4.13s in average and 4.08s in median.
[11/07 13:25:06][INFO] train_net.py:  702: For epoch 224, each iteraction takes 0.68s in average. From epoch 10 to 224, each iteraction takes 0.69s in average.
[11/07 13:25:10][INFO] logging.py:   99: json_stats: {"RAM": "41.77/501.50G", "_type": "train_epoch", "dt": 0.51424, "dt_data": 0.51424, "dt_net": 0.24322, "epoch": "226/300", "eta": "0:03:48", "gpu_mem": "5.93G", "grad_norm": 1.95681, "loss": 0.25857, "lr": 0.01434, "top1_err": 8.33333, "top5_err": 0.00000}
[11/07 13:25:10][INFO] train_net.py:  696: Epoch 225 takes 4.08s. Epochs from 10 to 225 take 4.13s in average and 4.08s in median.
[11/07 13:25:10][INFO] train_net.py:  702: For epoch 225, each iteraction takes 0.68s in average. From epoch 10 to 225, each iteraction takes 0.69s in average.
[11/07 13:25:14][INFO] logging.py:   99: json_stats: {"RAM": "41.76/501.50G", "_type": "train_epoch", "dt": 0.50259, "dt_data": 0.50259, "dt_net": 0.24260, "epoch": "227/300", "eta": "0:03:40", "gpu_mem": "5.93G", "grad_norm": 2.56941, "loss": 0.22682, "lr": 0.01397, "top1_err": 8.33333, "top5_err": 0.00000}
[11/07 13:25:14][INFO] train_net.py:  696: Epoch 226 takes 4.03s. Epochs from 10 to 226 take 4.13s in average and 4.08s in median.
[11/07 13:25:14][INFO] train_net.py:  702: For epoch 226, each iteraction takes 0.67s in average. From epoch 10 to 226, each iteraction takes 0.69s in average.
[11/07 13:25:18][INFO] logging.py:   99: json_stats: {"RAM": "41.77/501.50G", "_type": "train_epoch", "dt": 0.53012, "dt_data": 0.53012, "dt_net": 0.24343, "epoch": "228/300", "eta": "0:03:48", "gpu_mem": "5.93G", "grad_norm": 0.96494, "loss": 0.21800, "lr": 0.01361, "top1_err": 8.33333, "top5_err": 0.00000}
[11/07 13:25:18][INFO] train_net.py:  696: Epoch 227 takes 4.12s. Epochs from 10 to 227 take 4.13s in average and 4.08s in median.
[11/07 13:25:18][INFO] train_net.py:  702: For epoch 227, each iteraction takes 0.69s in average. From epoch 10 to 227, each iteraction takes 0.69s in average.
[11/07 13:25:22][INFO] logging.py:   99: json_stats: {"RAM": "41.76/501.50G", "_type": "train_epoch", "dt": 0.51765, "dt_data": 0.51765, "dt_net": 0.24364, "epoch": "229/300", "eta": "0:03:40", "gpu_mem": "5.93G", "grad_norm": 1.79653, "loss": 0.47976, "lr": 0.01325, "top1_err": 16.66667, "top5_err": 0.00000}
[11/07 13:25:22][INFO] train_net.py:  696: Epoch 228 takes 4.15s. Epochs from 10 to 228 take 4.13s in average and 4.08s in median.
[11/07 13:25:22][INFO] train_net.py:  702: For epoch 228, each iteraction takes 0.69s in average. From epoch 10 to 228, each iteraction takes 0.69s in average.
[11/07 13:25:26][INFO] logging.py:   99: json_stats: {"RAM": "41.76/501.50G", "_type": "train_epoch", "dt": 0.50534, "dt_data": 0.50534, "dt_net": 0.24317, "epoch": "230/300", "eta": "0:03:32", "gpu_mem": "5.93G", "grad_norm": 3.94675, "loss": 0.68893, "lr": 0.01290, "top1_err": 29.16667, "top5_err": 0.00000}
[11/07 13:25:26][INFO] train_net.py:  696: Epoch 229 takes 4.12s. Epochs from 10 to 229 take 4.13s in average and 4.08s in median.
[11/07 13:25:26][INFO] train_net.py:  702: For epoch 229, each iteraction takes 0.69s in average. From epoch 10 to 229, each iteraction takes 0.69s in average.
[11/07 13:25:26][INFO] precise_bn.py:  129: Computing precise BN statistics for 84 BN layers ...
[11/07 13:25:32][INFO] logging.py:   99: json_stats: {"RAM": "41.75/501.50G", "_type": "val_epoch", "epoch": "230/300", "gpu_mem": "5.93G", "min_top1_err": 73.91304, "min_top5_err": 13.04348, "time_diff": 0.38356, "top1_err": 86.95652, "top5_err": 26.08696}
[11/07 13:25:36][INFO] logging.py:   99: json_stats: {"RAM": "39.86/501.50G", "_type": "train_epoch", "dt": 0.50519, "dt_data": 0.50519, "dt_net": 0.24278, "epoch": "231/300", "eta": "0:03:29", "gpu_mem": "5.93G", "grad_norm": 1.97598, "loss": 1.03185, "lr": 0.01255, "top1_err": 33.33333, "top5_err": 4.16667}
[11/07 13:25:36][INFO] train_net.py:  696: Epoch 230 takes 4.03s. Epochs from 10 to 230 take 4.13s in average and 4.08s in median.
[11/07 13:25:36][INFO] train_net.py:  702: For epoch 230, each iteraction takes 0.67s in average. From epoch 10 to 230, each iteraction takes 0.69s in average.
[11/07 13:25:40][INFO] logging.py:   99: json_stats: {"RAM": "41.76/501.50G", "_type": "train_epoch", "dt": 0.52493, "dt_data": 0.52492, "dt_net": 0.24359, "epoch": "232/300", "eta": "0:03:34", "gpu_mem": "5.93G", "grad_norm": 5.58257, "loss": 0.70151, "lr": 0.01221, "top1_err": 29.16667, "top5_err": 4.16667}
[11/07 13:25:40][INFO] train_net.py:  696: Epoch 231 takes 4.17s. Epochs from 10 to 231 take 4.13s in average and 4.08s in median.
[11/07 13:25:40][INFO] train_net.py:  702: For epoch 231, each iteraction takes 0.70s in average. From epoch 10 to 231, each iteraction takes 0.69s in average.
[11/07 13:25:44][INFO] logging.py:   99: json_stats: {"RAM": "41.75/501.50G", "_type": "train_epoch", "dt": 0.51578, "dt_data": 0.51578, "dt_net": 0.24280, "epoch": "233/300", "eta": "0:03:27", "gpu_mem": "5.93G", "grad_norm": 2.89527, "loss": 0.64127, "lr": 0.01187, "top1_err": 25.00000, "top5_err": 0.00000}
[11/07 13:25:44][INFO] train_net.py:  696: Epoch 232 takes 4.09s. Epochs from 10 to 232 take 4.13s in average and 4.08s in median.
[11/07 13:25:44][INFO] train_net.py:  702: For epoch 232, each iteraction takes 0.68s in average. From epoch 10 to 232, each iteraction takes 0.69s in average.
[11/07 13:25:49][INFO] logging.py:   99: json_stats: {"RAM": "41.76/501.50G", "_type": "train_epoch", "dt": 0.49546, "dt_data": 0.49546, "dt_net": 0.24300, "epoch": "234/300", "eta": "0:03:16", "gpu_mem": "5.93G", "grad_norm": 2.63386, "loss": 0.36666, "lr": 0.01153, "top1_err": 12.50000, "top5_err": 0.00000}
[11/07 13:25:49][INFO] train_net.py:  696: Epoch 233 takes 4.12s. Epochs from 10 to 233 take 4.13s in average and 4.08s in median.
[11/07 13:25:49][INFO] train_net.py:  702: For epoch 233, each iteraction takes 0.69s in average. From epoch 10 to 233, each iteraction takes 0.69s in average.
[11/07 13:25:53][INFO] logging.py:   99: json_stats: {"RAM": "41.76/501.50G", "_type": "train_epoch", "dt": 0.51495, "dt_data": 0.51495, "dt_net": 0.24299, "epoch": "235/300", "eta": "0:03:20", "gpu_mem": "5.93G", "grad_norm": 4.18347, "loss": 0.48775, "lr": 0.01120, "top1_err": 25.00000, "top5_err": 0.00000}
[11/07 13:25:53][INFO] train_net.py:  696: Epoch 234 takes 4.08s. Epochs from 10 to 234 take 4.13s in average and 4.08s in median.
[11/07 13:25:53][INFO] train_net.py:  702: For epoch 234, each iteraction takes 0.68s in average. From epoch 10 to 234, each iteraction takes 0.69s in average.
[11/07 13:25:57][INFO] logging.py:   99: json_stats: {"RAM": "41.76/501.50G", "_type": "train_epoch", "dt": 0.53202, "dt_data": 0.53202, "dt_net": 0.24325, "epoch": "236/300", "eta": "0:03:24", "gpu_mem": "5.93G", "grad_norm": 3.08741, "loss": 0.56662, "lr": 0.01087, "top1_err": 12.50000, "top5_err": 4.16667}
[11/07 13:25:57][INFO] train_net.py:  696: Epoch 235 takes 4.15s. Epochs from 10 to 235 take 4.13s in average and 4.08s in median.
[11/07 13:25:57][INFO] train_net.py:  702: For epoch 235, each iteraction takes 0.69s in average. From epoch 10 to 235, each iteraction takes 0.69s in average.
[11/07 13:26:01][INFO] logging.py:   99: json_stats: {"RAM": "41.77/501.50G", "_type": "train_epoch", "dt": 0.51666, "dt_data": 0.51666, "dt_net": 0.24377, "epoch": "237/300", "eta": "0:03:15", "gpu_mem": "5.93G", "grad_norm": 1.76005, "loss": 0.43679, "lr": 0.01055, "top1_err": 20.83333, "top5_err": 0.00000}
[11/07 13:26:01][INFO] train_net.py:  696: Epoch 236 takes 4.17s. Epochs from 10 to 236 take 4.13s in average and 4.08s in median.
[11/07 13:26:01][INFO] train_net.py:  702: For epoch 236, each iteraction takes 0.70s in average. From epoch 10 to 236, each iteraction takes 0.69s in average.
[11/07 13:26:05][INFO] logging.py:   99: json_stats: {"RAM": "41.80/501.50G", "_type": "train_epoch", "dt": 0.51498, "dt_data": 0.51498, "dt_net": 0.24238, "epoch": "238/300", "eta": "0:03:11", "gpu_mem": "5.93G", "grad_norm": 2.87309, "loss": 0.56216, "lr": 0.01023, "top1_err": 16.66667, "top5_err": 0.00000}
[11/07 13:26:05][INFO] train_net.py:  696: Epoch 237 takes 4.05s. Epochs from 10 to 237 take 4.13s in average and 4.08s in median.
[11/07 13:26:05][INFO] train_net.py:  702: For epoch 237, each iteraction takes 0.67s in average. From epoch 10 to 237, each iteraction takes 0.69s in average.
[11/07 13:26:09][INFO] logging.py:   99: json_stats: {"RAM": "41.75/501.50G", "_type": "train_epoch", "dt": 0.52286, "dt_data": 0.52286, "dt_net": 0.24340, "epoch": "239/300", "eta": "0:03:11", "gpu_mem": "5.93G", "grad_norm": 4.38387, "loss": 0.63438, "lr": 0.00991, "top1_err": 25.00000, "top5_err": 0.00000}
[11/07 13:26:09][INFO] train_net.py:  696: Epoch 238 takes 4.13s. Epochs from 10 to 238 take 4.13s in average and 4.08s in median.
[11/07 13:26:09][INFO] train_net.py:  702: For epoch 238, each iteraction takes 0.69s in average. From epoch 10 to 238, each iteraction takes 0.69s in average.
[11/07 13:26:13][INFO] logging.py:   99: json_stats: {"RAM": "39.97/501.50G", "_type": "train_epoch", "dt": 0.53548, "dt_data": 0.53547, "dt_net": 0.24266, "epoch": "240/300", "eta": "0:03:12", "gpu_mem": "5.93G", "grad_norm": 2.10155, "loss": 0.30277, "lr": 0.00960, "top1_err": 4.16667, "top5_err": 0.00000}
[11/07 13:26:13][INFO] train_net.py:  696: Epoch 239 takes 4.16s. Epochs from 10 to 239 take 4.13s in average and 4.09s in median.
[11/07 13:26:13][INFO] train_net.py:  702: For epoch 239, each iteraction takes 0.69s in average. From epoch 10 to 239, each iteraction takes 0.69s in average.
[11/07 13:26:13][INFO] precise_bn.py:  129: Computing precise BN statistics for 84 BN layers ...
[11/07 13:26:19][INFO] logging.py:   99: json_stats: {"RAM": "41.72/501.50G", "_type": "val_epoch", "epoch": "240/300", "gpu_mem": "5.93G", "min_top1_err": 73.91304, "min_top5_err": 13.04348, "time_diff": 0.38278, "top1_err": 86.95652, "top5_err": 39.13043}
[11/07 13:26:23][INFO] logging.py:   99: json_stats: {"RAM": "41.75/501.50G", "_type": "train_epoch", "dt": 0.51309, "dt_data": 0.51309, "dt_net": 0.24295, "epoch": "241/300", "eta": "0:03:01", "gpu_mem": "5.93G", "grad_norm": 1.50182, "loss": 0.40718, "lr": 0.00929, "top1_err": 12.50000, "top5_err": 0.00000}
[11/07 13:26:23][INFO] train_net.py:  696: Epoch 240 takes 4.10s. Epochs from 10 to 240 take 4.13s in average and 4.09s in median.
[11/07 13:26:23][INFO] train_net.py:  702: For epoch 240, each iteraction takes 0.68s in average. From epoch 10 to 240, each iteraction takes 0.69s in average.
[11/07 13:26:27][INFO] logging.py:   99: json_stats: {"RAM": "41.71/501.50G", "_type": "train_epoch", "dt": 0.50248, "dt_data": 0.50247, "dt_net": 0.24335, "epoch": "242/300", "eta": "0:02:54", "gpu_mem": "5.93G", "grad_norm": 0.93081, "loss": 0.37163, "lr": 0.00899, "top1_err": 12.50000, "top5_err": 0.00000}
[11/07 13:26:27][INFO] train_net.py:  696: Epoch 241 takes 4.10s. Epochs from 10 to 241 take 4.13s in average and 4.09s in median.
[11/07 13:26:27][INFO] train_net.py:  702: For epoch 241, each iteraction takes 0.68s in average. From epoch 10 to 241, each iteraction takes 0.69s in average.
[11/07 13:26:31][INFO] logging.py:   99: json_stats: {"RAM": "41.71/501.50G", "_type": "train_epoch", "dt": 0.52330, "dt_data": 0.52330, "dt_net": 0.24304, "epoch": "243/300", "eta": "0:02:58", "gpu_mem": "5.93G", "grad_norm": 1.51022, "loss": 0.30505, "lr": 0.00870, "top1_err": 4.16667, "top5_err": 0.00000}
[11/07 13:26:31][INFO] train_net.py:  696: Epoch 242 takes 4.10s. Epochs from 10 to 242 take 4.13s in average and 4.09s in median.
[11/07 13:26:31][INFO] train_net.py:  702: For epoch 242, each iteraction takes 0.68s in average. From epoch 10 to 242, each iteraction takes 0.69s in average.
[11/07 13:26:35][INFO] logging.py:   99: json_stats: {"RAM": "41.73/501.50G", "_type": "train_epoch", "dt": 0.50349, "dt_data": 0.50349, "dt_net": 0.24278, "epoch": "244/300", "eta": "0:02:49", "gpu_mem": "5.93G", "grad_norm": 3.16681, "loss": 0.27637, "lr": 0.00840, "top1_err": 8.33333, "top5_err": 0.00000}
[11/07 13:26:35][INFO] train_net.py:  696: Epoch 243 takes 4.05s. Epochs from 10 to 243 take 4.13s in average and 4.09s in median.
[11/07 13:26:35][INFO] train_net.py:  702: For epoch 243, each iteraction takes 0.67s in average. From epoch 10 to 243, each iteraction takes 0.69s in average.
[11/07 13:26:40][INFO] logging.py:   99: json_stats: {"RAM": "41.71/501.50G", "_type": "train_epoch", "dt": 0.53127, "dt_data": 0.53127, "dt_net": 0.24385, "epoch": "245/300", "eta": "0:02:55", "gpu_mem": "5.93G", "grad_norm": 1.34336, "loss": 0.40238, "lr": 0.00811, "top1_err": 12.50000, "top5_err": 0.00000}
[11/07 13:26:40][INFO] train_net.py:  696: Epoch 244 takes 4.19s. Epochs from 10 to 244 take 4.13s in average and 4.09s in median.
[11/07 13:26:40][INFO] train_net.py:  702: For epoch 244, each iteraction takes 0.70s in average. From epoch 10 to 244, each iteraction takes 0.69s in average.
[11/07 13:26:44][INFO] logging.py:   99: json_stats: {"RAM": "41.71/501.50G", "_type": "train_epoch", "dt": 0.49797, "dt_data": 0.49797, "dt_net": 0.24281, "epoch": "246/300", "eta": "0:02:41", "gpu_mem": "5.93G", "grad_norm": 0.28759, "loss": 0.33432, "lr": 0.00783, "top1_err": 12.50000, "top5_err": 0.00000}
[11/07 13:26:44][INFO] train_net.py:  696: Epoch 245 takes 4.02s. Epochs from 10 to 245 take 4.13s in average and 4.09s in median.
[11/07 13:26:44][INFO] train_net.py:  702: For epoch 245, each iteraction takes 0.67s in average. From epoch 10 to 245, each iteraction takes 0.69s in average.
[11/07 13:26:48][INFO] logging.py:   99: json_stats: {"RAM": "41.25/501.50G", "_type": "train_epoch", "dt": 0.49569, "dt_data": 0.49569, "dt_net": 0.24309, "epoch": "247/300", "eta": "0:02:37", "gpu_mem": "5.93G", "grad_norm": 1.90790, "loss": 0.27615, "lr": 0.00755, "top1_err": 8.33333, "top5_err": 0.00000}
[11/07 13:26:48][INFO] train_net.py:  696: Epoch 246 takes 4.05s. Epochs from 10 to 246 take 4.13s in average and 4.09s in median.
[11/07 13:26:48][INFO] train_net.py:  702: For epoch 246, each iteraction takes 0.67s in average. From epoch 10 to 246, each iteraction takes 0.69s in average.
[11/07 13:26:52][INFO] logging.py:   99: json_stats: {"RAM": "41.22/501.50G", "_type": "train_epoch", "dt": 0.51455, "dt_data": 0.51455, "dt_net": 0.24275, "epoch": "248/300", "eta": "0:02:40", "gpu_mem": "5.93G", "grad_norm": 0.56587, "loss": 0.22521, "lr": 0.00728, "top1_err": 4.16667, "top5_err": 0.00000}
[11/07 13:26:52][INFO] train_net.py:  696: Epoch 247 takes 4.15s. Epochs from 10 to 247 take 4.13s in average and 4.09s in median.
[11/07 13:26:52][INFO] train_net.py:  702: For epoch 247, each iteraction takes 0.69s in average. From epoch 10 to 247, each iteraction takes 0.69s in average.
[11/07 13:26:56][INFO] logging.py:   99: json_stats: {"RAM": "41.70/501.50G", "_type": "train_epoch", "dt": 0.50338, "dt_data": 0.50338, "dt_net": 0.24275, "epoch": "249/300", "eta": "0:02:34", "gpu_mem": "5.93G", "grad_norm": 0.38343, "loss": 0.19999, "lr": 0.00701, "top1_err": 4.16667, "top5_err": 0.00000}
[11/07 13:26:56][INFO] train_net.py:  696: Epoch 248 takes 4.21s. Epochs from 10 to 248 take 4.13s in average and 4.09s in median.
[11/07 13:26:56][INFO] train_net.py:  702: For epoch 248, each iteraction takes 0.70s in average. From epoch 10 to 248, each iteraction takes 0.69s in average.
[11/07 13:27:00][INFO] logging.py:   99: json_stats: {"RAM": "41.72/501.50G", "_type": "train_epoch", "dt": 0.51601, "dt_data": 0.51601, "dt_net": 0.24316, "epoch": "250/300", "eta": "0:02:34", "gpu_mem": "5.93G", "grad_norm": 0.18815, "loss": 0.24045, "lr": 0.00674, "top1_err": 8.33333, "top5_err": 0.00000}
[11/07 13:27:00][INFO] train_net.py:  696: Epoch 249 takes 4.15s. Epochs from 10 to 249 take 4.13s in average and 4.09s in median.
[11/07 13:27:00][INFO] train_net.py:  702: For epoch 249, each iteraction takes 0.69s in average. From epoch 10 to 249, each iteraction takes 0.69s in average.
[11/07 13:27:00][INFO] precise_bn.py:  129: Computing precise BN statistics for 84 BN layers ...
[11/07 13:27:06][INFO] logging.py:   99: json_stats: {"RAM": "41.71/501.50G", "_type": "val_epoch", "epoch": "250/300", "gpu_mem": "5.93G", "min_top1_err": 73.91304, "min_top5_err": 13.04348, "time_diff": 0.38323, "top1_err": 86.95652, "top5_err": 26.08696}
[11/07 13:27:10][INFO] logging.py:   99: json_stats: {"RAM": "41.72/501.50G", "_type": "train_epoch", "dt": 0.51464, "dt_data": 0.51464, "dt_net": 0.24368, "epoch": "251/300", "eta": "0:02:31", "gpu_mem": "5.93G", "grad_norm": 2.97687, "loss": 0.20497, "lr": 0.00648, "top1_err": 4.16667, "top5_err": 0.00000}
[11/07 13:27:10][INFO] train_net.py:  696: Epoch 250 takes 4.06s. Epochs from 10 to 250 take 4.13s in average and 4.09s in median.
[11/07 13:27:10][INFO] train_net.py:  702: For epoch 250, each iteraction takes 0.68s in average. From epoch 10 to 250, each iteraction takes 0.69s in average.
[11/07 13:27:14][INFO] logging.py:   99: json_stats: {"RAM": "41.70/501.50G", "_type": "train_epoch", "dt": 0.51511, "dt_data": 0.51511, "dt_net": 0.24285, "epoch": "252/300", "eta": "0:02:28", "gpu_mem": "5.93G", "grad_norm": 5.63039, "loss": 0.31170, "lr": 0.00623, "top1_err": 16.66667, "top5_err": 0.00000}
[11/07 13:27:14][INFO] train_net.py:  696: Epoch 251 takes 4.05s. Epochs from 10 to 251 take 4.13s in average and 4.09s in median.
[11/07 13:27:14][INFO] train_net.py:  702: For epoch 251, each iteraction takes 0.68s in average. From epoch 10 to 251, each iteraction takes 0.69s in average.
[11/07 13:27:18][INFO] logging.py:   99: json_stats: {"RAM": "41.71/501.50G", "_type": "train_epoch", "dt": 0.49960, "dt_data": 0.49960, "dt_net": 0.24306, "epoch": "253/300", "eta": "0:02:20", "gpu_mem": "5.93G", "grad_norm": 0.89216, "loss": 0.17428, "lr": 0.00598, "top1_err": 4.16667, "top5_err": 0.00000}
[11/07 13:27:18][INFO] train_net.py:  696: Epoch 252 takes 4.07s. Epochs from 10 to 252 take 4.13s in average and 4.09s in median.
[11/07 13:27:18][INFO] train_net.py:  702: For epoch 252, each iteraction takes 0.68s in average. From epoch 10 to 252, each iteraction takes 0.69s in average.
[11/07 13:27:22][INFO] logging.py:   99: json_stats: {"RAM": "41.69/501.50G", "_type": "train_epoch", "dt": 0.50566, "dt_data": 0.50566, "dt_net": 0.24298, "epoch": "254/300", "eta": "0:02:19", "gpu_mem": "5.93G", "grad_norm": 1.11013, "loss": 0.17533, "lr": 0.00573, "top1_err": 4.16667, "top5_err": 0.00000}
[11/07 13:27:22][INFO] train_net.py:  696: Epoch 253 takes 4.07s. Epochs from 10 to 253 take 4.13s in average and 4.09s in median.
[11/07 13:27:22][INFO] train_net.py:  702: For epoch 253, each iteraction takes 0.68s in average. From epoch 10 to 253, each iteraction takes 0.69s in average.
[11/07 13:27:26][INFO] logging.py:   99: json_stats: {"RAM": "39.94/501.50G", "_type": "train_epoch", "dt": 0.52004, "dt_data": 0.52004, "dt_net": 0.24306, "epoch": "255/300", "eta": "0:02:20", "gpu_mem": "5.93G", "grad_norm": 1.37471, "loss": 0.19143, "lr": 0.00549, "top1_err": 0.00000, "top5_err": 0.00000}
[11/07 13:27:26][INFO] train_net.py:  696: Epoch 254 takes 4.14s. Epochs from 10 to 254 take 4.13s in average and 4.09s in median.
[11/07 13:27:26][INFO] train_net.py:  702: For epoch 254, each iteraction takes 0.69s in average. From epoch 10 to 254, each iteraction takes 0.69s in average.
[11/07 13:27:30][INFO] logging.py:   99: json_stats: {"RAM": "41.74/501.50G", "_type": "train_epoch", "dt": 0.53618, "dt_data": 0.53618, "dt_net": 0.24276, "epoch": "256/300", "eta": "0:02:21", "gpu_mem": "5.93G", "grad_norm": 1.32713, "loss": 0.15063, "lr": 0.00525, "top1_err": 0.00000, "top5_err": 0.00000}
[11/07 13:27:30][INFO] train_net.py:  696: Epoch 255 takes 4.21s. Epochs from 10 to 255 take 4.13s in average and 4.09s in median.
[11/07 13:27:30][INFO] train_net.py:  702: For epoch 255, each iteraction takes 0.70s in average. From epoch 10 to 255, each iteraction takes 0.69s in average.
[11/07 13:27:35][INFO] logging.py:   99: json_stats: {"RAM": "41.72/501.50G", "_type": "train_epoch", "dt": 0.51498, "dt_data": 0.51498, "dt_net": 0.24285, "epoch": "257/300", "eta": "0:02:12", "gpu_mem": "5.93G", "grad_norm": 1.42853, "loss": 0.14159, "lr": 0.00502, "top1_err": 0.00000, "top5_err": 0.00000}
[11/07 13:27:35][INFO] train_net.py:  696: Epoch 256 takes 4.15s. Epochs from 10 to 256 take 4.13s in average and 4.09s in median.
[11/07 13:27:35][INFO] train_net.py:  702: For epoch 256, each iteraction takes 0.69s in average. From epoch 10 to 256, each iteraction takes 0.69s in average.
[11/07 13:27:39][INFO] logging.py:   99: json_stats: {"RAM": "41.75/501.50G", "_type": "train_epoch", "dt": 0.50480, "dt_data": 0.50480, "dt_net": 0.24268, "epoch": "258/300", "eta": "0:02:07", "gpu_mem": "5.93G", "grad_norm": 3.56239, "loss": 0.25177, "lr": 0.00480, "top1_err": 12.50000, "top5_err": 0.00000}
[11/07 13:27:39][INFO] train_net.py:  696: Epoch 257 takes 4.15s. Epochs from 10 to 257 take 4.13s in average and 4.09s in median.
[11/07 13:27:39][INFO] train_net.py:  702: For epoch 257, each iteraction takes 0.69s in average. From epoch 10 to 257, each iteraction takes 0.69s in average.
[11/07 13:27:43][INFO] logging.py:   99: json_stats: {"RAM": "41.72/501.50G", "_type": "train_epoch", "dt": 0.52222, "dt_data": 0.52222, "dt_net": 0.24300, "epoch": "259/300", "eta": "0:02:08", "gpu_mem": "5.93G", "grad_norm": 0.19412, "loss": 0.18943, "lr": 0.00457, "top1_err": 4.16667, "top5_err": 0.00000}
[11/07 13:27:43][INFO] train_net.py:  696: Epoch 258 takes 4.18s. Epochs from 10 to 258 take 4.13s in average and 4.09s in median.
[11/07 13:27:43][INFO] train_net.py:  702: For epoch 258, each iteraction takes 0.70s in average. From epoch 10 to 258, each iteraction takes 0.69s in average.
[11/07 13:27:47][INFO] logging.py:   99: json_stats: {"RAM": "41.72/501.50G", "_type": "train_epoch", "dt": 0.50671, "dt_data": 0.50671, "dt_net": 0.24306, "epoch": "260/300", "eta": "0:02:01", "gpu_mem": "5.93G", "grad_norm": 0.05806, "loss": 0.24924, "lr": 0.00436, "top1_err": 4.16667, "top5_err": 0.00000}
[11/07 13:27:47][INFO] train_net.py:  696: Epoch 259 takes 4.13s. Epochs from 10 to 259 take 4.13s in average and 4.09s in median.
[11/07 13:27:47][INFO] train_net.py:  702: For epoch 259, each iteraction takes 0.69s in average. From epoch 10 to 259, each iteraction takes 0.69s in average.
[11/07 13:27:47][INFO] precise_bn.py:  129: Computing precise BN statistics for 84 BN layers ...
[11/07 13:27:53][INFO] logging.py:   99: json_stats: {"RAM": "41.73/501.50G", "_type": "val_epoch", "epoch": "260/300", "gpu_mem": "5.93G", "min_top1_err": 73.91304, "min_top5_err": 13.04348, "time_diff": 0.37382, "top1_err": 86.95652, "top5_err": 21.73913}
[11/07 13:27:57][INFO] logging.py:   99: json_stats: {"RAM": "41.71/501.50G", "_type": "train_epoch", "dt": 0.51079, "dt_data": 0.51079, "dt_net": 0.24304, "epoch": "261/300", "eta": "0:01:59", "gpu_mem": "5.93G", "grad_norm": 0.10372, "loss": 0.07877, "lr": 0.00415, "top1_err": 0.00000, "top5_err": 0.00000}
[11/07 13:27:57][INFO] train_net.py:  696: Epoch 260 takes 4.12s. Epochs from 10 to 260 take 4.13s in average and 4.09s in median.
[11/07 13:27:57][INFO] train_net.py:  702: For epoch 260, each iteraction takes 0.69s in average. From epoch 10 to 260, each iteraction takes 0.69s in average.
[11/07 13:28:01][INFO] logging.py:   99: json_stats: {"RAM": "38.77/501.50G", "_type": "train_epoch", "dt": 0.53683, "dt_data": 0.53683, "dt_net": 0.24291, "epoch": "262/300", "eta": "0:02:02", "gpu_mem": "5.93G", "grad_norm": 0.55342, "loss": 0.24221, "lr": 0.00394, "top1_err": 8.33333, "top5_err": 0.00000}
[11/07 13:28:01][INFO] train_net.py:  696: Epoch 261 takes 4.06s. Epochs from 10 to 261 take 4.13s in average and 4.09s in median.
[11/07 13:28:01][INFO] train_net.py:  702: For epoch 261, each iteraction takes 0.68s in average. From epoch 10 to 261, each iteraction takes 0.69s in average.
[11/07 13:28:05][INFO] logging.py:   99: json_stats: {"RAM": "41.38/501.50G", "_type": "train_epoch", "dt": 0.51867, "dt_data": 0.51867, "dt_net": 0.24224, "epoch": "263/300", "eta": "0:01:55", "gpu_mem": "5.93G", "grad_norm": 2.62816, "loss": 0.19672, "lr": 0.00374, "top1_err": 4.16667, "top5_err": 0.00000}
[11/07 13:28:05][INFO] train_net.py:  696: Epoch 262 takes 4.09s. Epochs from 10 to 262 take 4.13s in average and 4.09s in median.
[11/07 13:28:05][INFO] train_net.py:  702: For epoch 262, each iteraction takes 0.68s in average. From epoch 10 to 262, each iteraction takes 0.69s in average.
[11/07 13:28:09][INFO] logging.py:   99: json_stats: {"RAM": "41.73/501.50G", "_type": "train_epoch", "dt": 0.60474, "dt_data": 0.60474, "dt_net": 0.24287, "epoch": "264/300", "eta": "0:02:10", "gpu_mem": "5.93G", "grad_norm": 0.20977, "loss": 0.09897, "lr": 0.00354, "top1_err": 0.00000, "top5_err": 0.00000}
[11/07 13:28:09][INFO] train_net.py:  696: Epoch 263 takes 4.22s. Epochs from 10 to 263 take 4.13s in average and 4.09s in median.
[11/07 13:28:09][INFO] train_net.py:  702: For epoch 263, each iteraction takes 0.70s in average. From epoch 10 to 263, each iteraction takes 0.69s in average.
[11/07 13:28:13][INFO] logging.py:   99: json_stats: {"RAM": "41.74/501.50G", "_type": "train_epoch", "dt": 0.50188, "dt_data": 0.50188, "dt_net": 0.24331, "epoch": "265/300", "eta": "0:01:45", "gpu_mem": "5.93G", "grad_norm": 1.07226, "loss": 0.11625, "lr": 0.00335, "top1_err": 0.00000, "top5_err": 0.00000}
[11/07 13:28:13][INFO] train_net.py:  696: Epoch 264 takes 4.07s. Epochs from 10 to 264 take 4.13s in average and 4.09s in median.
[11/07 13:28:13][INFO] train_net.py:  702: For epoch 264, each iteraction takes 0.68s in average. From epoch 10 to 264, each iteraction takes 0.69s in average.
[11/07 13:28:18][INFO] logging.py:   99: json_stats: {"RAM": "41.73/501.50G", "_type": "train_epoch", "dt": 0.50731, "dt_data": 0.50731, "dt_net": 0.24300, "epoch": "266/300", "eta": "0:01:43", "gpu_mem": "5.93G", "grad_norm": 0.08341, "loss": 0.16458, "lr": 0.00317, "top1_err": 4.16667, "top5_err": 0.00000}
[11/07 13:28:18][INFO] train_net.py:  696: Epoch 265 takes 4.09s. Epochs from 10 to 265 take 4.13s in average and 4.09s in median.
[11/07 13:28:18][INFO] train_net.py:  702: For epoch 265, each iteraction takes 0.68s in average. From epoch 10 to 265, each iteraction takes 0.69s in average.
[11/07 13:28:22][INFO] logging.py:   99: json_stats: {"RAM": "41.78/501.50G", "_type": "train_epoch", "dt": 0.51455, "dt_data": 0.51455, "dt_net": 0.24308, "epoch": "267/300", "eta": "0:01:41", "gpu_mem": "5.93G", "grad_norm": 1.06413, "loss": 0.12561, "lr": 0.00299, "top1_err": 0.00000, "top5_err": 0.00000}
[11/07 13:28:22][INFO] train_net.py:  696: Epoch 266 takes 4.05s. Epochs from 10 to 266 take 4.13s in average and 4.09s in median.
[11/07 13:28:22][INFO] train_net.py:  702: For epoch 266, each iteraction takes 0.67s in average. From epoch 10 to 266, each iteraction takes 0.69s in average.
[11/07 13:28:26][INFO] logging.py:   99: json_stats: {"RAM": "41.73/501.50G", "_type": "train_epoch", "dt": 0.50849, "dt_data": 0.50849, "dt_net": 0.24285, "epoch": "268/300", "eta": "0:01:37", "gpu_mem": "5.93G", "grad_norm": 0.67695, "loss": 0.07892, "lr": 0.00281, "top1_err": 0.00000, "top5_err": 0.00000}
[11/07 13:28:26][INFO] train_net.py:  696: Epoch 267 takes 4.05s. Epochs from 10 to 267 take 4.13s in average and 4.09s in median.
[11/07 13:28:26][INFO] train_net.py:  702: For epoch 267, each iteraction takes 0.67s in average. From epoch 10 to 267, each iteraction takes 0.69s in average.
[11/07 13:28:30][INFO] logging.py:   99: json_stats: {"RAM": "41.73/501.50G", "_type": "train_epoch", "dt": 0.51070, "dt_data": 0.51070, "dt_net": 0.24387, "epoch": "269/300", "eta": "0:01:34", "gpu_mem": "5.93G", "grad_norm": 1.56608, "loss": 0.09443, "lr": 0.00264, "top1_err": 0.00000, "top5_err": 0.00000}
[11/07 13:28:30][INFO] train_net.py:  696: Epoch 268 takes 4.07s. Epochs from 10 to 268 take 4.13s in average and 4.09s in median.
[11/07 13:28:30][INFO] train_net.py:  702: For epoch 268, each iteraction takes 0.68s in average. From epoch 10 to 268, each iteraction takes 0.69s in average.
[11/07 13:28:34][INFO] logging.py:   99: json_stats: {"RAM": "41.72/501.50G", "_type": "train_epoch", "dt": 0.52669, "dt_data": 0.52669, "dt_net": 0.24241, "epoch": "270/300", "eta": "0:01:34", "gpu_mem": "5.93G", "grad_norm": 0.64384, "loss": 0.14685, "lr": 0.00247, "top1_err": 4.16667, "top5_err": 0.00000}
[11/07 13:28:34][INFO] train_net.py:  696: Epoch 269 takes 4.09s. Epochs from 10 to 269 take 4.13s in average and 4.09s in median.
[11/07 13:28:34][INFO] train_net.py:  702: For epoch 269, each iteraction takes 0.68s in average. From epoch 10 to 269, each iteraction takes 0.69s in average.
[11/07 13:28:34][INFO] precise_bn.py:  129: Computing precise BN statistics for 84 BN layers ...
[11/07 13:28:40][INFO] logging.py:   99: json_stats: {"RAM": "40.02/501.50G", "_type": "val_epoch", "epoch": "270/300", "gpu_mem": "5.93G", "min_top1_err": 73.91304, "min_top5_err": 13.04348, "time_diff": 0.38300, "top1_err": 86.95652, "top5_err": 21.73913}
[11/07 13:28:44][INFO] logging.py:   99: json_stats: {"RAM": "41.70/501.50G", "_type": "train_epoch", "dt": 0.54801, "dt_data": 0.54801, "dt_net": 0.24273, "epoch": "271/300", "eta": "0:01:35", "gpu_mem": "5.93G", "grad_norm": 0.05256, "loss": 0.10308, "lr": 0.00231, "top1_err": 4.16667, "top5_err": 0.00000}
[11/07 13:28:44][INFO] train_net.py:  696: Epoch 270 takes 4.12s. Epochs from 10 to 270 take 4.13s in average and 4.09s in median.
[11/07 13:28:44][INFO] train_net.py:  702: For epoch 270, each iteraction takes 0.69s in average. From epoch 10 to 270, each iteraction takes 0.69s in average.
[11/07 13:28:48][INFO] logging.py:   99: json_stats: {"RAM": "41.73/501.50G", "_type": "train_epoch", "dt": 0.50967, "dt_data": 0.50967, "dt_net": 0.24272, "epoch": "272/300", "eta": "0:01:25", "gpu_mem": "5.93G", "grad_norm": 1.00805, "loss": 0.09258, "lr": 0.00216, "top1_err": 0.00000, "top5_err": 0.00000}
[11/07 13:28:48][INFO] train_net.py:  696: Epoch 271 takes 4.03s. Epochs from 10 to 271 take 4.13s in average and 4.09s in median.
[11/07 13:28:48][INFO] train_net.py:  702: For epoch 271, each iteraction takes 0.67s in average. From epoch 10 to 271, each iteraction takes 0.69s in average.
[11/07 13:28:52][INFO] logging.py:   99: json_stats: {"RAM": "41.73/501.50G", "_type": "train_epoch", "dt": 0.50410, "dt_data": 0.50410, "dt_net": 0.24336, "epoch": "273/300", "eta": "0:01:21", "gpu_mem": "5.93G", "grad_norm": 3.17280, "loss": 0.20626, "lr": 0.00201, "top1_err": 12.50000, "top5_err": 0.00000}
[11/07 13:28:52][INFO] train_net.py:  696: Epoch 272 takes 4.05s. Epochs from 10 to 272 take 4.13s in average and 4.09s in median.
[11/07 13:28:52][INFO] train_net.py:  702: For epoch 272, each iteraction takes 0.68s in average. From epoch 10 to 272, each iteraction takes 0.69s in average.
[11/07 13:28:56][INFO] logging.py:   99: json_stats: {"RAM": "41.72/501.50G", "_type": "train_epoch", "dt": 0.51997, "dt_data": 0.51997, "dt_net": 0.24317, "epoch": "274/300", "eta": "0:01:21", "gpu_mem": "5.93G", "grad_norm": 0.93209, "loss": 0.10754, "lr": 0.00187, "top1_err": 0.00000, "top5_err": 0.00000}
[11/07 13:28:56][INFO] train_net.py:  696: Epoch 273 takes 4.06s. Epochs from 10 to 273 take 4.13s in average and 4.09s in median.
[11/07 13:28:56][INFO] train_net.py:  702: For epoch 273, each iteraction takes 0.68s in average. From epoch 10 to 273, each iteraction takes 0.69s in average.
[11/07 13:29:00][INFO] logging.py:   99: json_stats: {"RAM": "41.73/501.50G", "_type": "train_epoch", "dt": 0.50649, "dt_data": 0.50649, "dt_net": 0.24329, "epoch": "275/300", "eta": "0:01:15", "gpu_mem": "5.93G", "grad_norm": 0.59919, "loss": 0.06503, "lr": 0.00173, "top1_err": 0.00000, "top5_err": 0.00000}
[11/07 13:29:00][INFO] train_net.py:  696: Epoch 274 takes 4.05s. Epochs from 10 to 274 take 4.13s in average and 4.09s in median.
[11/07 13:29:00][INFO] train_net.py:  702: For epoch 274, each iteraction takes 0.68s in average. From epoch 10 to 274, each iteraction takes 0.69s in average.
[11/07 13:29:04][INFO] logging.py:   99: json_stats: {"RAM": "41.72/501.50G", "_type": "train_epoch", "dt": 0.51697, "dt_data": 0.51697, "dt_net": 0.24304, "epoch": "276/300", "eta": "0:01:14", "gpu_mem": "5.93G", "grad_norm": 1.66478, "loss": 0.13207, "lr": 0.00159, "top1_err": 4.16667, "top5_err": 0.00000}
[11/07 13:29:04][INFO] train_net.py:  696: Epoch 275 takes 4.07s. Epochs from 10 to 275 take 4.13s in average and 4.09s in median.
[11/07 13:29:04][INFO] train_net.py:  702: For epoch 275, each iteraction takes 0.68s in average. From epoch 10 to 275, each iteraction takes 0.69s in average.
[11/07 13:29:08][INFO] logging.py:   99: json_stats: {"RAM": "41.72/501.50G", "_type": "train_epoch", "dt": 0.50345, "dt_data": 0.50345, "dt_net": 0.24284, "epoch": "277/300", "eta": "0:01:09", "gpu_mem": "5.93G", "grad_norm": 1.24138, "loss": 0.11486, "lr": 0.00146, "top1_err": 0.00000, "top5_err": 0.00000}
[11/07 13:29:08][INFO] train_net.py:  696: Epoch 276 takes 4.07s. Epochs from 10 to 276 take 4.13s in average and 4.09s in median.
[11/07 13:29:08][INFO] train_net.py:  702: For epoch 276, each iteraction takes 0.68s in average. From epoch 10 to 276, each iteraction takes 0.69s in average.
[11/07 13:29:12][INFO] logging.py:   99: json_stats: {"RAM": "41.70/501.50G", "_type": "train_epoch", "dt": 0.51263, "dt_data": 0.51263, "dt_net": 0.24296, "epoch": "278/300", "eta": "0:01:07", "gpu_mem": "5.93G", "grad_norm": 0.16990, "loss": 0.04906, "lr": 0.00134, "top1_err": 0.00000, "top5_err": 0.00000}
[11/07 13:29:12][INFO] train_net.py:  696: Epoch 277 takes 4.05s. Epochs from 10 to 277 take 4.13s in average and 4.09s in median.
[11/07 13:29:12][INFO] train_net.py:  702: For epoch 277, each iteraction takes 0.68s in average. From epoch 10 to 277, each iteraction takes 0.69s in average.
[11/07 13:29:16][INFO] logging.py:   99: json_stats: {"RAM": "40.23/501.50G", "_type": "train_epoch", "dt": 0.50773, "dt_data": 0.50773, "dt_net": 0.24283, "epoch": "279/300", "eta": "0:01:03", "gpu_mem": "5.93G", "grad_norm": 3.75927, "loss": 0.16533, "lr": 0.00122, "top1_err": 4.16667, "top5_err": 0.00000}
[11/07 13:29:16][INFO] train_net.py:  696: Epoch 278 takes 4.13s. Epochs from 10 to 278 take 4.13s in average and 4.09s in median.
[11/07 13:29:16][INFO] train_net.py:  702: For epoch 278, each iteraction takes 0.69s in average. From epoch 10 to 278, each iteraction takes 0.69s in average.
[11/07 13:29:20][INFO] logging.py:   99: json_stats: {"RAM": "41.72/501.50G", "_type": "train_epoch", "dt": 0.52009, "dt_data": 0.52009, "dt_net": 0.24387, "epoch": "280/300", "eta": "0:01:02", "gpu_mem": "5.93G", "grad_norm": 0.57476, "loss": 0.10893, "lr": 0.00111, "top1_err": 4.16667, "top5_err": 0.00000}
[11/07 13:29:20][INFO] train_net.py:  696: Epoch 279 takes 4.12s. Epochs from 10 to 279 take 4.13s in average and 4.09s in median.
[11/07 13:29:20][INFO] train_net.py:  702: For epoch 279, each iteraction takes 0.69s in average. From epoch 10 to 279, each iteraction takes 0.69s in average.
[11/07 13:29:20][INFO] precise_bn.py:  129: Computing precise BN statistics for 84 BN layers ...
[11/07 13:29:26][INFO] logging.py:   99: json_stats: {"RAM": "41.74/501.50G", "_type": "val_epoch", "epoch": "280/300", "gpu_mem": "5.93G", "min_top1_err": 73.91304, "min_top5_err": 13.04348, "time_diff": 0.37974, "top1_err": 86.95652, "top5_err": 30.43478}
[11/07 13:29:30][INFO] logging.py:   99: json_stats: {"RAM": "41.73/501.50G", "_type": "train_epoch", "dt": 0.51536, "dt_data": 0.51536, "dt_net": 0.24314, "epoch": "281/300", "eta": "0:00:58", "gpu_mem": "5.93G", "grad_norm": 1.46400, "loss": 0.07955, "lr": 0.00100, "top1_err": 0.00000, "top5_err": 0.00000}
[11/07 13:29:30][INFO] train_net.py:  696: Epoch 280 takes 4.05s. Epochs from 10 to 280 take 4.13s in average and 4.09s in median.
[11/07 13:29:30][INFO] train_net.py:  702: For epoch 280, each iteraction takes 0.68s in average. From epoch 10 to 280, each iteraction takes 0.69s in average.
[11/07 13:29:34][INFO] logging.py:   99: json_stats: {"RAM": "41.71/501.50G", "_type": "train_epoch", "dt": 0.51397, "dt_data": 0.51397, "dt_net": 0.24300, "epoch": "282/300", "eta": "0:00:55", "gpu_mem": "5.93G", "grad_norm": 0.74841, "loss": 0.21703, "lr": 0.00090, "top1_err": 8.33333, "top5_err": 0.00000}
[11/07 13:29:34][INFO] train_net.py:  696: Epoch 281 takes 4.04s. Epochs from 10 to 281 take 4.13s in average and 4.09s in median.
[11/07 13:29:34][INFO] train_net.py:  702: For epoch 281, each iteraction takes 0.67s in average. From epoch 10 to 281, each iteraction takes 0.69s in average.
[11/07 13:29:38][INFO] logging.py:   99: json_stats: {"RAM": "41.75/501.50G", "_type": "train_epoch", "dt": 0.50369, "dt_data": 0.50369, "dt_net": 0.24303, "epoch": "283/300", "eta": "0:00:51", "gpu_mem": "5.93G", "grad_norm": 0.90394, "loss": 0.11816, "lr": 0.00081, "top1_err": 4.16667, "top5_err": 0.00000}
[11/07 13:29:38][INFO] train_net.py:  696: Epoch 282 takes 4.03s. Epochs from 10 to 282 take 4.13s in average and 4.08s in median.
[11/07 13:29:38][INFO] train_net.py:  702: For epoch 282, each iteraction takes 0.67s in average. From epoch 10 to 282, each iteraction takes 0.69s in average.
[11/07 13:29:42][INFO] logging.py:   99: json_stats: {"RAM": "41.73/501.50G", "_type": "train_epoch", "dt": 0.52291, "dt_data": 0.52291, "dt_net": 0.24275, "epoch": "284/300", "eta": "0:00:50", "gpu_mem": "5.93G", "grad_norm": 0.29564, "loss": 0.08802, "lr": 0.00071, "top1_err": 0.00000, "top5_err": 0.00000}
[11/07 13:29:42][INFO] train_net.py:  696: Epoch 283 takes 4.15s. Epochs from 10 to 283 take 4.13s in average and 4.09s in median.
[11/07 13:29:42][INFO] train_net.py:  702: For epoch 283, each iteraction takes 0.69s in average. From epoch 10 to 283, each iteraction takes 0.69s in average.
[11/07 13:29:46][INFO] logging.py:   99: json_stats: {"RAM": "41.72/501.50G", "_type": "train_epoch", "dt": 0.50420, "dt_data": 0.50420, "dt_net": 0.24276, "epoch": "285/300", "eta": "0:00:45", "gpu_mem": "5.93G", "grad_norm": 1.06460, "loss": 0.23057, "lr": 0.00063, "top1_err": 4.16667, "top5_err": 0.00000}
[11/07 13:29:46][INFO] train_net.py:  696: Epoch 284 takes 4.07s. Epochs from 10 to 284 take 4.13s in average and 4.08s in median.
[11/07 13:29:46][INFO] train_net.py:  702: For epoch 284, each iteraction takes 0.68s in average. From epoch 10 to 284, each iteraction takes 0.69s in average.
[11/07 13:29:50][INFO] logging.py:   99: json_stats: {"RAM": "39.26/501.50G", "_type": "train_epoch", "dt": 0.52746, "dt_data": 0.52746, "dt_net": 0.24352, "epoch": "286/300", "eta": "0:00:44", "gpu_mem": "5.93G", "grad_norm": 0.74083, "loss": 0.19019, "lr": 0.00055, "top1_err": 4.16667, "top5_err": 0.00000}
[11/07 13:29:50][INFO] train_net.py:  696: Epoch 285 takes 4.08s. Epochs from 10 to 285 take 4.13s in average and 4.08s in median.
[11/07 13:29:50][INFO] train_net.py:  702: For epoch 285, each iteraction takes 0.68s in average. From epoch 10 to 285, each iteraction takes 0.69s in average.
[11/07 13:29:55][INFO] logging.py:   99: json_stats: {"RAM": "41.22/501.50G", "_type": "train_epoch", "dt": 0.50317, "dt_data": 0.50316, "dt_net": 0.24295, "epoch": "287/300", "eta": "0:00:39", "gpu_mem": "5.93G", "grad_norm": 0.52384, "loss": 0.07050, "lr": 0.00047, "top1_err": 0.00000, "top5_err": 0.00000}
[11/07 13:29:55][INFO] train_net.py:  696: Epoch 286 takes 4.13s. Epochs from 10 to 286 take 4.13s in average and 4.08s in median.
[11/07 13:29:55][INFO] train_net.py:  702: For epoch 286, each iteraction takes 0.69s in average. From epoch 10 to 286, each iteraction takes 0.69s in average.
[11/07 13:29:59][INFO] logging.py:   99: json_stats: {"RAM": "41.73/501.50G", "_type": "train_epoch", "dt": 0.50084, "dt_data": 0.50084, "dt_net": 0.24282, "epoch": "288/300", "eta": "0:00:36", "gpu_mem": "5.93G", "grad_norm": 2.75067, "loss": 0.06938, "lr": 0.00041, "top1_err": 0.00000, "top5_err": 0.00000}
[11/07 13:29:59][INFO] train_net.py:  696: Epoch 287 takes 4.15s. Epochs from 10 to 287 take 4.13s in average and 4.09s in median.
[11/07 13:29:59][INFO] train_net.py:  702: For epoch 287, each iteraction takes 0.69s in average. From epoch 10 to 287, each iteraction takes 0.69s in average.
[11/07 13:30:03][INFO] logging.py:   99: json_stats: {"RAM": "41.74/501.50G", "_type": "train_epoch", "dt": 0.51448, "dt_data": 0.51448, "dt_net": 0.24309, "epoch": "289/300", "eta": "0:00:33", "gpu_mem": "5.93G", "grad_norm": 0.24479, "loss": 0.06520, "lr": 0.00034, "top1_err": 0.00000, "top5_err": 0.00000}
[11/07 13:30:03][INFO] train_net.py:  696: Epoch 288 takes 4.26s. Epochs from 10 to 288 take 4.13s in average and 4.09s in median.
[11/07 13:30:03][INFO] train_net.py:  702: For epoch 288, each iteraction takes 0.71s in average. From epoch 10 to 288, each iteraction takes 0.69s in average.
[11/07 13:30:07][INFO] logging.py:   99: json_stats: {"RAM": "41.74/501.50G", "_type": "train_epoch", "dt": 0.50299, "dt_data": 0.50299, "dt_net": 0.24251, "epoch": "290/300", "eta": "0:00:30", "gpu_mem": "5.93G", "grad_norm": 0.30537, "loss": 0.15456, "lr": 0.00028, "top1_err": 4.16667, "top5_err": 0.00000}
[11/07 13:30:07][INFO] train_net.py:  696: Epoch 289 takes 4.08s. Epochs from 10 to 289 take 4.13s in average and 4.09s in median.
[11/07 13:30:07][INFO] train_net.py:  702: For epoch 289, each iteraction takes 0.68s in average. From epoch 10 to 289, each iteraction takes 0.69s in average.
[11/07 13:30:07][INFO] precise_bn.py:  129: Computing precise BN statistics for 84 BN layers ...
[11/07 13:30:13][INFO] logging.py:   99: json_stats: {"RAM": "41.75/501.50G", "_type": "val_epoch", "epoch": "290/300", "gpu_mem": "5.93G", "min_top1_err": 73.91304, "min_top5_err": 13.04348, "time_diff": 0.37407, "top1_err": 86.95652, "top5_err": 17.39130}
[11/07 13:30:17][INFO] logging.py:   99: json_stats: {"RAM": "41.73/501.50G", "_type": "train_epoch", "dt": 0.51048, "dt_data": 0.51048, "dt_net": 0.24290, "epoch": "291/300", "eta": "0:00:27", "gpu_mem": "5.93G", "grad_norm": 6.30236, "loss": 0.15736, "lr": 0.00023, "top1_err": 4.16667, "top5_err": 0.00000}
[11/07 13:30:17][INFO] train_net.py:  696: Epoch 290 takes 4.04s. Epochs from 10 to 290 take 4.13s in average and 4.08s in median.
[11/07 13:30:17][INFO] train_net.py:  702: For epoch 290, each iteraction takes 0.67s in average. From epoch 10 to 290, each iteraction takes 0.69s in average.
[11/07 13:30:21][INFO] logging.py:   99: json_stats: {"RAM": "41.69/501.50G", "_type": "train_epoch", "dt": 0.51926, "dt_data": 0.51926, "dt_net": 0.24309, "epoch": "292/300", "eta": "0:00:24", "gpu_mem": "5.93G", "grad_norm": 0.29624, "loss": 0.09232, "lr": 0.00018, "top1_err": 0.00000, "top5_err": 0.00000}
[11/07 13:30:21][INFO] train_net.py:  696: Epoch 291 takes 4.02s. Epochs from 10 to 291 take 4.13s in average and 4.08s in median.
[11/07 13:30:21][INFO] train_net.py:  702: For epoch 291, each iteraction takes 0.67s in average. From epoch 10 to 291, each iteraction takes 0.69s in average.
[11/07 13:30:25][INFO] logging.py:   99: json_stats: {"RAM": "41.67/501.50G", "_type": "train_epoch", "dt": 0.49705, "dt_data": 0.49705, "dt_net": 0.24315, "epoch": "293/300", "eta": "0:00:20", "gpu_mem": "5.93G", "grad_norm": 3.76361, "loss": 0.21018, "lr": 0.00014, "top1_err": 4.16667, "top5_err": 0.00000}
[11/07 13:30:25][INFO] train_net.py:  696: Epoch 292 takes 4.14s. Epochs from 10 to 292 take 4.13s in average and 4.08s in median.
[11/07 13:30:25][INFO] train_net.py:  702: For epoch 292, each iteraction takes 0.69s in average. From epoch 10 to 292, each iteraction takes 0.69s in average.
[11/07 13:30:29][INFO] logging.py:   99: json_stats: {"RAM": "39.97/501.50G", "_type": "train_epoch", "dt": 0.50413, "dt_data": 0.50413, "dt_net": 0.24290, "epoch": "294/300", "eta": "0:00:18", "gpu_mem": "5.93G", "grad_norm": 1.72482, "loss": 0.11677, "lr": 0.00010, "top1_err": 0.00000, "top5_err": 0.00000}
[11/07 13:30:29][INFO] train_net.py:  696: Epoch 293 takes 4.10s. Epochs from 10 to 293 take 4.13s in average and 4.09s in median.
[11/07 13:30:29][INFO] train_net.py:  702: For epoch 293, each iteraction takes 0.68s in average. From epoch 10 to 293, each iteraction takes 0.69s in average.
[11/07 13:30:33][INFO] logging.py:   99: json_stats: {"RAM": "41.66/501.50G", "_type": "train_epoch", "dt": 0.50807, "dt_data": 0.50807, "dt_net": 0.24293, "epoch": "295/300", "eta": "0:00:15", "gpu_mem": "5.93G", "grad_norm": 0.06794, "loss": 0.08823, "lr": 0.00007, "top1_err": 0.00000, "top5_err": 0.00000}
[11/07 13:30:33][INFO] train_net.py:  696: Epoch 294 takes 4.11s. Epochs from 10 to 294 take 4.13s in average and 4.09s in median.
[11/07 13:30:33][INFO] train_net.py:  702: For epoch 294, each iteraction takes 0.69s in average. From epoch 10 to 294, each iteraction takes 0.69s in average.
[11/07 13:30:37][INFO] logging.py:   99: json_stats: {"RAM": "41.68/501.50G", "_type": "train_epoch", "dt": 0.50216, "dt_data": 0.50216, "dt_net": 0.24248, "epoch": "296/300", "eta": "0:00:12", "gpu_mem": "5.93G", "grad_norm": 0.13790, "loss": 0.09597, "lr": 0.00005, "top1_err": 0.00000, "top5_err": 0.00000}
[11/07 13:30:37][INFO] train_net.py:  696: Epoch 295 takes 4.03s. Epochs from 10 to 295 take 4.12s in average and 4.09s in median.
[11/07 13:30:37][INFO] train_net.py:  702: For epoch 295, each iteraction takes 0.67s in average. From epoch 10 to 295, each iteraction takes 0.69s in average.
[11/07 13:30:41][INFO] logging.py:   99: json_stats: {"RAM": "41.68/501.50G", "_type": "train_epoch", "dt": 0.51251, "dt_data": 0.51251, "dt_net": 0.24380, "epoch": "297/300", "eta": "0:00:09", "gpu_mem": "5.93G", "grad_norm": 0.04734, "loss": 0.15776, "lr": 0.00003, "top1_err": 4.16667, "top5_err": 0.00000}
[11/07 13:30:41][INFO] train_net.py:  696: Epoch 296 takes 4.23s. Epochs from 10 to 296 take 4.13s in average and 4.09s in median.
[11/07 13:30:41][INFO] train_net.py:  702: For epoch 296, each iteraction takes 0.71s in average. From epoch 10 to 296, each iteraction takes 0.69s in average.
[11/07 13:30:45][INFO] logging.py:   99: json_stats: {"RAM": "41.68/501.50G", "_type": "train_epoch", "dt": 0.51164, "dt_data": 0.51164, "dt_net": 0.24301, "epoch": "298/300", "eta": "0:00:06", "gpu_mem": "5.93G", "grad_norm": 1.73625, "loss": 0.07769, "lr": 0.00001, "top1_err": 0.00000, "top5_err": 0.00000}
[11/07 13:30:45][INFO] train_net.py:  696: Epoch 297 takes 4.06s. Epochs from 10 to 297 take 4.13s in average and 4.09s in median.
[11/07 13:30:45][INFO] train_net.py:  702: For epoch 297, each iteraction takes 0.68s in average. From epoch 10 to 297, each iteraction takes 0.69s in average.
[11/07 13:30:50][INFO] logging.py:   99: json_stats: {"RAM": "41.69/501.50G", "_type": "train_epoch", "dt": 0.50942, "dt_data": 0.50942, "dt_net": 0.24276, "epoch": "299/300", "eta": "0:00:03", "gpu_mem": "5.93G", "grad_norm": 1.10658, "loss": 0.06706, "lr": 0.00000, "top1_err": 0.00000, "top5_err": 0.00000}
[11/07 13:30:50][INFO] train_net.py:  696: Epoch 298 takes 4.06s. Epochs from 10 to 298 take 4.12s in average and 4.08s in median.
[11/07 13:30:50][INFO] train_net.py:  702: For epoch 298, each iteraction takes 0.68s in average. From epoch 10 to 298, each iteraction takes 0.69s in average.
[11/07 13:30:54][INFO] logging.py:   99: json_stats: {"RAM": "41.69/501.50G", "_type": "train_epoch", "dt": 0.49943, "dt_data": 0.49943, "dt_net": 0.24306, "epoch": "300/300", "eta": "0:00:00", "gpu_mem": "5.93G", "grad_norm": 1.35303, "loss": 0.05296, "lr": 0.00000, "top1_err": 0.00000, "top5_err": 0.00000}
[11/07 13:30:54][INFO] train_net.py:  696: Epoch 299 takes 4.06s. Epochs from 10 to 299 take 4.12s in average and 4.08s in median.
[11/07 13:30:54][INFO] train_net.py:  702: For epoch 299, each iteraction takes 0.68s in average. From epoch 10 to 299, each iteraction takes 0.69s in average.
[11/07 13:30:54][INFO] precise_bn.py:  129: Computing precise BN statistics for 84 BN layers ...
[11/07 13:30:59][INFO] logging.py:   99: json_stats: {"RAM": "41.69/501.50G", "_type": "val_epoch", "epoch": "300/300", "gpu_mem": "5.93G", "min_top1_err": 73.91304, "min_top5_err": 13.04348, "time_diff": 0.36665, "top1_err": 86.95652, "top5_err": 21.73913}
[11/07 13:30:59][INFO] train_net.py:  780: training done: _p3.00_f2.34 _t0.07_m5.93 _a26.09 Top5 Acc: 86.96 MEM: 5.93 f: 2.3433
[11/07 13:30:59][INFO] test_net.py:  191: Test with config:
[11/07 13:30:59][INFO] test_net.py:  192: AUG:
  AA_TYPE: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  ENABLE: False
  GEN_MASK_LOADER: False
  INTERPOLATION: bicubic
  MASK_FRAMES: False
  MASK_RATIO: 0.0
  MASK_TUBE: False
  MASK_WINDOW_SIZE: [8, 7, 7]
  MAX_MASK_PATCHES_PER_BLOCK: None
  NUM_SAMPLE: 1
  RE_COUNT: 1
  RE_MODE: pixel
  RE_PROB: 0.25
  RE_SPLIT: False
AVA:
  ANNOTATION_DIR: /mnt/vol/gfsai-flash3-east/ai-group/users/haoqifan/ava/frame_list/
  BGR: False
  DETECTION_SCORE_THRESH: 0.9
  EXCLUSION_FILE: ava_val_excluded_timestamps_v2.2.csv
  FRAME_DIR: /mnt/fair-flash3-east/ava_trainval_frames.img/
  FRAME_LIST_DIR: /mnt/vol/gfsai-flash3-east/ai-group/users/haoqifan/ava/frame_list/
  FULL_TEST_ON_VAL: False
  GROUNDTRUTH_FILE: ava_val_v2.2.csv
  IMG_PROC_BACKEND: cv2
  LABEL_MAP_FILE: ava_action_list_v2.2_for_activitynet_2019.pbtxt
  TEST_FORCE_FLIP: False
  TEST_LISTS: ['val.csv']
  TEST_PREDICT_BOX_LISTS: ['ava_val_predicted_boxes.csv']
  TRAIN_GT_BOX_LISTS: ['ava_train_v2.2.csv']
  TRAIN_LISTS: ['train.csv']
  TRAIN_PCA_JITTER_ONLY: True
  TRAIN_PREDICT_BOX_LISTS: []
  TRAIN_USE_COLOR_AUGMENTATION: False
BENCHMARK:
  LOG_PERIOD: 100
  NUM_EPOCHS: 5
  SHUFFLE: True
BN:
  GLOBAL_SYNC: False
  NORM_TYPE: batchnorm
  NUM_BATCHES_PRECISE: 200
  NUM_SPLITS: 1
  NUM_SYNC_DEVICES: 1
  USE_PRECISE_STATS: True
  WEIGHT_DECAY: 0.0
CONTRASTIVE:
  BN_MLP: False
  BN_SYNC_MLP: False
  DELTA_CLIPS_MAX: inf
  DELTA_CLIPS_MIN: -inf
  DIM: 128
  INTERP_MEMORY: False
  KNN_ON: True
  LENGTH: 239975
  LOCAL_SHUFFLE_BN: True
  MEM_TYPE: 1d
  MLP_DIM: 2048
  MOCO_MULTI_VIEW_QUEUE: False
  MOMENTUM: 0.5
  MOMENTUM_ANNEALING: False
  NUM_CLASSES_DOWNSTREAM: 400
  NUM_MLP_LAYERS: 1
  PREDICTOR_DEPTHS: []
  QUEUE_LEN: 65536
  SEQUENTIAL: False
  SIMCLR_DIST_ON: True
  SWAV_QEUE_LEN: 0
  T: 0.07
  TYPE: mem
DATA:
  COLOR_RND_GRAYSCALE: 0.0
  DECODING_BACKEND: torchvision
  DECODING_SHORT_SIZE: 256
  DUMMY_LOAD: False
  ENSEMBLE_METHOD: sum
  IN22K_TRAINVAL: False
  IN22k_VAL_IN1K: 
  INPUT_CHANNEL_NUM: [3]
  INV_UNIFORM_SAMPLE: False
  IN_VAL_CROP_RATIO: 0.875
  LOADER_CHUNK_OVERALL_SIZE: 0
  LOADER_CHUNK_SIZE: 0
  MEAN: [0.45, 0.45, 0.45]
  MULTI_LABEL: False
  NUM_FRAMES: 15
  PATH_LABEL_SEPARATOR: ;
  PATH_PREFIX: 
  PATH_TO_DATA_DIR: /cluster/work/cvl/robocup/data/Debug_Dataset
  PATH_TO_PRELOAD_IMDB: 
  RANDOM_FLIP: True
  REVERSE_INPUT_CHANNEL: False
  SAMPLING_RATE: 6
  SKIP_ROWS: 0
  SSL_BLUR_SIGMA_MAX: [0.0, 2.0]
  SSL_BLUR_SIGMA_MIN: [0.0, 0.1]
  SSL_COLOR_BRI_CON_SAT: [0.4, 0.4, 0.4]
  SSL_COLOR_HUE: 0.1
  SSL_COLOR_JITTER: False
  SSL_MOCOV2_AUG: False
  STD: [0.225, 0.225, 0.225]
  TARGET_FPS: 30
  TEST_CROP_SIZE: 182
  TIME_DIFF_PROB: 0.0
  TRAIN_CROP_NUM_SPATIAL: 1
  TRAIN_CROP_NUM_TEMPORAL: 1
  TRAIN_CROP_SIZE: 160
  TRAIN_JITTER_ASPECT_RELATIVE: []
  TRAIN_JITTER_FPS: 0.0
  TRAIN_JITTER_MOTION_SHIFT: False
  TRAIN_JITTER_SCALES: [182, 228]
  TRAIN_JITTER_SCALES_RELATIVE: []
  TRAIN_PCA_EIGVAL: [0.225, 0.224, 0.229]
  TRAIN_PCA_EIGVEC: [[-0.5675, 0.7192, 0.4009], [-0.5808, -0.0045, -0.814], [-0.5836, -0.6948, 0.4203]]
  USE_OFFSET_SAMPLING: False
DATA_LOADER:
  ENABLE_MULTI_THREAD_DECODE: False
  NUM_WORKERS: 8
  PIN_MEMORY: True
DEMO:
  BUFFER_SIZE: 0
  CLIP_VIS_SIZE: 10
  COMMON_CLASS_NAMES: ['watch (a person)', 'talk to (e.g., self, a person, a group)', 'listen to (a person)', 'touch (an object)', 'carry/hold (an object)', 'walk', 'sit', 'lie/sleep', 'bend/bow (at the waist)']
  COMMON_CLASS_THRES: 0.7
  DETECTRON2_CFG: COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml
  DETECTRON2_THRESH: 0.9
  DETECTRON2_WEIGHTS: detectron2://COCO-Detection/faster_rcnn_R_50_FPN_3x/137849458/model_final_280758.pkl
  DISPLAY_HEIGHT: 0
  DISPLAY_WIDTH: 0
  ENABLE: False
  FPS: 30
  GT_BOXES: 
  INPUT_FORMAT: BGR
  INPUT_VIDEO: 
  LABEL_FILE_PATH: 
  NUM_CLIPS_SKIP: 0
  NUM_VIS_INSTANCES: 2
  OUTPUT_FILE: 
  OUTPUT_FPS: -1
  PREDS_BOXES: 
  SLOWMO: 1
  STARTING_SECOND: 900
  THREAD_ENABLE: False
  UNCOMMON_CLASS_THRES: 0.3
  VIS_MODE: thres
  WEBCAM: -1
DETECTION:
  ALIGNED: True
  ENABLE: False
  ROI_XFORM_RESOLUTION: 7
  SPATIAL_SCALE_FACTOR: 16
DIST_BACKEND: nccl
LOG_MODEL_INFO: True
LOG_PERIOD: 10
MASK:
  DECODER_DEPTH: 0
  DECODER_EMBED_DIM: 512
  DECODER_SEP_POS_EMBED: False
  DEC_KV_KERNEL: []
  DEC_KV_STRIDE: []
  ENABLE: False
  HEAD_TYPE: separate
  MAE_ON: False
  MAE_RND_MASK: False
  NORM_PRED_PIXEL: True
  PER_FRAME_MASKING: False
  PRED_HOG: False
  PRETRAIN_DEPTH: [15]
  SCALE_INIT_BY_DEPTH: False
  TIME_STRIDE_LOSS: True
MIXUP:
  ALPHA: 0.8
  CUTMIX_ALPHA: 1.0
  ENABLE: False
  LABEL_SMOOTH_VALUE: 0.1
  PROB: 1.0
  SWITCH_PROB: 0.5
MODEL:
  ACT_CHECKPOINT: False
  ARCH: x3d
  DETACH_FINAL_FC: False
  DROPCONNECT_RATE: 0.0
  DROPOUT_RATE: 0.5
  FC_INIT_STD: 0.01
  FP16_ALLREDUCE: False
  FROZEN_BN: False
  HEAD_ACT: softmax
  LOSS_FUNC: cross_entropy
  MODEL_NAME: X3D
  MULTI_PATHWAY_ARCH: ['slowfast']
  NUM_CLASSES: 13
  SINGLE_PATHWAY_ARCH: ['2d', 'c2d', 'i3d', 'slow', 'x3d', 'mvit', 'maskmvit']
MULTIGRID:
  BN_BASE_SIZE: 8
  DEFAULT_B: 0
  DEFAULT_S: 0
  DEFAULT_T: 0
  EPOCH_FACTOR: 1.5
  EVAL_FREQ: 3
  LONG_CYCLE: False
  LONG_CYCLE_FACTORS: [(0.25, 0.7071067811865476), (0.5, 0.7071067811865476), (0.5, 1), (1, 1)]
  LONG_CYCLE_SAMPLING_RATE: 0
  SHORT_CYCLE: False
  SHORT_CYCLE_FACTORS: [0.5, 0.7071067811865476]
MVIT:
  CLS_EMBED_ON: True
  DEPTH: 16
  DIM_MUL: []
  DIM_MUL_IN_ATT: False
  DROPOUT_RATE: 0.0
  DROPPATH_RATE: 0.1
  EMBED_DIM: 96
  HEAD_INIT_SCALE: 1.0
  HEAD_MUL: []
  LAYER_SCALE_INIT_VALUE: 0.0
  MLP_RATIO: 4.0
  MODE: conv
  NORM: layernorm
  NORM_STEM: False
  NUM_HEADS: 1
  PATCH_2D: False
  PATCH_KERNEL: [3, 7, 7]
  PATCH_PADDING: [2, 4, 4]
  PATCH_STRIDE: [2, 4, 4]
  POOL_FIRST: False
  POOL_KVQ_KERNEL: None
  POOL_KV_STRIDE: []
  POOL_KV_STRIDE_ADAPTIVE: None
  POOL_Q_STRIDE: []
  QKV_BIAS: True
  REL_POS_SPATIAL: False
  REL_POS_TEMPORAL: False
  REL_POS_ZERO_INIT: False
  RESIDUAL_POOLING: False
  REV:
    BUFFER_LAYERS: []
    ENABLE: False
    PRE_Q_FUSION: avg
    RESPATH_FUSE: concat
    RES_PATH: conv
  SEPARATE_QKV: False
  SEP_POS_EMBED: False
  USE_ABS_POS: True
  USE_FIXED_SINCOS_POS: False
  USE_MEAN_POOLING: False
  ZERO_DECAY_POS_CLS: True
NONLOCAL:
  GROUP: [[1], [1], [1], [1]]
  INSTANTIATION: dot_product
  LOCATION: [[[]], [[]], [[]], [[]]]
  POOL: [[[1, 2, 2], [1, 2, 2]], [[1, 2, 2], [1, 2, 2]], [[1, 2, 2], [1, 2, 2]], [[1, 2, 2], [1, 2, 2]]]
NUM_GPUS: 1
NUM_SHARDS: 1
OUTPUT_DIR: .
RESNET:
  DEPTH: 50
  INPLACE_RELU: True
  NUM_BLOCK_TEMP_KERNEL: [[3], [4], [6], [3]]
  NUM_GROUPS: 1
  SPATIAL_DILATIONS: [[1], [1], [1], [1]]
  SPATIAL_STRIDES: [[1], [2], [2], [2]]
  STRIDE_1X1: False
  TRANS_FUNC: x3d_transform
  WIDTH_PER_GROUP: 64
  ZERO_INIT_FINAL_BN: True
  ZERO_INIT_FINAL_CONV: False
RNG_SEED: 0
SHARD_ID: 0
SLOWFAST:
  ALPHA: 8
  BETA_INV: 8
  FUSION_CONV_CHANNEL_RATIO: 2
  FUSION_KERNEL_SZ: 5
SOLVER:
  BASE_LR: 0.1
  BASE_LR_SCALE_NUM_SHARDS: True
  BETAS: (0.9, 0.999)
  CLIP_GRAD_L2NORM: None
  CLIP_GRAD_VAL: None
  COSINE_AFTER_WARMUP: False
  COSINE_END_LR: 0.0
  DAMPENING: 0.0
  GAMMA: 0.1
  LARS_ON: False
  LAYER_DECAY: 1.0
  LRS: []
  LR_POLICY: cosine
  MAX_EPOCH: 300
  MOMENTUM: 0.9
  NESTEROV: True
  OPTIMIZING_METHOD: sgd
  STEPS: []
  STEP_SIZE: 1
  WARMUP_EPOCHS: 35.0
  WARMUP_FACTOR: 0.1
  WARMUP_START_LR: 0.01
  WEIGHT_DECAY: 5e-05
  ZERO_WD_1D_PARAM: False
TASK: 
TENSORBOARD:
  CATEGORIES_PATH: 
  CLASS_NAMES_PATH: 
  CONFUSION_MATRIX:
    ENABLE: False
    FIGSIZE: [8, 8]
    SUBSET_PATH: 
  ENABLE: False
  HISTOGRAM:
    ENABLE: False
    FIGSIZE: [8, 8]
    SUBSET_PATH: 
    TOPK: 10
  LOG_DIR: 
  MODEL_VIS:
    ACTIVATIONS: False
    COLORMAP: Pastel2
    ENABLE: False
    GRAD_CAM:
      COLORMAP: viridis
      ENABLE: True
      LAYER_LIST: []
      USE_TRUE_LABEL: False
    INPUT_VIDEO: False
    LAYER_LIST: []
    MODEL_WEIGHTS: False
    TOPK_PREDS: 1
  PREDICTIONS_PATH: 
  WRONG_PRED_VIS:
    ENABLE: False
    SUBSET_PATH: 
    TAG: Incorrectly classified videos.
TEST:
  BATCH_SIZE: 4
  CHECKPOINT_FILE_PATH: 
  CHECKPOINT_TYPE: pytorch
  DATASET: kinetics
  ENABLE: True
  NUM_ENSEMBLE_VIEWS: 10
  NUM_SPATIAL_CROPS: 3
  NUM_TEMPORAL_CLIPS: [10]
  SAVE_RESULTS_PATH: 
TRAIN:
  AUTO_RESUME: True
  BATCH_SIZE: 4
  CHECKPOINT_CLEAR_NAME_PATTERN: ()
  CHECKPOINT_EPOCH_RESET: False
  CHECKPOINT_FILE_PATH: 
  CHECKPOINT_INFLATE: False
  CHECKPOINT_IN_INIT: False
  CHECKPOINT_PERIOD: 10
  CHECKPOINT_TYPE: pytorch
  DATASET: kinetics
  ENABLE: True
  EVAL_PERIOD: 10
  KILL_LOSS_EXPLOSION_FACTOR: 0.0
  MIXED_PRECISION: False
VIS_MASK:
  ENABLE: False
X3D:
  BN_LIN5: False
  BOTTLENECK_FACTOR: 2.25
  CHANNELWISE_3x3x3: True
  DEPTH_FACTOR: 2.2
  DIM_C1: 12
  DIM_C5: 2048
  SCALE_RES2: False
  WIDTH_FACTOR: 2.0
[11/07 13:30:59][INFO] misc.py:  185: Model:
X3D(
  (s1): VideoModelStem(
    (pathway0_stem): X3DStem(
      (conv_xy): Conv3d(3, 24, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1), bias=False)
      (conv): Conv3d(24, 24, kernel_size=(5, 1, 1), stride=(1, 1, 1), padding=(2, 0, 0), groups=24, bias=False)
      (bn): BatchNorm3d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (s2): ResStage(
    (pathway0_res0): ResBlock(
      (branch1): Conv3d(24, 24, kernel_size=(1, 1, 1), stride=(1, 2, 2), bias=False)
      (branch1_bn): BatchNorm3d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (branch2): X3DTransform(
        (a): Conv3d(24, 54, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (a_bn): BatchNorm3d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (a_relu): ReLU(inplace=True)
        (b): Conv3d(54, 54, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1), groups=54, bias=False)
        (b_bn): BatchNorm3d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): SE(
          (avg_pool): AdaptiveAvgPool3d(output_size=(1, 1, 1))
          (fc1): Conv3d(54, 8, kernel_size=(1, 1, 1), stride=(1, 1, 1))
          (fc1_act): ReLU()
          (fc2): Conv3d(8, 54, kernel_size=(1, 1, 1), stride=(1, 1, 1))
          (fc2_sig): Sigmoid()
        )
        (b_relu): Swish()
        (c): Conv3d(54, 24, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (c_bn): BatchNorm3d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (relu): ReLU(inplace=True)
    )
    (pathway0_res1): ResBlock(
      (branch2): X3DTransform(
        (a): Conv3d(24, 54, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (a_bn): BatchNorm3d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (a_relu): ReLU(inplace=True)
        (b): Conv3d(54, 54, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=54, bias=False)
        (b_bn): BatchNorm3d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (b_relu): Swish()
        (c): Conv3d(54, 24, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (c_bn): BatchNorm3d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (relu): ReLU(inplace=True)
    )
    (pathway0_res2): ResBlock(
      (branch2): X3DTransform(
        (a): Conv3d(24, 54, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (a_bn): BatchNorm3d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (a_relu): ReLU(inplace=True)
        (b): Conv3d(54, 54, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=54, bias=False)
        (b_bn): BatchNorm3d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): SE(
          (avg_pool): AdaptiveAvgPool3d(output_size=(1, 1, 1))
          (fc1): Conv3d(54, 8, kernel_size=(1, 1, 1), stride=(1, 1, 1))
          (fc1_act): ReLU()
          (fc2): Conv3d(8, 54, kernel_size=(1, 1, 1), stride=(1, 1, 1))
          (fc2_sig): Sigmoid()
        )
        (b_relu): Swish()
        (c): Conv3d(54, 24, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (c_bn): BatchNorm3d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (relu): ReLU(inplace=True)
    )
  )
  (s3): ResStage(
    (pathway0_res0): ResBlock(
      (branch1): Conv3d(24, 48, kernel_size=(1, 1, 1), stride=(1, 2, 2), bias=False)
      (branch1_bn): BatchNorm3d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (branch2): X3DTransform(
        (a): Conv3d(24, 108, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (a_bn): BatchNorm3d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (a_relu): ReLU(inplace=True)
        (b): Conv3d(108, 108, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1), groups=108, bias=False)
        (b_bn): BatchNorm3d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): SE(
          (avg_pool): AdaptiveAvgPool3d(output_size=(1, 1, 1))
          (fc1): Conv3d(108, 8, kernel_size=(1, 1, 1), stride=(1, 1, 1))
          (fc1_act): ReLU()
          (fc2): Conv3d(8, 108, kernel_size=(1, 1, 1), stride=(1, 1, 1))
          (fc2_sig): Sigmoid()
        )
        (b_relu): Swish()
        (c): Conv3d(108, 48, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (c_bn): BatchNorm3d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (relu): ReLU(inplace=True)
    )
    (pathway0_res1): ResBlock(
      (branch2): X3DTransform(
        (a): Conv3d(48, 108, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (a_bn): BatchNorm3d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (a_relu): ReLU(inplace=True)
        (b): Conv3d(108, 108, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=108, bias=False)
        (b_bn): BatchNorm3d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (b_relu): Swish()
        (c): Conv3d(108, 48, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (c_bn): BatchNorm3d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (relu): ReLU(inplace=True)
    )
    (pathway0_res2): ResBlock(
      (branch2): X3DTransform(
        (a): Conv3d(48, 108, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (a_bn): BatchNorm3d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (a_relu): ReLU(inplace=True)
        (b): Conv3d(108, 108, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=108, bias=False)
        (b_bn): BatchNorm3d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): SE(
          (avg_pool): AdaptiveAvgPool3d(output_size=(1, 1, 1))
          (fc1): Conv3d(108, 8, kernel_size=(1, 1, 1), stride=(1, 1, 1))
          (fc1_act): ReLU()
          (fc2): Conv3d(8, 108, kernel_size=(1, 1, 1), stride=(1, 1, 1))
          (fc2_sig): Sigmoid()
        )
        (b_relu): Swish()
        (c): Conv3d(108, 48, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (c_bn): BatchNorm3d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (relu): ReLU(inplace=True)
    )
    (pathway0_res3): ResBlock(
      (branch2): X3DTransform(
        (a): Conv3d(48, 108, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (a_bn): BatchNorm3d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (a_relu): ReLU(inplace=True)
        (b): Conv3d(108, 108, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=108, bias=False)
        (b_bn): BatchNorm3d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (b_relu): Swish()
        (c): Conv3d(108, 48, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (c_bn): BatchNorm3d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (relu): ReLU(inplace=True)
    )
    (pathway0_res4): ResBlock(
      (branch2): X3DTransform(
        (a): Conv3d(48, 108, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (a_bn): BatchNorm3d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (a_relu): ReLU(inplace=True)
        (b): Conv3d(108, 108, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=108, bias=False)
        (b_bn): BatchNorm3d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): SE(
          (avg_pool): AdaptiveAvgPool3d(output_size=(1, 1, 1))
          (fc1): Conv3d(108, 8, kernel_size=(1, 1, 1), stride=(1, 1, 1))
          (fc1_act): ReLU()
          (fc2): Conv3d(8, 108, kernel_size=(1, 1, 1), stride=(1, 1, 1))
          (fc2_sig): Sigmoid()
        )
        (b_relu): Swish()
        (c): Conv3d(108, 48, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (c_bn): BatchNorm3d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (relu): ReLU(inplace=True)
    )
  )
  (s4): ResStage(
    (pathway0_res0): ResBlock(
      (branch1): Conv3d(48, 96, kernel_size=(1, 1, 1), stride=(1, 2, 2), bias=False)
      (branch1_bn): BatchNorm3d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (branch2): X3DTransform(
        (a): Conv3d(48, 216, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (a_bn): BatchNorm3d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (a_relu): ReLU(inplace=True)
        (b): Conv3d(216, 216, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1), groups=216, bias=False)
        (b_bn): BatchNorm3d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): SE(
          (avg_pool): AdaptiveAvgPool3d(output_size=(1, 1, 1))
          (fc1): Conv3d(216, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1))
          (fc1_act): ReLU()
          (fc2): Conv3d(16, 216, kernel_size=(1, 1, 1), stride=(1, 1, 1))
          (fc2_sig): Sigmoid()
        )
        (b_relu): Swish()
        (c): Conv3d(216, 96, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (c_bn): BatchNorm3d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (relu): ReLU(inplace=True)
    )
    (pathway0_res1): ResBlock(
      (branch2): X3DTransform(
        (a): Conv3d(96, 216, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (a_bn): BatchNorm3d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (a_relu): ReLU(inplace=True)
        (b): Conv3d(216, 216, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=216, bias=False)
        (b_bn): BatchNorm3d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (b_relu): Swish()
        (c): Conv3d(216, 96, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (c_bn): BatchNorm3d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (relu): ReLU(inplace=True)
    )
    (pathway0_res2): ResBlock(
      (branch2): X3DTransform(
        (a): Conv3d(96, 216, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (a_bn): BatchNorm3d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (a_relu): ReLU(inplace=True)
        (b): Conv3d(216, 216, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=216, bias=False)
        (b_bn): BatchNorm3d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): SE(
          (avg_pool): AdaptiveAvgPool3d(output_size=(1, 1, 1))
          (fc1): Conv3d(216, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1))
          (fc1_act): ReLU()
          (fc2): Conv3d(16, 216, kernel_size=(1, 1, 1), stride=(1, 1, 1))
          (fc2_sig): Sigmoid()
        )
        (b_relu): Swish()
        (c): Conv3d(216, 96, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (c_bn): BatchNorm3d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (relu): ReLU(inplace=True)
    )
    (pathway0_res3): ResBlock(
      (branch2): X3DTransform(
        (a): Conv3d(96, 216, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (a_bn): BatchNorm3d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (a_relu): ReLU(inplace=True)
        (b): Conv3d(216, 216, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=216, bias=False)
        (b_bn): BatchNorm3d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (b_relu): Swish()
        (c): Conv3d(216, 96, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (c_bn): BatchNorm3d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (relu): ReLU(inplace=True)
    )
    (pathway0_res4): ResBlock(
      (branch2): X3DTransform(
        (a): Conv3d(96, 216, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (a_bn): BatchNorm3d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (a_relu): ReLU(inplace=True)
        (b): Conv3d(216, 216, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=216, bias=False)
        (b_bn): BatchNorm3d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): SE(
          (avg_pool): AdaptiveAvgPool3d(output_size=(1, 1, 1))
          (fc1): Conv3d(216, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1))
          (fc1_act): ReLU()
          (fc2): Conv3d(16, 216, kernel_size=(1, 1, 1), stride=(1, 1, 1))
          (fc2_sig): Sigmoid()
        )
        (b_relu): Swish()
        (c): Conv3d(216, 96, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (c_bn): BatchNorm3d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (relu): ReLU(inplace=True)
    )
    (pathway0_res5): ResBlock(
      (branch2): X3DTransform(
        (a): Conv3d(96, 216, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (a_bn): BatchNorm3d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (a_relu): ReLU(inplace=True)
        (b): Conv3d(216, 216, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=216, bias=False)
        (b_bn): BatchNorm3d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (b_relu): Swish()
        (c): Conv3d(216, 96, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (c_bn): BatchNorm3d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (relu): ReLU(inplace=True)
    )
    (pathway0_res6): ResBlock(
      (branch2): X3DTransform(
        (a): Conv3d(96, 216, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (a_bn): BatchNorm3d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (a_relu): ReLU(inplace=True)
        (b): Conv3d(216, 216, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=216, bias=False)
        (b_bn): BatchNorm3d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): SE(
          (avg_pool): AdaptiveAvgPool3d(output_size=(1, 1, 1))
          (fc1): Conv3d(216, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1))
          (fc1_act): ReLU()
          (fc2): Conv3d(16, 216, kernel_size=(1, 1, 1), stride=(1, 1, 1))
          (fc2_sig): Sigmoid()
        )
        (b_relu): Swish()
        (c): Conv3d(216, 96, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (c_bn): BatchNorm3d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (relu): ReLU(inplace=True)
    )
    (pathway0_res7): ResBlock(
      (branch2): X3DTransform(
        (a): Conv3d(96, 216, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (a_bn): BatchNorm3d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (a_relu): ReLU(inplace=True)
        (b): Conv3d(216, 216, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=216, bias=False)
        (b_bn): BatchNorm3d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (b_relu): Swish()
        (c): Conv3d(216, 96, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (c_bn): BatchNorm3d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (relu): ReLU(inplace=True)
    )
    (pathway0_res8): ResBlock(
      (branch2): X3DTransform(
        (a): Conv3d(96, 216, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (a_bn): BatchNorm3d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (a_relu): ReLU(inplace=True)
        (b): Conv3d(216, 216, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=216, bias=False)
        (b_bn): BatchNorm3d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): SE(
          (avg_pool): AdaptiveAvgPool3d(output_size=(1, 1, 1))
          (fc1): Conv3d(216, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1))
          (fc1_act): ReLU()
          (fc2): Conv3d(16, 216, kernel_size=(1, 1, 1), stride=(1, 1, 1))
          (fc2_sig): Sigmoid()
        )
        (b_relu): Swish()
        (c): Conv3d(216, 96, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (c_bn): BatchNorm3d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (relu): ReLU(inplace=True)
    )
    (pathway0_res9): ResBlock(
      (branch2): X3DTransform(
        (a): Conv3d(96, 216, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (a_bn): BatchNorm3d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (a_relu): ReLU(inplace=True)
        (b): Conv3d(216, 216, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=216, bias=False)
        (b_bn): BatchNorm3d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (b_relu): Swish()
        (c): Conv3d(216, 96, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (c_bn): BatchNorm3d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (relu): ReLU(inplace=True)
    )
    (pathway0_res10): ResBlock(
      (branch2): X3DTransform(
        (a): Conv3d(96, 216, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (a_bn): BatchNorm3d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (a_relu): ReLU(inplace=True)
        (b): Conv3d(216, 216, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=216, bias=False)
        (b_bn): BatchNorm3d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): SE(
          (avg_pool): AdaptiveAvgPool3d(output_size=(1, 1, 1))
          (fc1): Conv3d(216, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1))
          (fc1_act): ReLU()
          (fc2): Conv3d(16, 216, kernel_size=(1, 1, 1), stride=(1, 1, 1))
          (fc2_sig): Sigmoid()
        )
        (b_relu): Swish()
        (c): Conv3d(216, 96, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (c_bn): BatchNorm3d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (relu): ReLU(inplace=True)
    )
  )
  (s5): ResStage(
    (pathway0_res0): ResBlock(
      (branch1): Conv3d(96, 192, kernel_size=(1, 1, 1), stride=(1, 2, 2), bias=False)
      (branch1_bn): BatchNorm3d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (branch2): X3DTransform(
        (a): Conv3d(96, 432, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (a_bn): BatchNorm3d(432, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (a_relu): ReLU(inplace=True)
        (b): Conv3d(432, 432, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1), groups=432, bias=False)
        (b_bn): BatchNorm3d(432, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): SE(
          (avg_pool): AdaptiveAvgPool3d(output_size=(1, 1, 1))
          (fc1): Conv3d(432, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1))
          (fc1_act): ReLU()
          (fc2): Conv3d(32, 432, kernel_size=(1, 1, 1), stride=(1, 1, 1))
          (fc2_sig): Sigmoid()
        )
        (b_relu): Swish()
        (c): Conv3d(432, 192, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (c_bn): BatchNorm3d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (relu): ReLU(inplace=True)
    )
    (pathway0_res1): ResBlock(
      (branch2): X3DTransform(
        (a): Conv3d(192, 432, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (a_bn): BatchNorm3d(432, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (a_relu): ReLU(inplace=True)
        (b): Conv3d(432, 432, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=432, bias=False)
        (b_bn): BatchNorm3d(432, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (b_relu): Swish()
        (c): Conv3d(432, 192, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (c_bn): BatchNorm3d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (relu): ReLU(inplace=True)
    )
    (pathway0_res2): ResBlock(
      (branch2): X3DTransform(
        (a): Conv3d(192, 432, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (a_bn): BatchNorm3d(432, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (a_relu): ReLU(inplace=True)
        (b): Conv3d(432, 432, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=432, bias=False)
        (b_bn): BatchNorm3d(432, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): SE(
          (avg_pool): AdaptiveAvgPool3d(output_size=(1, 1, 1))
          (fc1): Conv3d(432, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1))
          (fc1_act): ReLU()
          (fc2): Conv3d(32, 432, kernel_size=(1, 1, 1), stride=(1, 1, 1))
          (fc2_sig): Sigmoid()
        )
        (b_relu): Swish()
        (c): Conv3d(432, 192, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (c_bn): BatchNorm3d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (relu): ReLU(inplace=True)
    )
    (pathway0_res3): ResBlock(
      (branch2): X3DTransform(
        (a): Conv3d(192, 432, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (a_bn): BatchNorm3d(432, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (a_relu): ReLU(inplace=True)
        (b): Conv3d(432, 432, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=432, bias=False)
        (b_bn): BatchNorm3d(432, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (b_relu): Swish()
        (c): Conv3d(432, 192, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (c_bn): BatchNorm3d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (relu): ReLU(inplace=True)
    )
    (pathway0_res4): ResBlock(
      (branch2): X3DTransform(
        (a): Conv3d(192, 432, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (a_bn): BatchNorm3d(432, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (a_relu): ReLU(inplace=True)
        (b): Conv3d(432, 432, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=432, bias=False)
        (b_bn): BatchNorm3d(432, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): SE(
          (avg_pool): AdaptiveAvgPool3d(output_size=(1, 1, 1))
          (fc1): Conv3d(432, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1))
          (fc1_act): ReLU()
          (fc2): Conv3d(32, 432, kernel_size=(1, 1, 1), stride=(1, 1, 1))
          (fc2_sig): Sigmoid()
        )
        (b_relu): Swish()
        (c): Conv3d(432, 192, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (c_bn): BatchNorm3d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (relu): ReLU(inplace=True)
    )
    (pathway0_res5): ResBlock(
      (branch2): X3DTransform(
        (a): Conv3d(192, 432, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (a_bn): BatchNorm3d(432, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (a_relu): ReLU(inplace=True)
        (b): Conv3d(432, 432, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=432, bias=False)
        (b_bn): BatchNorm3d(432, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (b_relu): Swish()
        (c): Conv3d(432, 192, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (c_bn): BatchNorm3d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (relu): ReLU(inplace=True)
    )
    (pathway0_res6): ResBlock(
      (branch2): X3DTransform(
        (a): Conv3d(192, 432, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (a_bn): BatchNorm3d(432, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (a_relu): ReLU(inplace=True)
        (b): Conv3d(432, 432, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=432, bias=False)
        (b_bn): BatchNorm3d(432, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): SE(
          (avg_pool): AdaptiveAvgPool3d(output_size=(1, 1, 1))
          (fc1): Conv3d(432, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1))
          (fc1_act): ReLU()
          (fc2): Conv3d(32, 432, kernel_size=(1, 1, 1), stride=(1, 1, 1))
          (fc2_sig): Sigmoid()
        )
        (b_relu): Swish()
        (c): Conv3d(432, 192, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (c_bn): BatchNorm3d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (relu): ReLU(inplace=True)
    )
  )
  (head): X3DHead(
    (conv_5): Conv3d(192, 432, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (conv_5_bn): BatchNorm3d(432, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_5_relu): ReLU(inplace=True)
    (avg_pool): AvgPool3d(kernel_size=[15, 5, 5], stride=1, padding=0)
    (lin_5): Conv3d(432, 2048, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (lin_5_relu): ReLU(inplace=True)
    (dropout): Dropout(p=0.5, inplace=False)
    (projection): Linear(in_features=2048, out_features=13, bias=True)
    (act): Softmax(dim=4)
  )
)
[11/07 13:30:59][INFO] misc.py:  187: Params: 3,001,359
[11/07 13:30:59][INFO] misc.py:  188: Mem: 5.93186616897583 MB
[11/07 13:31:00][WARNING] jit_analysis.py:  499: Unsupported operator aten::adaptive_avg_pool3d encountered 15 time(s)
[11/07 13:31:00][WARNING] jit_analysis.py:  499: Unsupported operator aten::sigmoid encountered 15 time(s)
[11/07 13:31:00][WARNING] jit_analysis.py:  499: Unsupported operator aten::mul encountered 15 time(s)
[11/07 13:31:00][WARNING] jit_analysis.py:  499: Unsupported operator prim::PythonOp.SwishFunction encountered 26 time(s)
[11/07 13:31:00][WARNING] jit_analysis.py:  499: Unsupported operator aten::add encountered 26 time(s)
[11/07 13:31:00][WARNING] jit_analysis.py:  499: Unsupported operator aten::avg_pool3d encountered 1 time(s)
[11/07 13:31:00][WARNING] jit_analysis.py:  499: Unsupported operator aten::softmax encountered 1 time(s)
[11/07 13:31:00][WARNING] jit_analysis.py:  499: Unsupported operator aten::mean encountered 1 time(s)
[11/07 13:31:00][INFO] misc.py:  190: Flops: 3.249867536 G
[11/07 13:31:00][WARNING] jit_analysis.py:  499: Unsupported operator aten::batch_norm encountered 84 time(s)
[11/07 13:31:00][WARNING] jit_analysis.py:  499: Unsupported operator aten::adaptive_avg_pool3d encountered 15 time(s)
[11/07 13:31:00][WARNING] jit_analysis.py:  499: Unsupported operator aten::sigmoid encountered 15 time(s)
[11/07 13:31:00][WARNING] jit_analysis.py:  499: Unsupported operator aten::mul encountered 15 time(s)
[11/07 13:31:00][WARNING] jit_analysis.py:  499: Unsupported operator prim::PythonOp.SwishFunction encountered 26 time(s)
[11/07 13:31:00][WARNING] jit_analysis.py:  499: Unsupported operator aten::add encountered 26 time(s)
[11/07 13:31:00][WARNING] jit_analysis.py:  499: Unsupported operator aten::avg_pool3d encountered 1 time(s)
[11/07 13:31:00][WARNING] jit_analysis.py:  499: Unsupported operator aten::softmax encountered 1 time(s)
[11/07 13:31:00][WARNING] jit_analysis.py:  499: Unsupported operator aten::mean encountered 1 time(s)
[11/07 13:31:00][INFO] misc.py:  191: Activations: 56.753274 M
[11/07 13:31:00][INFO] misc.py:  196: nvidia-smi
Tue Nov  7 13:31:00 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 525.125.06   Driver Version: 525.125.06   CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Quadro RTX 6000     On   | 00000000:C1:00.0 Off |                  Off |
| 33%   46C    P2    86W / 260W |   2209MiB / 24576MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A     89828      C   python                           2206MiB |
+-----------------------------------------------------------------------------+
[11/07 13:31:00][INFO] misc.py:  185: Model:
X3D(
  (s1): VideoModelStem(
    (pathway0_stem): X3DStem(
      (conv_xy): Conv3d(3, 24, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1), bias=False)
      (conv): Conv3d(24, 24, kernel_size=(5, 1, 1), stride=(1, 1, 1), padding=(2, 0, 0), groups=24, bias=False)
      (bn): BatchNorm3d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (s2): ResStage(
    (pathway0_res0): ResBlock(
      (branch1): Conv3d(24, 24, kernel_size=(1, 1, 1), stride=(1, 2, 2), bias=False)
      (branch1_bn): BatchNorm3d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (branch2): X3DTransform(
        (a): Conv3d(24, 54, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (a_bn): BatchNorm3d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (a_relu): ReLU(inplace=True)
        (b): Conv3d(54, 54, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1), groups=54, bias=False)
        (b_bn): BatchNorm3d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): SE(
          (avg_pool): AdaptiveAvgPool3d(output_size=(1, 1, 1))
          (fc1): Conv3d(54, 8, kernel_size=(1, 1, 1), stride=(1, 1, 1))
          (fc1_act): ReLU()
          (fc2): Conv3d(8, 54, kernel_size=(1, 1, 1), stride=(1, 1, 1))
          (fc2_sig): Sigmoid()
        )
        (b_relu): Swish()
        (c): Conv3d(54, 24, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (c_bn): BatchNorm3d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (relu): ReLU(inplace=True)
    )
    (pathway0_res1): ResBlock(
      (branch2): X3DTransform(
        (a): Conv3d(24, 54, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (a_bn): BatchNorm3d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (a_relu): ReLU(inplace=True)
        (b): Conv3d(54, 54, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=54, bias=False)
        (b_bn): BatchNorm3d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (b_relu): Swish()
        (c): Conv3d(54, 24, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (c_bn): BatchNorm3d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (relu): ReLU(inplace=True)
    )
    (pathway0_res2): ResBlock(
      (branch2): X3DTransform(
        (a): Conv3d(24, 54, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (a_bn): BatchNorm3d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (a_relu): ReLU(inplace=True)
        (b): Conv3d(54, 54, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=54, bias=False)
        (b_bn): BatchNorm3d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): SE(
          (avg_pool): AdaptiveAvgPool3d(output_size=(1, 1, 1))
          (fc1): Conv3d(54, 8, kernel_size=(1, 1, 1), stride=(1, 1, 1))
          (fc1_act): ReLU()
          (fc2): Conv3d(8, 54, kernel_size=(1, 1, 1), stride=(1, 1, 1))
          (fc2_sig): Sigmoid()
        )
        (b_relu): Swish()
        (c): Conv3d(54, 24, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (c_bn): BatchNorm3d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (relu): ReLU(inplace=True)
    )
  )
  (s3): ResStage(
    (pathway0_res0): ResBlock(
      (branch1): Conv3d(24, 48, kernel_size=(1, 1, 1), stride=(1, 2, 2), bias=False)
      (branch1_bn): BatchNorm3d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (branch2): X3DTransform(
        (a): Conv3d(24, 108, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (a_bn): BatchNorm3d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (a_relu): ReLU(inplace=True)
        (b): Conv3d(108, 108, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1), groups=108, bias=False)
        (b_bn): BatchNorm3d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): SE(
          (avg_pool): AdaptiveAvgPool3d(output_size=(1, 1, 1))
          (fc1): Conv3d(108, 8, kernel_size=(1, 1, 1), stride=(1, 1, 1))
          (fc1_act): ReLU()
          (fc2): Conv3d(8, 108, kernel_size=(1, 1, 1), stride=(1, 1, 1))
          (fc2_sig): Sigmoid()
        )
        (b_relu): Swish()
        (c): Conv3d(108, 48, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (c_bn): BatchNorm3d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (relu): ReLU(inplace=True)
    )
    (pathway0_res1): ResBlock(
      (branch2): X3DTransform(
        (a): Conv3d(48, 108, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (a_bn): BatchNorm3d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (a_relu): ReLU(inplace=True)
        (b): Conv3d(108, 108, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=108, bias=False)
        (b_bn): BatchNorm3d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (b_relu): Swish()
        (c): Conv3d(108, 48, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (c_bn): BatchNorm3d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (relu): ReLU(inplace=True)
    )
    (pathway0_res2): ResBlock(
      (branch2): X3DTransform(
        (a): Conv3d(48, 108, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (a_bn): BatchNorm3d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (a_relu): ReLU(inplace=True)
        (b): Conv3d(108, 108, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=108, bias=False)
        (b_bn): BatchNorm3d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): SE(
          (avg_pool): AdaptiveAvgPool3d(output_size=(1, 1, 1))
          (fc1): Conv3d(108, 8, kernel_size=(1, 1, 1), stride=(1, 1, 1))
          (fc1_act): ReLU()
          (fc2): Conv3d(8, 108, kernel_size=(1, 1, 1), stride=(1, 1, 1))
          (fc2_sig): Sigmoid()
        )
        (b_relu): Swish()
        (c): Conv3d(108, 48, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (c_bn): BatchNorm3d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (relu): ReLU(inplace=True)
    )
    (pathway0_res3): ResBlock(
      (branch2): X3DTransform(
        (a): Conv3d(48, 108, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (a_bn): BatchNorm3d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (a_relu): ReLU(inplace=True)
        (b): Conv3d(108, 108, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=108, bias=False)
        (b_bn): BatchNorm3d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (b_relu): Swish()
        (c): Conv3d(108, 48, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (c_bn): BatchNorm3d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (relu): ReLU(inplace=True)
    )
    (pathway0_res4): ResBlock(
      (branch2): X3DTransform(
        (a): Conv3d(48, 108, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (a_bn): BatchNorm3d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (a_relu): ReLU(inplace=True)
        (b): Conv3d(108, 108, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=108, bias=False)
        (b_bn): BatchNorm3d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): SE(
          (avg_pool): AdaptiveAvgPool3d(output_size=(1, 1, 1))
          (fc1): Conv3d(108, 8, kernel_size=(1, 1, 1), stride=(1, 1, 1))
          (fc1_act): ReLU()
          (fc2): Conv3d(8, 108, kernel_size=(1, 1, 1), stride=(1, 1, 1))
          (fc2_sig): Sigmoid()
        )
        (b_relu): Swish()
        (c): Conv3d(108, 48, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (c_bn): BatchNorm3d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (relu): ReLU(inplace=True)
    )
  )
  (s4): ResStage(
    (pathway0_res0): ResBlock(
      (branch1): Conv3d(48, 96, kernel_size=(1, 1, 1), stride=(1, 2, 2), bias=False)
      (branch1_bn): BatchNorm3d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (branch2): X3DTransform(
        (a): Conv3d(48, 216, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (a_bn): BatchNorm3d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (a_relu): ReLU(inplace=True)
        (b): Conv3d(216, 216, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1), groups=216, bias=False)
        (b_bn): BatchNorm3d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): SE(
          (avg_pool): AdaptiveAvgPool3d(output_size=(1, 1, 1))
          (fc1): Conv3d(216, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1))
          (fc1_act): ReLU()
          (fc2): Conv3d(16, 216, kernel_size=(1, 1, 1), stride=(1, 1, 1))
          (fc2_sig): Sigmoid()
        )
        (b_relu): Swish()
        (c): Conv3d(216, 96, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (c_bn): BatchNorm3d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (relu): ReLU(inplace=True)
    )
    (pathway0_res1): ResBlock(
      (branch2): X3DTransform(
        (a): Conv3d(96, 216, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (a_bn): BatchNorm3d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (a_relu): ReLU(inplace=True)
        (b): Conv3d(216, 216, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=216, bias=False)
        (b_bn): BatchNorm3d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (b_relu): Swish()
        (c): Conv3d(216, 96, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (c_bn): BatchNorm3d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (relu): ReLU(inplace=True)
    )
    (pathway0_res2): ResBlock(
      (branch2): X3DTransform(
        (a): Conv3d(96, 216, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (a_bn): BatchNorm3d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (a_relu): ReLU(inplace=True)
        (b): Conv3d(216, 216, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=216, bias=False)
        (b_bn): BatchNorm3d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): SE(
          (avg_pool): AdaptiveAvgPool3d(output_size=(1, 1, 1))
          (fc1): Conv3d(216, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1))
          (fc1_act): ReLU()
          (fc2): Conv3d(16, 216, kernel_size=(1, 1, 1), stride=(1, 1, 1))
          (fc2_sig): Sigmoid()
        )
        (b_relu): Swish()
        (c): Conv3d(216, 96, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (c_bn): BatchNorm3d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (relu): ReLU(inplace=True)
    )
    (pathway0_res3): ResBlock(
      (branch2): X3DTransform(
        (a): Conv3d(96, 216, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (a_bn): BatchNorm3d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (a_relu): ReLU(inplace=True)
        (b): Conv3d(216, 216, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=216, bias=False)
        (b_bn): BatchNorm3d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (b_relu): Swish()
        (c): Conv3d(216, 96, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (c_bn): BatchNorm3d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (relu): ReLU(inplace=True)
    )
    (pathway0_res4): ResBlock(
      (branch2): X3DTransform(
        (a): Conv3d(96, 216, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (a_bn): BatchNorm3d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (a_relu): ReLU(inplace=True)
        (b): Conv3d(216, 216, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=216, bias=False)
        (b_bn): BatchNorm3d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): SE(
          (avg_pool): AdaptiveAvgPool3d(output_size=(1, 1, 1))
          (fc1): Conv3d(216, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1))
          (fc1_act): ReLU()
          (fc2): Conv3d(16, 216, kernel_size=(1, 1, 1), stride=(1, 1, 1))
          (fc2_sig): Sigmoid()
        )
        (b_relu): Swish()
        (c): Conv3d(216, 96, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (c_bn): BatchNorm3d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (relu): ReLU(inplace=True)
    )
    (pathway0_res5): ResBlock(
      (branch2): X3DTransform(
        (a): Conv3d(96, 216, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (a_bn): BatchNorm3d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (a_relu): ReLU(inplace=True)
        (b): Conv3d(216, 216, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=216, bias=False)
        (b_bn): BatchNorm3d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (b_relu): Swish()
        (c): Conv3d(216, 96, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (c_bn): BatchNorm3d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (relu): ReLU(inplace=True)
    )
    (pathway0_res6): ResBlock(
      (branch2): X3DTransform(
        (a): Conv3d(96, 216, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (a_bn): BatchNorm3d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (a_relu): ReLU(inplace=True)
        (b): Conv3d(216, 216, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=216, bias=False)
        (b_bn): BatchNorm3d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): SE(
          (avg_pool): AdaptiveAvgPool3d(output_size=(1, 1, 1))
          (fc1): Conv3d(216, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1))
          (fc1_act): ReLU()
          (fc2): Conv3d(16, 216, kernel_size=(1, 1, 1), stride=(1, 1, 1))
          (fc2_sig): Sigmoid()
        )
        (b_relu): Swish()
        (c): Conv3d(216, 96, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (c_bn): BatchNorm3d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (relu): ReLU(inplace=True)
    )
    (pathway0_res7): ResBlock(
      (branch2): X3DTransform(
        (a): Conv3d(96, 216, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (a_bn): BatchNorm3d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (a_relu): ReLU(inplace=True)
        (b): Conv3d(216, 216, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=216, bias=False)
        (b_bn): BatchNorm3d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (b_relu): Swish()
        (c): Conv3d(216, 96, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (c_bn): BatchNorm3d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (relu): ReLU(inplace=True)
    )
    (pathway0_res8): ResBlock(
      (branch2): X3DTransform(
        (a): Conv3d(96, 216, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (a_bn): BatchNorm3d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (a_relu): ReLU(inplace=True)
        (b): Conv3d(216, 216, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=216, bias=False)
        (b_bn): BatchNorm3d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): SE(
          (avg_pool): AdaptiveAvgPool3d(output_size=(1, 1, 1))
          (fc1): Conv3d(216, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1))
          (fc1_act): ReLU()
          (fc2): Conv3d(16, 216, kernel_size=(1, 1, 1), stride=(1, 1, 1))
          (fc2_sig): Sigmoid()
        )
        (b_relu): Swish()
        (c): Conv3d(216, 96, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (c_bn): BatchNorm3d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (relu): ReLU(inplace=True)
    )
    (pathway0_res9): ResBlock(
      (branch2): X3DTransform(
        (a): Conv3d(96, 216, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (a_bn): BatchNorm3d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (a_relu): ReLU(inplace=True)
        (b): Conv3d(216, 216, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=216, bias=False)
        (b_bn): BatchNorm3d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (b_relu): Swish()
        (c): Conv3d(216, 96, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (c_bn): BatchNorm3d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (relu): ReLU(inplace=True)
    )
    (pathway0_res10): ResBlock(
      (branch2): X3DTransform(
        (a): Conv3d(96, 216, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (a_bn): BatchNorm3d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (a_relu): ReLU(inplace=True)
        (b): Conv3d(216, 216, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=216, bias=False)
        (b_bn): BatchNorm3d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): SE(
          (avg_pool): AdaptiveAvgPool3d(output_size=(1, 1, 1))
          (fc1): Conv3d(216, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1))
          (fc1_act): ReLU()
          (fc2): Conv3d(16, 216, kernel_size=(1, 1, 1), stride=(1, 1, 1))
          (fc2_sig): Sigmoid()
        )
        (b_relu): Swish()
        (c): Conv3d(216, 96, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (c_bn): BatchNorm3d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (relu): ReLU(inplace=True)
    )
  )
  (s5): ResStage(
    (pathway0_res0): ResBlock(
      (branch1): Conv3d(96, 192, kernel_size=(1, 1, 1), stride=(1, 2, 2), bias=False)
      (branch1_bn): BatchNorm3d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (branch2): X3DTransform(
        (a): Conv3d(96, 432, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (a_bn): BatchNorm3d(432, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (a_relu): ReLU(inplace=True)
        (b): Conv3d(432, 432, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1), groups=432, bias=False)
        (b_bn): BatchNorm3d(432, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): SE(
          (avg_pool): AdaptiveAvgPool3d(output_size=(1, 1, 1))
          (fc1): Conv3d(432, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1))
          (fc1_act): ReLU()
          (fc2): Conv3d(32, 432, kernel_size=(1, 1, 1), stride=(1, 1, 1))
          (fc2_sig): Sigmoid()
        )
        (b_relu): Swish()
        (c): Conv3d(432, 192, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (c_bn): BatchNorm3d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (relu): ReLU(inplace=True)
    )
    (pathway0_res1): ResBlock(
      (branch2): X3DTransform(
        (a): Conv3d(192, 432, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (a_bn): BatchNorm3d(432, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (a_relu): ReLU(inplace=True)
        (b): Conv3d(432, 432, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=432, bias=False)
        (b_bn): BatchNorm3d(432, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (b_relu): Swish()
        (c): Conv3d(432, 192, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (c_bn): BatchNorm3d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (relu): ReLU(inplace=True)
    )
    (pathway0_res2): ResBlock(
      (branch2): X3DTransform(
        (a): Conv3d(192, 432, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (a_bn): BatchNorm3d(432, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (a_relu): ReLU(inplace=True)
        (b): Conv3d(432, 432, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=432, bias=False)
        (b_bn): BatchNorm3d(432, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): SE(
          (avg_pool): AdaptiveAvgPool3d(output_size=(1, 1, 1))
          (fc1): Conv3d(432, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1))
          (fc1_act): ReLU()
          (fc2): Conv3d(32, 432, kernel_size=(1, 1, 1), stride=(1, 1, 1))
          (fc2_sig): Sigmoid()
        )
        (b_relu): Swish()
        (c): Conv3d(432, 192, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (c_bn): BatchNorm3d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (relu): ReLU(inplace=True)
    )
    (pathway0_res3): ResBlock(
      (branch2): X3DTransform(
        (a): Conv3d(192, 432, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (a_bn): BatchNorm3d(432, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (a_relu): ReLU(inplace=True)
        (b): Conv3d(432, 432, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=432, bias=False)
        (b_bn): BatchNorm3d(432, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (b_relu): Swish()
        (c): Conv3d(432, 192, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (c_bn): BatchNorm3d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (relu): ReLU(inplace=True)
    )
    (pathway0_res4): ResBlock(
      (branch2): X3DTransform(
        (a): Conv3d(192, 432, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (a_bn): BatchNorm3d(432, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (a_relu): ReLU(inplace=True)
        (b): Conv3d(432, 432, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=432, bias=False)
        (b_bn): BatchNorm3d(432, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): SE(
          (avg_pool): AdaptiveAvgPool3d(output_size=(1, 1, 1))
          (fc1): Conv3d(432, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1))
          (fc1_act): ReLU()
          (fc2): Conv3d(32, 432, kernel_size=(1, 1, 1), stride=(1, 1, 1))
          (fc2_sig): Sigmoid()
        )
        (b_relu): Swish()
        (c): Conv3d(432, 192, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (c_bn): BatchNorm3d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (relu): ReLU(inplace=True)
    )
    (pathway0_res5): ResBlock(
      (branch2): X3DTransform(
        (a): Conv3d(192, 432, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (a_bn): BatchNorm3d(432, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (a_relu): ReLU(inplace=True)
        (b): Conv3d(432, 432, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=432, bias=False)
        (b_bn): BatchNorm3d(432, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (b_relu): Swish()
        (c): Conv3d(432, 192, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (c_bn): BatchNorm3d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (relu): ReLU(inplace=True)
    )
    (pathway0_res6): ResBlock(
      (branch2): X3DTransform(
        (a): Conv3d(192, 432, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (a_bn): BatchNorm3d(432, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (a_relu): ReLU(inplace=True)
        (b): Conv3d(432, 432, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=432, bias=False)
        (b_bn): BatchNorm3d(432, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (se): SE(
          (avg_pool): AdaptiveAvgPool3d(output_size=(1, 1, 1))
          (fc1): Conv3d(432, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1))
          (fc1_act): ReLU()
          (fc2): Conv3d(32, 432, kernel_size=(1, 1, 1), stride=(1, 1, 1))
          (fc2_sig): Sigmoid()
        )
        (b_relu): Swish()
        (c): Conv3d(432, 192, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        (c_bn): BatchNorm3d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (relu): ReLU(inplace=True)
    )
  )
  (head): X3DHead(
    (conv_5): Conv3d(192, 432, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (conv_5_bn): BatchNorm3d(432, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv_5_relu): ReLU(inplace=True)
    (avg_pool): AvgPool3d(kernel_size=[15, 5, 5], stride=1, padding=0)
    (lin_5): Conv3d(432, 2048, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (lin_5_relu): ReLU(inplace=True)
    (dropout): Dropout(p=0.5, inplace=False)
    (projection): Linear(in_features=2048, out_features=13, bias=True)
    (act): Softmax(dim=4)
  )
)
[11/07 13:31:00][INFO] misc.py:  187: Params: 3,001,359
[11/07 13:31:00][INFO] misc.py:  188: Mem: 5.93186616897583 MB
[11/07 13:31:01][WARNING] jit_analysis.py:  499: Unsupported operator aten::adaptive_avg_pool3d encountered 15 time(s)
[11/07 13:31:01][WARNING] jit_analysis.py:  499: Unsupported operator aten::sigmoid encountered 15 time(s)
[11/07 13:31:01][WARNING] jit_analysis.py:  499: Unsupported operator aten::mul encountered 15 time(s)
[11/07 13:31:01][WARNING] jit_analysis.py:  499: Unsupported operator prim::PythonOp.SwishFunction encountered 26 time(s)
[11/07 13:31:01][WARNING] jit_analysis.py:  499: Unsupported operator aten::add encountered 26 time(s)
[11/07 13:31:01][WARNING] jit_analysis.py:  499: Unsupported operator aten::avg_pool3d encountered 1 time(s)
[11/07 13:31:01][WARNING] jit_analysis.py:  499: Unsupported operator aten::softmax encountered 1 time(s)
[11/07 13:31:01][WARNING] jit_analysis.py:  499: Unsupported operator aten::mean encountered 1 time(s)
[11/07 13:31:01][INFO] misc.py:  190: Flops: 3.249867536 G
[11/07 13:31:01][WARNING] jit_analysis.py:  499: Unsupported operator aten::batch_norm encountered 84 time(s)
[11/07 13:31:01][WARNING] jit_analysis.py:  499: Unsupported operator aten::adaptive_avg_pool3d encountered 15 time(s)
[11/07 13:31:01][WARNING] jit_analysis.py:  499: Unsupported operator aten::sigmoid encountered 15 time(s)
[11/07 13:31:01][WARNING] jit_analysis.py:  499: Unsupported operator aten::mul encountered 15 time(s)
[11/07 13:31:01][WARNING] jit_analysis.py:  499: Unsupported operator prim::PythonOp.SwishFunction encountered 26 time(s)
[11/07 13:31:01][WARNING] jit_analysis.py:  499: Unsupported operator aten::add encountered 26 time(s)
[11/07 13:31:01][WARNING] jit_analysis.py:  499: Unsupported operator aten::avg_pool3d encountered 1 time(s)
[11/07 13:31:01][WARNING] jit_analysis.py:  499: Unsupported operator aten::softmax encountered 1 time(s)
[11/07 13:31:01][WARNING] jit_analysis.py:  499: Unsupported operator aten::mean encountered 1 time(s)
[11/07 13:31:01][INFO] misc.py:  191: Activations: 56.753274 M
[11/07 13:31:01][INFO] misc.py:  196: nvidia-smi
Tue Nov  7 13:31:01 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 525.125.06   Driver Version: 525.125.06   CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Quadro RTX 6000     On   | 00000000:C1:00.0 Off |                  Off |
| 33%   46C    P2    61W / 260W |   2209MiB / 24576MiB |      1%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A     89828      C   python                           2206MiB |
+-----------------------------------------------------------------------------+
[11/07 13:31:01][INFO] checkpoint.py:  222: Loading network weights from ./checkpoints/checkpoint_epoch_00300.pyth.
missing keys: []
unexpected keys: []
[11/07 13:31:01][INFO] kinetics.py:  106: Constructing VRC test...
[11/07 13:31:01][INFO] kinetics.py:  162: Constructing VRC dataloader (size: 8 skip_rows 0) from /cluster/work/cvl/robocup/data/Debug_Dataset/small_test.csv 
[11/07 13:31:01][INFO] test_net.py:  220: Testing model for 2 iterations
